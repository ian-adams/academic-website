[{"authors":null,"categories":null,"content":"Ian T. Adams is a leading scholar of policing. He is an Assistant Professor in the Department of Criminology \u0026amp; Criminal Justice at the University of South Carolina. His applied research focuses on the practical concerns of police practitioners, with a specific interest in technology, policy, behavior, and use-of-force in law enforcement.\nDr. Adams has over fifty peer-reviewed publications on these and related topics, and his work has been published in the top general interest journals of both criminal justice and public administration, including Criminology, Justice Quarterly, Criminology \u0026amp; Public Policy, and Public Administration Review. He is the recipient of the 2024 Early Career Award from the American Society of Criminology, Division of Policing, the 2025 Early Career Award from the Division of Experimetnal Criminology (also ASC), and the 2026 Emerging Scholar Award, from the Police Section of the Academy of Criminal Justice Sciences.\nDr. Adams is the co-founder of the Police Accountability and Policy Evaluation Research (PAPER) Lab, and senior faculty of the Governance \u0026amp; Responsible AI Laboratory (GRAIL). He is committed to the goal of marrying police practice and high-quality scientific evidence, an aim that is supported by his appointment as a 2023 National Institute of Justice LEADS Academic. In sum, he and his colleagues have secured more than $3.5 million in research grants from both governmental and philanthropic organizations.\nDr. Adams is an elected member of the Council on Criminal Justice where he serves on Task Force on Artificial Intelligence. He also serves as a Senior Research Advisor for the Excellence in Policing and Public Safety (EPPS) program at the University of South Carolina’s Joseph F. Rice School of Law, is an affiliate of the Police Staffing Observatory at Michigan State University, and a managing editor of Police Practice \u0026amp; Research: An International Journal. In addition, he has consulted as a policing expert across the US and internationally, including Australia and Europe.\nHis research and comments are routinely featured in media coverage, including at the Washington Post, New York Times, and NPR, among others, and his public-facing communication has appeared in multiple written, radio, and podcast outlets.\n  Download his CV here.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://ianadamsresearch.com/author/ian-t.-adams-ph.d./","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ian-t.-adams-ph.d./","section":"authors","summary":"Ian T. Adams is a leading scholar of policing. He is an Assistant Professor in the Department of Criminology \u0026amp; Criminal Justice at the University of South Carolina. His applied research focuses on the practical concerns of police practitioners, with a specific interest in technology, policy, behavior, and use-of-force in law enforcement.","tags":null,"title":"Ian T. Adams, Ph.D.","type":"authors"},{"authors":null,"categories":null,"content":"Welcome to PUBPL 6002 Most of the course takes place on Canvas, but occasionally I\u0026rsquo;ll develop additional information that I host here. You should see some links to the left!\n Under Construction!\n ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"f5b3a988e854240906143c05d743887a","permalink":"https://ianadamsresearch.com/courses/pubpl-6002/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/pubpl-6002/","section":"courses","summary":"Additional resources for PUBPL 6002 students","tags":null,"title":"Overview","type":"docs"},{"authors":[],"categories":["R","Police","Data Analysis"],"content":"\rImportant Data Limitations\rThis post presents data on police killings, which represents only a subset of all police uses of lethal force. The data captures fatal encounters Mapping Police Violence but does not include non-fatal police shootings (research suggests that a majority of police shootings are survived).\nTherefore, by definition these numbers undercount the total instances of lethal force used by police. When interpreting these statistics, it’s critical to remember that we are looking at deaths only, not all incidents where officers used potentially lethal force.\nAdditionally, this data represents descriptive statistics only and cannot establish causal relationships without additional controls for local crime rates, police deployment patterns, population demographics, socioeconomic factors, and jurisdictional policies.\nFinally, this is a vibe-coded prototype: a rapid, exploratory build to see how far I can get in standing up a public-data dashboard with automated ingestion and basic visuals. You definitely should not rely on these numbers in any kind of professional capacity without further investigation and validation.\nCritical disclaimer: I have not independently verified the MPV data, nor have I formally audited or validated my cleaning and harmonization routines. Treat everything here as exploratory and illustrative. The outputs are not suitable for professional, policy, legal, journalistic, or operational use, and should not be relied upon as an authoritative accounting of levels, rates, rankings, or trends.\n\rAutomatic Data Updates\rThis post automatically downloads the latest data from Mapping Police Violence whenever the website is rebuilt. The visualizations below will update as new data becomes available.\n\rData Summary\rData current as of January 04, 2026\n\rAll Deaths: 14,806 incidents from January 01, 2013 to December 31, 2025\rFatal Shootings: 14,110 incidents (95.3%)\rYears covered: 2013 - 2025\r\r\rCumulative Trends\rThese charts show the cumulative count of police killings throughout each year, allowing year-over-year comparison.\n\rFigure 1: Cumulative count of all police killings by method (gunshot, taser, vehicle, restraint, etc.)\r\r\rFigure 2: Cumulative count of fatal police shootings only\r\r\rDemographic Patterns\rRace and Ethnicity\r\rFigure 3: Distribution of fatal police shootings by race/ethnicity\r\r\rFigure 4: Fatal police shootings by year and race, with percentage of unknown race highlighted\r\r\rAge Distribution\r\rFigure 5: Age distribution of police shooting victims\r\r\rFigure 6: Age distribution by race for the three largest racial groups\r\r\r\rBehavioral Context\rArmed Status\r\rFigure 7: Armed/unarmed status of police shooting victims\r\r\rFigure 8: Percentage of victims who were unarmed, by race\r\r\rMental Health Context\r\rFigure 9: Percentage of victims with documented mental illness symptoms over time\r\r\r\rTemporal Patterns\rWhen Do Shootings Occur?\r\rFigure 10: Heatmap showing daily patterns of police shootings across the calendar year\r\r\rFigure 11: Distribution of police shootings by day of week\r\r\rFigure 12: Top 15 states by number of police fatal shootings\r\r\rFigure 13: Top 20 cities by number of shootings, colored by percentage of victims who were unarmed\r\r\r\rPer Capita Analysis\r\rFigure 14: Police shooting deaths per capita by year, with 92% counting accuracy assumption\r\r\rAccountability and Socioeconomic Context\r\rFigure 15: Percentage of police shooting cases resulting in criminal charges\r\r\rFigure 16: Police shootings by neighborhood median income and race\r\r\rData Sources and Methods\rPrimary Data Source\rAll data comes from Mapping Police Violence, which aggregates information from:\n\rFatal Encounters - A database documenting all deaths through police interaction\rThe Washington Post’s Fatal Force database - Tracking fatal police shootings (no longer active)\rAdditional cases identified through crowdsourcing and independent research\r\r\rUpdate Frequency\rThis blog post automatically downloads the latest MPV data whenever my website is rebuilt. The database is continuously updated as new incidents are reported and verified.\n\rImportant Methodological Considerations\rIncomplete Coverage: This data captures police killings only, not all uses of lethal force. Research indicates that a slight majority of police shootings are survived, meaning these statistics undercount total lethal force incidents.\n\rDescriptive Statistics: All analyses presented here are descriptive. Causal interpretation requires controlling for a host of unobserved variables, including but not limited to:\n\rLocal crime rates and patterns\rPolice deployment density\rPopulation demographics\rSocioeconomic factors\rJurisdictional policies and training\r\rData Quality: While MPV represents the most comprehensive publicly available database, it relies on media reports and public records, which have gaps and inconsistencies.\n\rRacial Categories: Racial/ethnic categorizations follow the source data and may not capture the full complexity of identity.\n\r\r\rCode Availability\rThe R code for this analysis is embedded in this R Markdown document. The source code is available in the academic-website repository.\nLast updated: January 04, 2026\nThis analysis is provided for research and educational purposes. The data should be interpreted within the broader context of research on policing.\n\r\r","date":1767484800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1767484800,"objectID":"2a085e6faaf9d7a1f877c9d3412671c6","permalink":"https://ianadamsresearch.com/post/2026-01-04-mpv-data-analysis/","publishdate":"2026-01-04T00:00:00Z","relpermalink":"/post/2026-01-04-mpv-data-analysis/","section":"post","summary":"An automatically-updating analysis of police killings in the United States using data from Mapping Police Violence, with static visualizations showing temporal trends, demographic patterns, and geographic distributions.","tags":["police","data visualization","R"],"title":"Analyzing Police Violence in America: Updated Data Through 2025","type":"post"},{"authors":[],"categories":["R","Police"],"content":"\rI’ve developed an interactive dashboard to explore data from Mapping Police Violence, a comprehensive database tracking police killings in the United States. The dashboard automatically pulls the latest data and provides multiple analytical views to examine temporal trends, demographic patterns, and geographic distributions.\nDashboard Features\rThe MPV Analytical Engine includes several key analytical modules:\nOverview Panel\r\rReal-time statistics including total incidents, per capita rates, and demographic breakdowns\rCumulative trajectory visualization comparing year-over-year trends\rTemporal heatmaps showing patterns by day and month\rDay-of-week analysis\r\r\rDemographics\r\rRacial and ethnic distribution of victims\rAge distribution and age-by-race breakdowns\rAnalysis of unarmed victims by demographic group\r\r\rBehavioral Context\r\rArmed/unarmed status of victims\rTypes of alleged weapons\rFleeing status at time of incident\rMental health-related encounters over time\rBody camera adoption trends\r\r\rGeographic Analysis\r\rNational map of incidents\rState and city-level breakdowns\rTop jurisdictions by incident count\r\r\rAccountability\r\rCriminal charges filed against officers over time\rNeighborhood income disparities\r\r\r\rInteractive Dashboard\r\rNote: The dashboard may take a moment to load as it downloads and processes the latest data from Mapping Police Violence.\nClick here to open the dashboard in a new window\n\rImportant Methodological Notes\rThis dashboard presents descriptive statistics only. The data shows observed patterns but cannot establish causal relationships without additional controls for:\n\rLocal crime rates\rPolice deployment density\rPopulation demographics\rSocioeconomic factors\rJurisdictional policies\r\rThe dashboard uses US Census population projections (2013-2025) to calculate per capita rates. All racial categorizations follow the source data from Mapping Police Violence.\n\rData Source\rData is automatically synchronized from Mapping Police Violence, which aggregates information from:\r- Fatal Encounters\r- The Washington Post’s Fatal Force database\r- Additional cases identified through crowdsourcing and research\n\rTechnical Details\rThe dashboard is built using:\r- R Shiny with the bslib framework for modern UI components\r- tidyverse for data manipulation\r- plotly for interactive visualizations\r- sf and rnaturalearth for geographic mapping\nThe app automatically downloads fresh data daily and includes robust data cleaning pipelines to standardize variables across time periods.\n\rCode Availability\rThe complete source code for this dashboard is available in the academic-website repository.\nThis dashboard is provided for research and educational purposes. The data presented reflects reported incidents and should be interpreted within the context of broader research on policing, public safety, and racial justice.\n\r","date":1767484800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1767484800,"objectID":"e421e3959f8df66404c181836c4ef653","permalink":"https://ianadamsresearch.com/post/2026-01-04-mapping-police-violence-dashboard/","publishdate":"2026-01-04T00:00:00Z","relpermalink":"/post/2026-01-04-mapping-police-violence-dashboard/","section":"post","summary":"An interactive Shiny dashboard for exploring temporal, demographic, and geographic patterns in the Mapping Police Violence dataset.","tags":["police","data visualization","shiny"],"title":"Interactive Dashboard for Mapping Police Violence Data","type":"post"},{"authors":[],"categories":[],"content":"\rDr. Justin Nix has been hard at work locating historical data for lethal police shootings in the United States. With his permission, the following tool allows you to see individual agencies in the dataset.\nSeveral limitations are to be noted with this data, please see his post explaining the project for more.\n\r","date":1680998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1681080963,"objectID":"eaebae5afbf4c90ce5b339130691b947","permalink":"https://ianadamsresearch.com/post/agency-level-fatal-police-shootings-over-time/","publishdate":"2023-04-09T00:00:00Z","relpermalink":"/post/agency-level-fatal-police-shootings-over-time/","section":"post","summary":"Dr. Justin Nix has been hard at work locating historical data for lethal police shootings in the United States. With his permission, the following tool allows you to see individual agencies in the dataset.","tags":[],"title":"Agency-level fatal police shootings over time","type":"post"},{"authors":[],"categories":[],"content":"\rIn this blog post, we will analyze and compare the turnover rates of local police agencies with 100 or more sworn officers in the United States using the Law Enforcement Management and Administrative Statistics (LEMAS) data from 2016 and 2020. By the end of this post, you will learn how to import, preprocess, and visualize the LEMAS data to gain insights into the turnover rates of law enforcement agencies.\nSetup\rFirst, let’s load the required libraries and set some global options for our code chunks. We will be using:\nlibrary(tidyverse)\rlibrary(readr)\rlibrary(here)\rlibrary(scales)\rlibrary(viridis)\rNext, we will import the 2016 and 2020 LEMAS data and preprocess it, including cleaning column names and selecting relevant columns for our analysis. Make sure you have already imported the 2016 data to an object named df_raw_2016 and the 2020 data to one named df_raw_2020.\n\rData Import and Preprocessing\rIn this section, we will import the 2016 and 2020 LEMAS data, preprocess the data, and calculate the turnover rates.\nLEMAS 2016 Data\rLet’s start by importing the 2016 LEMAS data and preprocessing it.\n## Import 2016 data\r# Select relevant columns and calculate vacancies\rdf_2016 \u0026lt;- df_raw_2016 %\u0026gt;% select(\rlear_id,\ragencyname,\ragencytype,\rstrata,\rpopulation = popserved,\rftsauth,\rftsworn,\rtotal_separations = pers_sep_totr\r) %\u0026gt;% mutate(vacancies = ftsauth - ftsworn) %\u0026gt;%\rfilter(vacancies \u0026gt;= 0)\r# Calculate vacancy and turnover rates\rdf_2016 \u0026lt;- df_2016 %\u0026gt;%\rmutate(vac_rate = (round((vacancies)/(ftsauth), digits = 2))*100,\rturnover_rate = total_separations / ftsworn)\r# Filter agencies with 100 or more sworn officers\rdf100_2016 \u0026lt;- df_2016 %\u0026gt;% filter(strata == \u0026quot;(101) LP: 100+\u0026quot;)\r\rLEMAS 2020 Data\rNow, let’s import and preprocess the 2020 LEMAS data.\n# Import 2020 data\r# Renaming\rdf_2020 \u0026lt;- df_raw_2020 %\u0026gt;% select(\ragencyid,\ragencyname,\ragencytype = agencysamptype,\rstrata,\rpopulation = primarypop2020,\rsworn2019 = ftsworn_2019,\rsworn2020 = ftsworn,\rtotal_separations = tot_sep,\rvacancy2019 = ftvac_2019,\rvacancy2020 = ftvac\r)\r# Calculate vacancy rates\rdf_2020$vac_rate2020 \u0026lt;- (round((df_2020$vacancy2020)/(df_2020$sworn2020 + df_2020$vacancy2020), digits = 2))*100\rdf_2020$vac_rate2019 \u0026lt;- (round((df_2020$vacancy2019)/(df_2020$sworn2019 + df_2020$vacancy2019), digits = 2))*100\rdf_2020$turnover_rate \u0026lt;- df_2020$total_separations / df_2020$sworn2020\rdf100_2020 \u0026lt;- df_2020 %\u0026gt;% filter(sworn2019 \u0026gt; 99)\rNow that we have imported and preprocessed both the 2016 and 2020 LEMAS datasets, let’s move on to visualizing the turnover rates and comparing them side-by-side.\n\r\rTurnover Rate Comparison\rIn this section, we will create a side-by-side comparison of the 2016 and 2020 turnover rates for local police agencies with 100 or more sworn officers.\nFirst, we will filter out the outliers and calculate the mean and standard deviation for the turnover rates in both datasets.\ndf100_filtered_2016 \u0026lt;- df100_2016 %\u0026gt;% filter(turnover_rate \u0026lt;= 0.30)\rturnover_mean_2016 \u0026lt;- mean(df100_filtered_2016$turnover_rate, na.rm = TRUE)\rturnover_sd_2016 \u0026lt;- sd(df100_filtered_2016$turnover_rate, na.rm = TRUE)\rdf100_filtered_2020 \u0026lt;- df100_2020 %\u0026gt;% filter(turnover_rate \u0026lt;= 0.30)\rturnover_mean_2020 \u0026lt;- mean(df100_filtered_2020$turnover_rate, na.rm = TRUE)\rturnover_sd_2020 \u0026lt;- sd(df100_filtered_2020$turnover_rate, na.rm = TRUE)\rNext, let’s create a reusable function to generate the turnover rate plots for both years.\nplot_turnover \u0026lt;- function(df, year, mean, sd) {\rggplot(data = df, aes(x = turnover_rate)) + # Create the base ggplot with turnover_rate on the x-axis\rgeom_histogram(binwidth = 0.025, fill = \u0026quot;steelblue\u0026quot;, color = \u0026quot;white\u0026quot;) + # Add a histogram with custom colors and binwidth\rscale_x_continuous(breaks = seq(0, 0.5, 0.02), labels = scales::percent) + # Set x-axis breaks and labels\rlabs(title = paste0(year, \u0026quot; Turnover Rate\u0026quot;), # Set the title\rsubtitle = paste0(\u0026quot;Local Police Agencies with 100+ Sworn Officers (n=\u0026quot;, nrow(df), \u0026quot;)\\n\u0026quot;,\r\u0026quot;Mean: \u0026quot;, scales::percent(mean, accuracy = 0.01), \u0026quot; | \u0026quot;,\r\u0026quot;SD: \u0026quot;, scales::percent(sd, accuracy = 0.01)), # Set the subtitle with mean and SD values\rx = \u0026quot;Turnover Rate\u0026quot;, # Set the x-axis label\ry = \u0026quot;Frequency\u0026quot;) + # Set the y-axis label\rgeom_vline(xintercept = mean, linetype=\u0026quot;dashed\u0026quot;, linewidth = 1.5, color = \u0026quot;yellow\u0026quot;) + # Add a vertical dashed line at the mean\rtheme_minimal() + # Apply the minimal theme\rtheme(plot.title = element_text(size = 18, face = \u0026quot;bold\u0026quot;, margin = margin(10, 0, 5, 0)),\rplot.subtitle = element_text(size = 14, margin = margin(0, 0, 10, 0)),\raxis.title = element_text(size = 14, face = \u0026quot;bold\u0026quot;),\raxis.text = element_text(size = 12),\rpanel.grid.major.y = element_line(color = \u0026quot;gray\u0026quot;, linetype = \u0026quot;dashed\u0026quot;)) # Customize theme elements\r}\rUsing the plot_turnover function, let’s create the turnover rate plots for 2016 and 2020.\nplot_2016 \u0026lt;- plot_turnover(df100_filtered_2016, \u0026quot;2016\u0026quot;, turnover_mean_2016, turnover_sd_2016)\rplot_2020 \u0026lt;- plot_turnover(df100_filtered_2020, \u0026quot;2020\u0026quot;, turnover_mean_2020, turnover_sd_2020)\rFinally, let’s display the plots side-by-side to compare the turnover rates in 2016 and 2020\nplot_2016\rplot_2020\rFrom the side-by-side comparison, we can observe the differences in turnover rates between 2016 and 2020 for local police agencies with 100 or more sworn officers. Note the distribution of the turnover rates and how the mean and standard deviation have changed over time.\n\rConclusion\rIn this blog post, I have demonstrated how to import, preprocess, and visualize the LEMAS 2016 and 2020 datasets to analyze and compare the turnover rates in local police agencies with 100 or more sworn officers. The side-by-side comparison of the turnover rates provides valuable insights into the changes within law enforcement agencies over the years.\nBy following these steps, you can further explore and analyze the LEMAS data and draw conclusions about various aspects of law enforcement management and administration.\nFeel free to extend this analysis to other variables or subsets of agencies within the LEMAS data or even compare other years to gain a deeper understanding of trends in law enforcement. Happy analyzing!\n\r","date":1678838400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678909004,"objectID":"097937f7b78c263a0cc4fb35e055e584","permalink":"https://ianadamsresearch.com/post/lemas-2016-and-2020-turnover-comparison/","publishdate":"2023-03-15T00:00:00Z","relpermalink":"/post/lemas-2016-and-2020-turnover-comparison/","section":"post","summary":"In this blog post, we will analyze and compare the turnover rates of local police agencies with 100 or more sworn officers in the United States using the Law Enforcement Management and Administrative Statistics (LEMAS) data from 2016 and 2020.","tags":[],"title":"LEMAS 2016 and 2020 Turnover Comparison","type":"post"},{"authors":[],"categories":["R","stats","research"],"content":"\r\rQualtrics Messy Data\rMy friend Devon Cantwell reached out with an interesting messy data caused by how Qualtrics produces “select all that apply” variables. For example, in her (mock) survey, she asks students to select all the colors that they personally find attractive from a list. When downloaded from Qualtrics, we get a dataframe that looks like this:\nglimpse(dat)\r## Rows: 940\r## Columns: 4\r## $ color_1 \u0026lt;fct\u0026gt; Sparkle, Blue, Blue, Sparkle, Blue, Sparkle, Sparkle, Green, B~\r## $ color_2 \u0026lt;fct\u0026gt; NA, Moldy Book, NA, Moldy Book, Moldy Book, Honey Bee, Moldy B~\r## $ color_3 \u0026lt;fct\u0026gt; NA, Apple Core Brown, NA, Apple Core Brown, NA, NA, NA, NA, NA~\r## $ color_4 \u0026lt;fct\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA~\rSo all students pick at least one color, some pick two, but relatively few pick three or four. One thing we might want to know is the first color selected by respondent? That’s relatively easy:\ndat %\u0026gt;% count(color_1)\r## # A tibble: 8 x 2\r## color_1 n\r## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt;\r## 1 Blue 233\r## 2 Green 134\r## 3 Yellow 14\r## 4 Sparkle 189\r## 5 Apple Core Brown 6\r## 6 Honey Bee 13\r## 7 Moldy Book 42\r## 8 \u0026lt;NA\u0026gt; 309\rBut this only tells us the first color selected, not how many times a color was selected. What if we want to count all the instances where “Moldy Book” was selected, across columns? Or getting a more succinct answer for all colors? Because these are not ordered in any way, and the respondent wasn’t asked for an ordered preference, we need to count across the variables.\nWe can use tidyr for a quick solution:\nlibrary(tidyr)\rdat %\u0026gt;%\rgather(key, value, na.rm = TRUE) %\u0026gt;%\rcount(value)\r## Warning: attributes are not identical across measure variables;\r## they will be dropped\r## # A tibble: 7 x 2\r## value n\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 Apple Core Brown 78\r## 2 Blue 233\r## 3 Green 134\r## 4 Honey Bee 32\r## 5 Moldy Book 222\r## 6 Sparkle 230\r## 7 Yellow 38\rGood thing we checked! It turns out that Sparkle and Moldy Book are basically just as popular as Blue! If we had stopped with just checking the first color picked, our inference for color preference would have been way off.\n\r","date":1629849600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629904655,"objectID":"d3ccf22541765992766965b377860802","permalink":"https://ianadamsresearch.com/post/counting-select-all-that-apply-questions-in-qualtrics-using-r/","publishdate":"2021-08-25T00:00:00Z","relpermalink":"/post/counting-select-all-that-apply-questions-in-qualtrics-using-r/","section":"post","summary":"Qualtrics Messy Data\rMy friend Devon Cantwell reached out with an interesting messy data caused by how Qualtrics produces “select all that apply” variables. For example, in her (mock) survey, she asks students to select all the colors that they personally find attractive from a list.","tags":["Demo","R Markdown","qualtrics","stats","survey","research"],"title":"Counting \"Select All That Apply\" Questions in Qualtrics","type":"post"},{"authors":[],"categories":["police","R","research","stats"],"content":"\r\r\rIntroduction\rResearchers might be interested in developing a descriptive understanding of the gender and race composition of a particular industry, organization, or other institution. Oftentimes this is done with sampling from a population. This is the case in law enforcement. With approximately 18,000 sub-federal law enforcement agencies in the United States, and somewhere around 800,000 officers, it can be a challenging environment for researchers. Given the huge variation in agency type, size, composition, etc., generalizing across “law enforcement” is tricky at best.\nIn this preliminary analysis, I attempt a population-level inference for US law enforcement agencies, to develop estimates of race and gender proportions in the “chief executive” spot. The chief executive for a sherrif’s office is the Sheriff (often elected), while in a state-level agency it might be Executive Director - there is a lot of variation.\nGender and Race in US Law Enforcement\rJohn Shjarback and Natalie Todak (2019) use data from the 2013 Law Enforcement Management and Administrative Statistics (LEMAS) survey to analyze correlates of women in supervisory, mid-level, and chief executive roles in 2,826 municipal police, sheriff’s offices, and primary state law enforcement agencies. The 2013 LEMAS data was the first national survey to report on this level of data, and just 2.7% of the agencies were led by women. My goal here will be to see if using a commercial database of a much larger set of agencies, combined with a probabilistic estimate of gender and race, compares to the estimates from the 2013 LEMAS.\nThe 2016 LEMAS estimates that for chiefs across all size of local agencies, 89.6% were White, 4% Black, 3.1% Hispanic, and 2.4% other. It also estimates that in those same agencies, just 2.6% of chiefs were female. However, this 2016 sample design results in 2,612 local agencies (rather than the larger sample of all agencies), and uses a stratified sampling that intentionally oversamples from the largest agencies (+100 full-time officers).\nBut another method might be obtaining population-level information and inferring race and gender for the individuals based on that information. Jacob Kaplan has developed the predictrace package to do just that. The package develops a probability of race and gender based on the first name of a subject. This is from the package’s introduction:\n\rThe goal of predictrace is to predict the race of a surname or first name and the gender of a first name. This package uses U.S. Census data which says how many people of each race has a certain surname. For first name data, this package uses data from Tzioumis (2018). From this we can predict which race is mostly likely to have that surname or first name. The possible races are American Indian, Asian, Black, Hispanic, White, or two or more races. For the gender of first names, this package uses data from the United States Social Security Administration (SSA) that tells how many people of a given name are female and how many are male (no other genders are included). I use this to determine the proportion of each gender a name is, and use the gender with the higher proportion as the most likely gender for that name. Please note that the Census data on the race of first names is far smaller than the SSA data on the gender of first names, so you will match far fewer first names to race than to gender.\n\r\rData\rIn this short demonstration, I will attempt to develop race and gender estimates for individuals who lead US law enforcement agencies. To do so, I will rely on a commercial dataset from the National Directory of Law Enforcement Administrators (NDLEA). The dataset contains just over 37,000 listings for the chief administrator of law enforcement organizations at every level of the US system - from municipal police to heads of major federal agencies like the FBI, and everything in-between. The company that puts this database together commits to contacting every agency on the list at least once a year, and the company representative I spoke to said they are closer to once every three months. In my experience the dataset has been very reliable when I need to contact a head administrator directly.\nHowever, in order to constrain the analysis, I will just be looking at Campus Law Enforcement, County Sheriffs, and Municipal Law Enforcement agencies (n=17,104). Because I look at some correlations later with population, I drop any observations missing that information (missing n= 204), leaving a total of 16,900 observations. I’ll also reduce this to a simpler dataset by retaining only the department type, first name of administrator, state, and population served.\nLet’s check and see if that looks right.\r\r\r\rDeptType\r\rFirstName\r\rMailingState\r\rPopulation\r\r\r\r\r\rCampus Law Enforcement\r\rDave\r\rMA\r\r\u0026lt;25,000\r\r\r\rMunicipal Law Enforcement\r\rPaul\r\rVA\r\r\u0026lt;25,000\r\r\r\rMunicipal Law Enforcement\r\rJustin\r\rAK\r\r100k-1M\r\r\r\rMunicipal Law Enforcement\r\rThomas\r\rMI\r\r25k-50k\r\r\r\rMunicipal Law Enforcement\r\rTroy\r\rMO\r\r\u0026lt;25,000\r\r\r\rMunicipal Law Enforcement\r\rRonald\r\rMA\r\r\u0026lt;25,000\r\r\r\rMunicipal Law Enforcement\r\rJulian\r\rCA\r\r100k-1M\r\r\r\rMunicipal Law Enforcement\r\rJohn\r\rOK\r\r\u0026lt;25,000\r\r\r\rMunicipal Law Enforcement\r\rDavid\r\rMI\r\r\u0026lt;25,000\r\r\r\rMunicipal Law Enforcement\r\rMatt\r\rPA\r\r\u0026lt;25,000\r\r\r\r\rLooks like population data is pretty spotty (there’s an outlier from a typo that had the population of Shelby County, TN, at over 93 million! I fixed it behind the scenes here), but that’s not our main focus here today. Overall, it’s looking pretty good!\n\rInferring Race and Gender from First Name Data\rKaplan’s package predictrace will derive a gender and race classification for first names contained within our dataset. First we’ll use the predict_gender call, and then the predict_race functions to build the initial lists.\nAs you can see, the package reports probabilities for each entry, and gives a best-guess (likely_gender and likely_race) given those probabilities.\n\r\r\rname\r\rmatch_name\r\rlikely_race\r\rprobability_american_indian\r\rprobability_asian\r\rprobability_black\r\rprobability_hispanic\r\rprobability_white\r\rprobability_2races\r\r\r\r\r\rSteve\r\rsteve\r\rwhite\r\r0.0024\r\r0.0721\r\r0.0221\r\r0.0483\r\r0.8540\r\r0.0010\r\r\r\rEliezer\r\reliezer\r\rNA\r\rNA\r\rNA\r\rNA\r\rNA\r\rNA\r\rNA\r\r\r\rHector\r\rhector\r\rhispanic\r\r0.0000\r\r0.0135\r\r0.0045\r\r0.9270\r\r0.0550\r\r0.0000\r\r\r\rRon\r\rron\r\rwhite\r\r0.0034\r\r0.0469\r\r0.0402\r\r0.0235\r\r0.8844\r\r0.0017\r\r\r\rJames\r\rjames\r\rwhite\r\r0.0012\r\r0.0147\r\r0.0328\r\r0.0100\r\r0.9402\r\r0.0012\r\r\r\rDesiree\r\rdesiree\r\rwhite\r\r0.0030\r\r0.0334\r\r0.1246\r\r0.1155\r\r0.7143\r\r0.0091\r\r\r\rKevin\r\rkevin\r\rwhite\r\r0.0006\r\r0.0324\r\r0.0284\r\r0.0082\r\r0.9296\r\r0.0009\r\r\r\rRick\r\rrick\r\rwhite\r\r0.0029\r\r0.0284\r\r0.0073\r\r0.0277\r\r0.9314\r\r0.0022\r\r\r\rChristopher\r\rchristopher\r\rwhite\r\r0.0013\r\r0.0140\r\r0.0200\r\r0.0179\r\r0.9454\r\r0.0014\r\r\r\rKarl\r\rkarl\r\rwhite\r\r0.0007\r\r0.0260\r\r0.0281\r\r0.0070\r\r0.9374\r\r0.0007\r\r\r\r\r\r\r\rname\r\rmatch_name\r\rlikely_gender\r\rprobability_female\r\rprobability_male\r\r\r\r\r\rMichael\r\rmichael\r\rmale\r\r0.0049518\r\r0.9950482\r\r\r\rBerkley\r\rberkley\r\rfemale\r\r0.6417722\r\r0.3582278\r\r\r\rKelly\r\rkelly\r\rfemale\r\r0.8523312\r\r0.1476688\r\r\r\rDonald\r\rdonald\r\rmale\r\r0.0039238\r\r0.9960762\r\r\r\rAlfonzo\r\ralfonzo\r\rmale\r\r0.0000000\r\r1.0000000\r\r\r\rJoseph\r\rjoseph\r\rmale\r\r0.0040515\r\r0.9959485\r\r\r\rScott\r\rscott\r\rmale\r\r0.0033662\r\r0.9966338\r\r\r\rChristopher\r\rchristopher\r\rmale\r\r0.0046306\r\r0.9953694\r\r\r\rDennis\r\rdennis\r\rmale\r\r0.0042935\r\r0.9957065\r\r\r\rDonald\r\rdonald\r\rmale\r\r0.0039238\r\r0.9960762\r\r\r\r\rSo now let’s quickly add the best-guess from the predictrace package back to our original data, and quickly get a feel for the overall distribution of gender and race.\r\rTable 1: Summary Statistics\r\r\r\rVariable\r\rN\r\rPercent\r\r\r\r\r\rDeptType\r\r16900\r\r\r\r\r… Municipal Law Enforcement\r\r11697\r\r69.2%\r\r\r\r… Campus Law Enforcement\r\r2038\r\r12.1%\r\r\r\r… County Sheriffs\r\r3165\r\r18.7%\r\r\r\rPopulation\r\r16900\r\r\r\r\r… \u0026lt;25,000\r\r13614\r\r80.6%\r\r\r\r… 25k-50k\r\r1562\r\r9.2%\r\r\r\r… 50k-100k\r\r868\r\r5.1%\r\r\r\r… 100k-1M\r\r797\r\r4.7%\r\r\r\r… 1M-10M\r\r59\r\r0.3%\r\r\r\rgender\r\r16619\r\r\r\r\r… male\r\r15583\r\r93.8%\r\r\r\r… female\r\r1036\r\r6.2%\r\r\r\rrace\r\r16175\r\r\r\r\r… white\r\r15844\r\r98%\r\r\r\r… black\r\r67\r\r0.4%\r\r\r\r… hispanic\r\r234\r\r1.4%\r\r\r\r… hispanic, white\r\r2\r\r0%\r\r\r\r… asian\r\r27\r\r0.2%\r\r\r\r… asian, white\r\r1\r\r0%\r\r\r\r\r\rResults\rLet’s breakdown race and gender estimates by population of the area served by the agency. Because of the very low counts in Hispanic/White, and Asian/White, I’m going to collapse those into Hispanic and Asian categories respectively. As population data for very small areas (\u0026lt;1000 pop.) can be spotty in the NDLEA, we lose some observations.\nhtml {\rfont-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r}\r#phheblmehe .gt_table {\rdisplay: table;\rborder-collapse: collapse;\rmargin-left: auto;\rmargin-right: auto;\rcolor: #333333;\rfont-size: 16px;\rfont-weight: normal;\rfont-style: normal;\rbackground-color: #FFFFFF;\rwidth: auto;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #A8A8A8;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #A8A8A8;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\r}\r#phheblmehe .gt_heading {\rbackground-color: #FFFFFF;\rtext-align: center;\rborder-bottom-color: #FFFFFF;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\r}\r#phheblmehe .gt_title {\rcolor: #333333;\rfont-size: 125%;\rfont-weight: initial;\rpadding-top: 4px;\rpadding-bottom: 4px;\rborder-bottom-color: #FFFFFF;\rborder-bottom-width: 0;\r}\r#phheblmehe .gt_subtitle {\rcolor: #333333;\rfont-size: 85%;\rfont-weight: initial;\rpadding-top: 0;\rpadding-bottom: 4px;\rborder-top-color: #FFFFFF;\rborder-top-width: 0;\r}\r#phheblmehe .gt_bottom_border {\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\r}\r#phheblmehe .gt_col_headings {\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\r}\r#phheblmehe .gt_col_heading {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: normal;\rtext-transform: inherit;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: bottom;\rpadding-top: 5px;\rpadding-bottom: 6px;\rpadding-left: 5px;\rpadding-right: 5px;\roverflow-x: hidden;\r}\r#phheblmehe .gt_column_spanner_outer {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: normal;\rtext-transform: inherit;\rpadding-top: 0;\rpadding-bottom: 0;\rpadding-left: 4px;\rpadding-right: 4px;\r}\r#phheblmehe .gt_column_spanner_outer:first-child {\rpadding-left: 0;\r}\r#phheblmehe .gt_column_spanner_outer:last-child {\rpadding-right: 0;\r}\r#phheblmehe .gt_column_spanner {\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rvertical-align: bottom;\rpadding-top: 5px;\rpadding-bottom: 6px;\roverflow-x: hidden;\rdisplay: inline-block;\rwidth: 100%;\r}\r#phheblmehe .gt_group_heading {\rpadding: 8px;\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rtext-transform: inherit;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: middle;\r}\r#phheblmehe .gt_empty_group_heading {\rpadding: 0.5px;\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rvertical-align: middle;\r}\r#phheblmehe .gt_from_md  :first-child {\rmargin-top: 0;\r}\r#phheblmehe .gt_from_md  :last-child {\rmargin-bottom: 0;\r}\r#phheblmehe .gt_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rmargin: 10px;\rborder-top-style: solid;\rborder-top-width: 1px;\rborder-top-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: middle;\roverflow-x: hidden;\r}\r#phheblmehe .gt_stub {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rtext-transform: inherit;\rborder-right-style: solid;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\rpadding-left: 12px;\r}\r#phheblmehe .gt_summary_row {\rcolor: #333333;\rbackground-color: #FFFFFF;\rtext-transform: inherit;\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\r}\r#phheblmehe .gt_first_summary_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\r}\r#phheblmehe .gt_grand_summary_row {\rcolor: #333333;\rbackground-color: #FFFFFF;\rtext-transform: inherit;\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\r}\r#phheblmehe .gt_first_grand_summary_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rborder-top-style: double;\rborder-top-width: 6px;\rborder-top-color: #D3D3D3;\r}\r#phheblmehe .gt_striped {\rbackground-color: rgba(128, 128, 128, 0.05);\r}\r#phheblmehe .gt_table_body {\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\r}\r#phheblmehe .gt_footnotes {\rcolor: #333333;\rbackground-color: #FFFFFF;\rborder-bottom-style: none;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\r}\r#phheblmehe .gt_footnote {\rmargin: 0px;\rfont-size: 90%;\rpadding: 4px;\r}\r#phheblmehe .gt_sourcenotes {\rcolor: #333333;\rbackground-color: #FFFFFF;\rborder-bottom-style: none;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\r}\r#phheblmehe .gt_sourcenote {\rfont-size: 90%;\rpadding: 4px;\r}\r#phheblmehe .gt_left {\rtext-align: left;\r}\r#phheblmehe .gt_center {\rtext-align: center;\r}\r#phheblmehe .gt_right {\rtext-align: right;\rfont-variant-numeric: tabular-nums;\r}\r#phheblmehe .gt_font_normal {\rfont-weight: normal;\r}\r#phheblmehe .gt_font_bold {\rfont-weight: bold;\r}\r#phheblmehe .gt_font_italic {\rfont-style: italic;\r}\r#phheblmehe .gt_super {\rfont-size: 65%;\r}\r#phheblmehe .gt_footnote_marks {\rfont-style: italic;\rfont-weight: normal;\rfont-size: 65%;\r}\r\rTable 2: Race and Gender of Chief Administrator, by Population Served\r\rVariable\rOverall, N = 16,9001\r\u0026lt;25,000, N = 13,6141\r25k-50k, N = 1,5621\r50k-100k, N = 8681\r100k-1M, N = 7971\r1M-10M, N = 591\r\r\rrace\r\r\r\r\r\r\rWhite\r15,844 (97.95%)\r12,796 (98.13%)\r1,483 (97.95%)\r793 (97.42%)\r720 (95.87%)\r52 (92.86%)\rBlack\r67 (0.41%)\r50 (0.38%)\r6 (0.40%)\r6 (0.74%)\r4 (0.53%)\r1 (1.79%)\rHispanic\r236 (1.46%)\r175 (1.34%)\r23 (1.52%)\r14 (1.72%)\r21 (2.80%)\r3 (5.36%)\rAsian\r28 (0.17%)\r19 (0.15%)\r2 (0.13%)\r1 (0.12%)\r6 (0.80%)\r0 (0.00%)\rUnknown\r725\r574\r48\r54\r46\r3\rgender\r\r\r\r\r\r\rmale\r15,583 (93.77%)\r12,569 (93.85%)\r1,456 (94.12%)\r791 (93.94%)\r718 (92.17%)\r49 (83.05%)\rfemale\r1,036 (6.23%)\r823 (6.15%)\r91 (5.88%)\r51 (6.06%)\r61 (7.83%)\r10 (16.95%)\rUnknown\r281\r222\r15\r26\r18\r0\r\r\r1\r\rn (%)\r\r\r\r\r\rPerhaps unsurprisingly, law enforcement agencies are predominantly led by males. However, there may be progress over the decade or so. Compared to the LEMAS 2013 data, which estimated just 2.7% of agencies were led by women, my analysis estimates that overall 6.2% of agencies are led by women. The proportion of women-led agencies tends to be stable around 6% until we get to the larger population centers, and in the largest (between 1M and 10M pop.), 17% of the agencies are led by women. This is much larger than the 8.5% suggested by the 2016 LEMAS, though the largest category there is 250,000+ population.\nIn terms of racial characteristics, this analysis suggests that, overall, 98% of agencies are led by White chief executives. This percentage is negatively correlated with population. In other words, the percentage of White chief executives tends to decrease as the size of population served increases. Even at the top-end of population size, however, these positions are heavily skewed, as seen in the largest (1M to 10M) areas, where 93% of chief executives are estimated to be White.\nLet’s see if the proportions hold across agency types as well.\nhtml {\rfont-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r}\r#iixvrpylhv .gt_table {\rdisplay: table;\rborder-collapse: collapse;\rmargin-left: auto;\rmargin-right: auto;\rcolor: #333333;\rfont-size: 16px;\rfont-weight: normal;\rfont-style: normal;\rbackground-color: #FFFFFF;\rwidth: auto;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #A8A8A8;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #A8A8A8;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\r}\r#iixvrpylhv .gt_heading {\rbackground-color: #FFFFFF;\rtext-align: center;\rborder-bottom-color: #FFFFFF;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\r}\r#iixvrpylhv .gt_title {\rcolor: #333333;\rfont-size: 125%;\rfont-weight: initial;\rpadding-top: 4px;\rpadding-bottom: 4px;\rborder-bottom-color: #FFFFFF;\rborder-bottom-width: 0;\r}\r#iixvrpylhv .gt_subtitle {\rcolor: #333333;\rfont-size: 85%;\rfont-weight: initial;\rpadding-top: 0;\rpadding-bottom: 4px;\rborder-top-color: #FFFFFF;\rborder-top-width: 0;\r}\r#iixvrpylhv .gt_bottom_border {\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\r}\r#iixvrpylhv .gt_col_headings {\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\r}\r#iixvrpylhv .gt_col_heading {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: normal;\rtext-transform: inherit;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: bottom;\rpadding-top: 5px;\rpadding-bottom: 6px;\rpadding-left: 5px;\rpadding-right: 5px;\roverflow-x: hidden;\r}\r#iixvrpylhv .gt_column_spanner_outer {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: normal;\rtext-transform: inherit;\rpadding-top: 0;\rpadding-bottom: 0;\rpadding-left: 4px;\rpadding-right: 4px;\r}\r#iixvrpylhv .gt_column_spanner_outer:first-child {\rpadding-left: 0;\r}\r#iixvrpylhv .gt_column_spanner_outer:last-child {\rpadding-right: 0;\r}\r#iixvrpylhv .gt_column_spanner {\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rvertical-align: bottom;\rpadding-top: 5px;\rpadding-bottom: 6px;\roverflow-x: hidden;\rdisplay: inline-block;\rwidth: 100%;\r}\r#iixvrpylhv .gt_group_heading {\rpadding: 8px;\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rtext-transform: inherit;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: middle;\r}\r#iixvrpylhv .gt_empty_group_heading {\rpadding: 0.5px;\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rvertical-align: middle;\r}\r#iixvrpylhv .gt_from_md  :first-child {\rmargin-top: 0;\r}\r#iixvrpylhv .gt_from_md  :last-child {\rmargin-bottom: 0;\r}\r#iixvrpylhv .gt_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rmargin: 10px;\rborder-top-style: solid;\rborder-top-width: 1px;\rborder-top-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: middle;\roverflow-x: hidden;\r}\r#iixvrpylhv .gt_stub {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rtext-transform: inherit;\rborder-right-style: solid;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\rpadding-left: 12px;\r}\r#iixvrpylhv .gt_summary_row {\rcolor: #333333;\rbackground-color: #FFFFFF;\rtext-transform: inherit;\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\r}\r#iixvrpylhv .gt_first_summary_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\r}\r#iixvrpylhv .gt_grand_summary_row {\rcolor: #333333;\rbackground-color: #FFFFFF;\rtext-transform: inherit;\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\r}\r#iixvrpylhv .gt_first_grand_summary_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rborder-top-style: double;\rborder-top-width: 6px;\rborder-top-color: #D3D3D3;\r}\r#iixvrpylhv .gt_striped {\rbackground-color: rgba(128, 128, 128, 0.05);\r}\r#iixvrpylhv .gt_table_body {\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\r}\r#iixvrpylhv .gt_footnotes {\rcolor: #333333;\rbackground-color: #FFFFFF;\rborder-bottom-style: none;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\r}\r#iixvrpylhv .gt_footnote {\rmargin: 0px;\rfont-size: 90%;\rpadding: 4px;\r}\r#iixvrpylhv .gt_sourcenotes {\rcolor: #333333;\rbackground-color: #FFFFFF;\rborder-bottom-style: none;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\r}\r#iixvrpylhv .gt_sourcenote {\rfont-size: 90%;\rpadding: 4px;\r}\r#iixvrpylhv .gt_left {\rtext-align: left;\r}\r#iixvrpylhv .gt_center {\rtext-align: center;\r}\r#iixvrpylhv .gt_right {\rtext-align: right;\rfont-variant-numeric: tabular-nums;\r}\r#iixvrpylhv .gt_font_normal {\rfont-weight: normal;\r}\r#iixvrpylhv .gt_font_bold {\rfont-weight: bold;\r}\r#iixvrpylhv .gt_font_italic {\rfont-style: italic;\r}\r#iixvrpylhv .gt_super {\rfont-size: 65%;\r}\r#iixvrpylhv .gt_footnote_marks {\rfont-style: italic;\rfont-weight: normal;\rfont-size: 65%;\r}\r\rTable 3: Race and Gender of Chief Administrator, by Department Type\r\rVariable\rOverall, N = 16,9001\rMunicipal Law Enforcement, N = 11,6971\rCampus Law Enforcement, N = 2,0381\rCounty Sheriffs, N = 3,1651\r\r\rrace\r\r\r\r\rWhite\r15,844 (97.95%)\r11,035 (98.12%)\r1,866 (96.73%)\r2,943 (98.10%)\rBlack\r67 (0.41%)\r40 (0.36%)\r15 (0.78%)\r12 (0.40%)\rHispanic\r236 (1.46%)\r155 (1.38%)\r45 (2.33%)\r36 (1.20%)\rAsian\r28 (0.17%)\r16 (0.14%)\r3 (0.16%)\r9 (0.30%)\rUnknown\r725\r451\r109\r165\rgender\r\r\r\r\rmale\r15,583 (93.77%)\r10,910 (94.56%)\r1,729 (86.71%)\r2,944 (95.37%)\rfemale\r1,036 (6.23%)\r628 (5.44%)\r265 (13.29%)\r143 (4.63%)\rUnknown\r281\r159\r44\r78\r\r\r1\r\rn (%)\r\r\r\r\r\rAs you can see, based on these results, agency type does not seem to be correlated with higher percentages of non-white chief executives. However, campus law enforcement agencies are much more likely than other agency types to be led by women - over 13% compared to the average of 6.3% overall.\n\rConclusion\rThere is a lot of investigation needed before relying on these estimates, as they are even more overwhelmingly White than previous reporting would suggest. Recall that the 2016 LEMAS estimated that among local agency chiefs, 89.6% were White, 4% Black, 3.1% Hispanic, and 2.4% other race. The differences here suggest more analysis is needed, but several obvious options present themselves. It may be there are substantial gaps between the sampling in the LEMAS versus a population-level estimate. Alternatively, the probabilities themselves are skewing towards White likelihoods. The inclusion of more than just local agencies in this analysis also deserves some thought, as there may be agency characteristics that lead to higher proportions of non-Whites to be selected for the top job.\nSome of the gaps are too large to comfortably chalk up to sampling or research design. The 2016 LEMAS estimated that in agencies serving over 250,000 people, just 65% of chiefs were White, while the current analysis would suggest this number is between 92-96%. That large of a gap is a strong suggestion that the inference of race for this population is questionable. On the other hand, the gender inferences seem much more stable across this analysis and previous ones.\nAs always, lots of warnings here about how seriously we should take these estimates. They are, after all, based on probabilistic inferences about race and gender given only a first name. There are lots of weaknesses to consider in that approach. On the other hand, this gives a much broader look at nearly the entire population of US law enforcement agencies in their respective categories (municipal, sheriff’s, campus, and state law enforcement).\nMany thanks to Jacob Kaplan, who developed the predictrace package for R, as this quick analysis would not be possible without his hard work.\n\r\r","date":1625270400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625357447,"objectID":"be4ee0fd2f9da2df84a781d4f2cf63ef","permalink":"https://ianadamsresearch.com/post/developing-race-and-gender-estimates-for-us-law-enforcement-leadership/","publishdate":"2021-07-03T00:00:00Z","relpermalink":"/post/developing-race-and-gender-estimates-for-us-law-enforcement-leadership/","section":"post","summary":"Introduction\rResearchers might be interested in developing a descriptive understanding of the gender and race composition of a particular industry, organization, or other institution. Oftentimes this is done with sampling from a population.","tags":["police","stats"],"title":"Developing Race and Gender Estimates for US Law Enforcement Leadership","type":"post"},{"authors":[],"categories":[],"content":"\r\rHow popular is your name?\rI really liked this simple ggplot2 exercise from Jenna Eagleson that I stumbled across today. I’m going to reproduce it here, and I think it’s a useful exercise for students who are still learning the tidyverse and ggplot2 packages to play around with. I know I get bored with diamonds, and even palmerspenguins, so it’s good to throw something else into the mix to keep the learners’ minds engaged.\nFirst Steps\rWe’ll be using just two packages today - so make sure you have both tidyverse and the babynames packages loaded up (and installed if this is the first time you’ve encountered them).\nlibrary(tidyverse)\rlibrary(babynames)\rYou might not be familiar with the babynames package, but it contains a very large data frame containing 1,924,665 entries of all names used at least five times from 1880 thru 2017. Here’s how Jenna describes it:\n\rThe babynames package has a data frame provided by the Social Security Administration with: year, sex, name, n (number of instances), and prop (number of instances of given name and gender in that year divided by total applicants). Unfortunately, this data only has binary male/female as sex options. This data set includes every name with at least 5 instances!\n\r\rInitial Plotting\rOur most basic plot takes all the names and plots them over time. Keep in mind that the dataset we’re working with is very large, so this plot might take a while to generate!\nbabynames %\u0026gt;%\rggplot() +\rgeom_point(mapping = aes(x = year, y = n))\r\rBut What About Your Name?\rThe babynames package lets us tease out specific names. For now, let’s assign your name and sex to some variables that we can then plug into the plot. I’ll use my info here, but replace with whatever combination of name and sex you are interested in!\nmyname \u0026lt;- \u0026quot;Ian\u0026quot;\rmysex \u0026lt;- \u0026quot;M\u0026quot;\rNow let’s create a plot using those parameters to see how common the name has been over time:\nbabynames %\u0026gt;%\rfilter(name == myname, sex == mysex) %\u0026gt;%\rggplot() +\rgeom_point(mapping = aes(x = year, y = n))\rThere you go! I was born in 1978, so it looks like I got in on the name before it was too cool :)\n\rFurther Steps\rBut what if I want to see the distribution of my name’s popularity plotted against all other names? Good question, and here’s one way to go about it:\nmynameis \u0026lt;- \u0026quot;Ian\u0026quot;\rmysexis \u0026lt;- \u0026quot;M\u0026quot;\rmyname \u0026lt;- babynames %\u0026gt;%\rfilter(name == mynameis, sex == mysexis)\rmynameminyear \u0026lt;- min(myname$year)-5\rmaxyear \u0026lt;- max(babynames$year)\rbabynames %\u0026gt;%\rfilter(year \u0026gt; mynameminyear) %\u0026gt;%\rggplot() +\rgeom_point(mapping = aes(x = year, y = prop), alpha = 0.2, color = \u0026quot;gray\u0026quot;) +\rgeom_point(data = myname, mapping = aes(x = year, y = prop), alpha = 0.8, color = \u0026quot;#013175\u0026quot;) +\r# the below is just formatting, not required! theme_minimal() +\rtheme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(),\raxis.title = element_blank(),\raxis.text.y = element_blank(),\raxis.ticks.y = element_blank()) +\rggtitle(paste(\u0026quot;Popularity of the name \u0026quot;, mynameis, \u0026quot; from \u0026quot;, mynameminyear, \u0026quot; to \u0026quot;, maxyear))\rCool!\n\rPlotting Multiple Names\rMaybe you want to compare names with your siblings or your children - easily done. In this example we’ll be comparing three names, but the example could be expanded to however many you want!\nname_one \u0026lt;- \u0026quot;Ian\u0026quot;\rsex_one \u0026lt;- \u0026quot;M\u0026quot;\rname_two \u0026lt;- \u0026quot;Annette\u0026quot;\rsex_two \u0026lt;- \u0026quot;F\u0026quot;\rname_three \u0026lt;- \u0026quot;Nancy\u0026quot;\rsex_three \u0026lt;- \u0026quot;F\u0026quot;\rWith the names set, now we can plot. You might start to see patterns to what we’ve been doing before. That’s good - one of the advantages of ggplot2 is that it brings a “grammar of graphics” to R, meaning we should be able to take separate pieces from different places and put them into new contexts.\nfirstname \u0026lt;- babynames %\u0026gt;%\rfilter(name == name_one, sex == sex_one)\rsecondname \u0026lt;- babynames %\u0026gt;%\rfilter(name == name_two, sex == sex_two)\rthirdname \u0026lt;- babynames %\u0026gt;%\rfilter(name == name_three, sex == sex_three)\rlegendcolors \u0026lt;- c(\u0026quot;name_one\u0026quot; = \u0026quot;#219EBC\u0026quot;, \u0026quot;name_two\u0026quot; = \u0026quot;#FB8500\u0026quot;, \u0026quot;name_three\u0026quot; = \u0026quot;#023047\u0026quot;)\rbabynames %\u0026gt;%\rggplot() +\rgeom_point(mapping = aes(x = year, y = prop), alpha = 0.1, color = \u0026quot;gray\u0026quot;) +\rgeom_point(data = firstname, mapping = aes(x = year, y = prop, color = \u0026quot;name_one\u0026quot;), alpha = 0.8) +\rgeom_point(data = secondname, mapping = aes(x = year, y = prop, color = \u0026quot;name_two\u0026quot;), alpha = 0.8) +\rgeom_point(data = thirdname, mapping = aes(x = year, y = prop, color = \u0026quot;name_three\u0026quot;), alpha = 0.8) +\r# The below is formatting and not required!\rtheme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(),\raxis.title = element_blank(),\raxis.text.y = element_blank(),\raxis.ticks.y = element_blank()) +\rggtitle(paste(\u0026quot;Who has the most popular name?\u0026quot;)) +\rscale_color_manual(name = \u0026quot;Name\u0026quot;, values = legendcolors, labels = c(\u0026quot;Ian\u0026quot;, \u0026quot;Nancy\u0026quot;, \u0026quot;Annette\u0026quot;))\r\r\r","date":1616976000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617061055,"objectID":"691d3cbcbfdcbfb1db42f0efa0c53786","permalink":"https://ianadamsresearch.com/post/using-ggplot2-to-visualize-the-frequency-of-your-name/","publishdate":"2021-03-29T00:00:00Z","relpermalink":"/post/using-ggplot2-to-visualize-the-frequency-of-your-name/","section":"post","summary":"How popular is your name?\rI really liked this simple ggplot2 exercise from Jenna Eagleson that I stumbled across today. I’m going to reproduce it here, and I think it’s a useful exercise for students who are still learning the tidyverse and ggplot2 packages to play around with.","tags":[],"title":"Using ggplot2 to visualize the frequency of your name!","type":"post"},{"authors":null,"categories":["R"],"content":"\r\r\rSo you want to be a data scientist\rThis week can feel like a bit of a doozy, as the difficulty really ramps up. Something to remember - most data scientists spend most of their time cleaning, transforming, and tidying their data. That’s what these chapters and questions are all about. So I want to make sure you have the correct answers, but more importantly, I want you to know you are not alone, this stuff is hard! I’ve put the quote in front of you before, but this little bit of wisdom from Hadley Wickham, lead author of our class textbook, is important during times like we’re having right now:\n\r“There is no way of going from knowing nothing about a subject to knowing something about a subject without going through a period of much frustration and suckiness. Push through. You’ll suck less.”\n\rNow, onto the answers!\nExercise 5.2.4\r\rWhy is NA ^ 0 not missing? Why is NA | TRUE not missing?\rWhy is FALSE \u0026amp; NA not missing? Can you figure out the general rule?\r(NA * 0 is a tricky counterexample!)\n\r\rNA ^ 0\r## [1] 1\rNA ^ 0 == 1 since for all numeric values \\(x ^ 0 = 1\\).\nNA | TRUE\r## [1] TRUE\rNA | TRUE is TRUE because anything or TRUE is TRUE.\rIf the missing value were TRUE, then TRUE | TRUE == TRUE,\rand if the missing value was FALSE, then FALSE | TRUE == TRUE.\nNA \u0026amp; FALSE\r## [1] FALSE\rThe value of NA \u0026amp; FALSE is FALSE because anything and FALSE is always FALSE.\rIf the missing value were TRUE, then TRUE \u0026amp; FALSE == FALSE,\rand if the missing value was FALSE, then FALSE \u0026amp; FALSE == FALSE.\nNA | FALSE\r## [1] NA\rFor NA | FALSE, the value is unknown since TRUE | FALSE == TRUE, but FALSE | FALSE == FALSE.\nNA \u0026amp; TRUE\r## [1] NA\rFor NA \u0026amp; TRUE, the value is unknown since FALSE \u0026amp; TRUE== FALSE, but TRUE \u0026amp; TRUE == TRUE.\nNA * 0\r## [1] NA\rSince \\(x * 0 = 0\\) for all finite numbers we might expect NA * 0 == 0, but that’s not the case.\rThe reason that NA * 0 != 0 is that \\(0 \\times \\infty\\) and \\(0 \\times -\\infty\\) are undefined.\rR represents undefined results as NaN, which is an abbreviation of “not a number”.\nInf * 0\r## [1] NaN\r-Inf * 0\r## [1] NaN\r\r\rExercise 5.3.1\r\rHow could you use arrange() to sort all missing values to the start? (Hint: use is.na()).\n\r\rThe arrange() function puts NA values last.\narrange(flights, dep_time) %\u0026gt;%\rtail()\r## # A tibble: 6 x 19\r## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time\r## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r## 1 2013 9 30 NA 1842 NA NA 2019\r## 2 2013 9 30 NA 1455 NA NA 1634\r## 3 2013 9 30 NA 2200 NA NA 2312\r## 4 2013 9 30 NA 1210 NA NA 1330\r## 5 2013 9 30 NA 1159 NA NA 1344\r## 6 2013 9 30 NA 840 NA NA 1020\r## # ... with 11 more variables: arr_delay \u0026lt;dbl\u0026gt;, carrier \u0026lt;chr\u0026gt;, flight \u0026lt;int\u0026gt;,\r## # tailnum \u0026lt;chr\u0026gt;, origin \u0026lt;chr\u0026gt;, dest \u0026lt;chr\u0026gt;, air_time \u0026lt;dbl\u0026gt;, distance \u0026lt;dbl\u0026gt;,\r## # hour \u0026lt;dbl\u0026gt;, minute \u0026lt;dbl\u0026gt;, time_hour \u0026lt;dttm\u0026gt;\rUsing desc() does not change that.\narrange(flights, desc(dep_time))\r## # A tibble: 336,776 x 19\r## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time\r## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r## 1 2013 10 30 2400 2359 1 327 337\r## 2 2013 11 27 2400 2359 1 515 445\r## 3 2013 12 5 2400 2359 1 427 440\r## 4 2013 12 9 2400 2359 1 432 440\r## 5 2013 12 9 2400 2250 70 59 2356\r## 6 2013 12 13 2400 2359 1 432 440\r## 7 2013 12 19 2400 2359 1 434 440\r## 8 2013 12 29 2400 1700 420 302 2025\r## 9 2013 2 7 2400 2359 1 432 436\r## 10 2013 2 7 2400 2359 1 443 444\r## # ... with 336,766 more rows, and 11 more variables: arr_delay \u0026lt;dbl\u0026gt;,\r## # carrier \u0026lt;chr\u0026gt;, flight \u0026lt;int\u0026gt;, tailnum \u0026lt;chr\u0026gt;, origin \u0026lt;chr\u0026gt;, dest \u0026lt;chr\u0026gt;,\r## # air_time \u0026lt;dbl\u0026gt;, distance \u0026lt;dbl\u0026gt;, hour \u0026lt;dbl\u0026gt;, minute \u0026lt;dbl\u0026gt;, time_hour \u0026lt;dttm\u0026gt;\rTo put NA values first, we can add an indicator of whether the column has a missing value.\rThen we sort by the missing indicator column and the column of interest.\rFor example, to sort the data frame by departure time (dep_time) in ascending order but NA values first, run the following.\narrange(flights, desc(is.na(dep_time)), dep_time)\r## # A tibble: 336,776 x 19\r## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time\r## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r## 1 2013 1 1 NA 1630 NA NA 1815\r## 2 2013 1 1 NA 1935 NA NA 2240\r## 3 2013 1 1 NA 1500 NA NA 1825\r## 4 2013 1 1 NA 600 NA NA 901\r## 5 2013 1 2 NA 1540 NA NA 1747\r## 6 2013 1 2 NA 1620 NA NA 1746\r## 7 2013 1 2 NA 1355 NA NA 1459\r## 8 2013 1 2 NA 1420 NA NA 1644\r## 9 2013 1 2 NA 1321 NA NA 1536\r## 10 2013 1 2 NA 1545 NA NA 1910\r## # ... with 336,766 more rows, and 11 more variables: arr_delay \u0026lt;dbl\u0026gt;,\r## # carrier \u0026lt;chr\u0026gt;, flight \u0026lt;int\u0026gt;, tailnum \u0026lt;chr\u0026gt;, origin \u0026lt;chr\u0026gt;, dest \u0026lt;chr\u0026gt;,\r## # air_time \u0026lt;dbl\u0026gt;, distance \u0026lt;dbl\u0026gt;, hour \u0026lt;dbl\u0026gt;, minute \u0026lt;dbl\u0026gt;, time_hour \u0026lt;dttm\u0026gt;\rThe flights will first be sorted by desc(is.na(dep_time)).\rSince desc(is.na(dep_time)) is either TRUE when dep_time is missing, or FALSE, when it is not, the rows with missing values of dep_time will come first, since TRUE \u0026gt; FALSE.\n\r\rExercise 5.4.1\r\rBrainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights.\n\r\rThese are a few ways to select columns.\n\rSpecify columns names as unquoted variable names.\nselect(flights, dep_time, dep_delay, arr_time, arr_delay)\r## # A tibble: 336,776 x 4\r## dep_time dep_delay arr_time arr_delay\r## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 517 2 830 11\r## 2 533 4 850 20\r## 3 542 2 923 33\r## 4 544 -1 1004 -18\r## 5 554 -6 812 -25\r## 6 554 -4 740 12\r## 7 555 -5 913 19\r## 8 557 -3 709 -14\r## 9 557 -3 838 -8\r## 10 558 -2 753 8\r## # ... with 336,766 more rows\rSpecify column names as strings.\nselect(flights, \u0026quot;dep_time\u0026quot;, \u0026quot;dep_delay\u0026quot;, \u0026quot;arr_time\u0026quot;, \u0026quot;arr_delay\u0026quot;)\r## # A tibble: 336,776 x 4\r## dep_time dep_delay arr_time arr_delay\r## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 517 2 830 11\r## 2 533 4 850 20\r## 3 542 2 923 33\r## 4 544 -1 1004 -18\r## 5 554 -6 812 -25\r## 6 554 -4 740 12\r## 7 555 -5 913 19\r## 8 557 -3 709 -14\r## 9 557 -3 838 -8\r## 10 558 -2 753 8\r## # ... with 336,766 more rows\rSpecify the column numbers of the variables.\nselect(flights, 4, 6, 7, 9)\r## # A tibble: 336,776 x 4\r## dep_time dep_delay arr_time arr_delay\r## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 517 2 830 11\r## 2 533 4 850 20\r## 3 542 2 923 33\r## 4 544 -1 1004 -18\r## 5 554 -6 812 -25\r## 6 554 -4 740 12\r## 7 555 -5 913 19\r## 8 557 -3 709 -14\r## 9 557 -3 838 -8\r## 10 558 -2 753 8\r## # ... with 336,766 more rows\rThis works, but is not good practice for two reasons.\rFirst, the column location of variables may change, resulting in code that\rmay continue to run without error, but produce the wrong answer.\rSecond code is obfuscated, since it is not clear from the code which\rvariables are being selected. What variable does column 6 correspond to?\rI just wrote the code, and I’ve already forgotten.\n\rSpecify the names of the variables with character vector and any_of() or all_of()\nselect(flights, all_of(c(\u0026quot;dep_time\u0026quot;, \u0026quot;dep_delay\u0026quot;, \u0026quot;arr_time\u0026quot;, \u0026quot;arr_delay\u0026quot;)))\r## # A tibble: 336,776 x 4\r## dep_time dep_delay arr_time arr_delay\r## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 517 2 830 11\r## 2 533 4 850 20\r## 3 542 2 923 33\r## 4 544 -1 1004 -18\r## 5 554 -6 812 -25\r## 6 554 -4 740 12\r## 7 555 -5 913 19\r## 8 557 -3 709 -14\r## 9 557 -3 838 -8\r## 10 558 -2 753 8\r## # ... with 336,766 more rows\rselect(flights, any_of(c(\u0026quot;dep_time\u0026quot;, \u0026quot;dep_delay\u0026quot;, \u0026quot;arr_time\u0026quot;, \u0026quot;arr_delay\u0026quot;)))\r## # A tibble: 336,776 x 4\r## dep_time dep_delay arr_time arr_delay\r## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 517 2 830 11\r## 2 533 4 850 20\r## 3 542 2 923 33\r## 4 544 -1 1004 -18\r## 5 554 -6 812 -25\r## 6 554 -4 740 12\r## 7 555 -5 913 19\r## 8 557 -3 709 -14\r## 9 557 -3 838 -8\r## 10 558 -2 753 8\r## # ... with 336,766 more rows\rThis is useful because the names of the variables can be stored in a\rvariable and passed to all_of() or any_of().\nvariables \u0026lt;- c(\u0026quot;dep_time\u0026quot;, \u0026quot;dep_delay\u0026quot;, \u0026quot;arr_time\u0026quot;, \u0026quot;arr_delay\u0026quot;)\rselect(flights, all_of(variables))\r## # A tibble: 336,776 x 4\r## dep_time dep_delay arr_time arr_delay\r## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 517 2 830 11\r## 2 533 4 850 20\r## 3 542 2 923 33\r## 4 544 -1 1004 -18\r## 5 554 -6 812 -25\r## 6 554 -4 740 12\r## 7 555 -5 913 19\r## 8 557 -3 709 -14\r## 9 557 -3 838 -8\r## 10 558 -2 753 8\r## # ... with 336,766 more rows\rThese two functions replace the deprecated function one_of().\n\rSelecting the variables by matching the start of their names using starts_with().\nselect(flights, starts_with(\u0026quot;dep_\u0026quot;), starts_with(\u0026quot;arr_\u0026quot;))\r## # A tibble: 336,776 x 4\r## dep_time dep_delay arr_time arr_delay\r## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 517 2 830 11\r## 2 533 4 850 20\r## 3 542 2 923 33\r## 4 544 -1 1004 -18\r## 5 554 -6 812 -25\r## 6 554 -4 740 12\r## 7 555 -5 913 19\r## 8 557 -3 709 -14\r## 9 557 -3 838 -8\r## 10 558 -2 753 8\r## # ... with 336,766 more rows\rSelecting the variables using regular expressions with matches().\rRegular expressions provide a flexible way to match string patterns\rand are discussed in the Strings chapter.\nselect(flights, matches(\u0026quot;^(dep|arr)_(time|delay)$\u0026quot;))\r## # A tibble: 336,776 x 4\r## dep_time dep_delay arr_time arr_delay\r## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 517 2 830 11\r## 2 533 4 850 20\r## 3 542 2 923 33\r## 4 544 -1 1004 -18\r## 5 554 -6 812 -25\r## 6 554 -4 740 12\r## 7 555 -5 913 19\r## 8 557 -3 709 -14\r## 9 557 -3 838 -8\r## 10 558 -2 753 8\r## # ... with 336,766 more rows\rSpecify the names of the variables with a character vector and use the bang-bang operator (!!).\nvariables \u0026lt;- c(\u0026quot;dep_time\u0026quot;, \u0026quot;dep_delay\u0026quot;, \u0026quot;arr_time\u0026quot;, \u0026quot;arr_delay\u0026quot;)\rselect(flights, !!variables)\r## # A tibble: 336,776 x 4\r## dep_time dep_delay arr_time arr_delay\r## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 517 2 830 11\r## 2 533 4 850 20\r## 3 542 2 923 33\r## 4 544 -1 1004 -18\r## 5 554 -6 812 -25\r## 6 554 -4 740 12\r## 7 555 -5 913 19\r## 8 557 -3 709 -14\r## 9 557 -3 838 -8\r## 10 558 -2 753 8\r## # ... with 336,766 more rows\rThis and the following answers use the features of tidy evaluation not covered in R4DS but covered in the Programming with dplyr vignette.\n\rSpecify the names of the variables in a character or list vector and use the bang-bang-bang operator.\nvariables \u0026lt;- c(\u0026quot;dep_time\u0026quot;, \u0026quot;dep_delay\u0026quot;, \u0026quot;arr_time\u0026quot;, \u0026quot;arr_delay\u0026quot;)\rselect(flights, !!!variables)\r## # A tibble: 336,776 x 4\r## dep_time dep_delay arr_time arr_delay\r## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 517 2 830 11\r## 2 533 4 850 20\r## 3 542 2 923 33\r## 4 544 -1 1004 -18\r## 5 554 -6 812 -25\r## 6 554 -4 740 12\r## 7 555 -5 913 19\r## 8 557 -3 709 -14\r## 9 557 -3 838 -8\r## 10 558 -2 753 8\r## # ... with 336,766 more rows\rSpecify the unquoted names of the variables in a list using syms() and use the bang-bang-bang operator.\nvariables \u0026lt;- syms(c(\u0026quot;dep_time\u0026quot;, \u0026quot;dep_delay\u0026quot;, \u0026quot;arr_time\u0026quot;, \u0026quot;arr_delay\u0026quot;))\rselect(flights, !!!variables)\r## # A tibble: 336,776 x 4\r## dep_time dep_delay arr_time arr_delay\r## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 517 2 830 11\r## 2 533 4 850 20\r## 3 542 2 923 33\r## 4 544 -1 1004 -18\r## 5 554 -6 812 -25\r## 6 554 -4 740 12\r## 7 555 -5 913 19\r## 8 557 -3 709 -14\r## 9 557 -3 838 -8\r## 10 558 -2 753 8\r## # ... with 336,766 more rows\r\rSome things that don’t work are:\n\rMatching the ends of their names using ends_with() since this will incorrectly\rinclude other variables. For example,\nselect(flights, ends_with(\u0026quot;arr_time\u0026quot;), ends_with(\u0026quot;dep_time\u0026quot;))\r## # A tibble: 336,776 x 4\r## arr_time sched_arr_time dep_time sched_dep_time\r## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r## 1 830 819 517 515\r## 2 850 830 533 529\r## 3 923 850 542 540\r## 4 1004 1022 544 545\r## 5 812 837 554 600\r## 6 740 728 554 558\r## 7 913 854 555 600\r## 8 709 723 557 600\r## 9 838 846 557 600\r## 10 753 745 558 600\r## # ... with 336,766 more rows\rMatching the names using contains() since there is not a pattern that can\rinclude all these variables without incorrectly including others.\nselect(flights, contains(\u0026quot;_time\u0026quot;), contains(\u0026quot;arr_\u0026quot;))\r## # A tibble: 336,776 x 6\r## dep_time sched_dep_time arr_time sched_arr_time air_time arr_delay\r## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 517 515 830 819 227 11\r## 2 533 529 850 830 227 20\r## 3 542 540 923 850 160 33\r## 4 544 545 1004 1022 183 -18\r## 5 554 600 812 837 116 -25\r## 6 554 558 740 728 150 12\r## 7 555 600 913 854 158 19\r## 8 557 600 709 723 53 -14\r## 9 557 600 838 846 140 -8\r## 10 558 600 753 745 138 8\r## # ... with 336,766 more rows\r\r\r\rExercise 5.5.2\r\rCompare air_time with arr_time - dep_time.\rWhat do you expect to see?\rWhat do you see?\rWhat do you need to do to fix it?\n\r\rI expect that air_time is the difference between the arrival (arr_time) and departure times (dep_time).\rIn other words, air_time = arr_time - dep_time.\nTo check that this relationship, I’ll first need to convert the times to a form more amenable to arithmetic operations using the same calculations as the previous exercise.\nflights_airtime \u0026lt;-\rmutate(flights,\rdep_time = (dep_time %/% 100 * 60 + dep_time %% 100) %% 1440,\rarr_time = (arr_time %/% 100 * 60 + arr_time %% 100) %% 1440,\rair_time_diff = air_time - arr_time + dep_time\r)\rSo, does air_time = arr_time - dep_time?\rIf so, there should be no flights with non-zero values of air_time_diff.\nnrow(filter(flights_airtime, air_time_diff != 0))\r## [1] 327150\rIt turns out that there are many flights for which air_time != arr_time - dep_time.\rOther than data errors, I can think of two reasons why air_time would not equal arr_time - dep_time.\nThe flight passes midnight, so arr_time \u0026lt; dep_time.\rIn these cases, the difference in airtime should be by 24 hours (1,440 minutes).\n\rThe flight crosses time zones, and the total air time will be off by hours (multiples of 60).\rAll flights in flights departed from New York City and are domestic flights in the US.\rThis means that flights will all be to the same or more westerly time zones.\rGiven the time-zones in the US, the differences due to time-zone should be 60 minutes (Central)\r120 minutes (Mountain), 180 minutes (Pacific), 240 minutes (Alaska), or 300 minutes (Hawaii).\n\r\rBoth of these explanations have clear patterns that I would expect to see if they\rwere true.\rIn particular, in both cases, since time-zones and crossing midnight only affects the hour part of the time, all values of air_time_diff should be divisible by 60.\rI’ll visually check this hypothesis by plotting the distribution of air_time_diff.\rIf those two explanations are correct, distribution of air_time_diff should comprise only spikes at multiples of 60.\nggplot(flights_airtime, aes(x = air_time_diff)) +\rgeom_histogram(binwidth = 1)\r## Warning: Removed 9430 rows containing non-finite values (stat_bin).\rThis is not the case.\rWhile, the distribution of air_time_diff has modes at multiples of 60 as hypothesized,\rit shows that there are many flights in which the difference between air time and local arrival and departure times is not divisible by 60.\nLet’s also look at flights with Los Angeles as a destination.\rThe discrepancy should be 180 minutes.\nggplot(filter(flights_airtime, dest == \u0026quot;LAX\u0026quot;), aes(x = air_time_diff)) +\rgeom_histogram(binwidth = 1)\r## Warning: Removed 148 rows containing non-finite values (stat_bin).\rTo fix these time-zone issues, I would want to convert all the times to a date-time to handle overnight flights, and from local time to a common time zone, most likely UTC, to handle flights crossing time-zones.\rThe tzone column of nycflights13::airports gives the time-zone of each airport.\rSee the “Dates and Times” for an introduction on working with date and time data.\nBut that still leaves the other differences unexplained.\rSo what else might be going on?\rThere seem to be too many problems for this to be data entry problems, so I’m probably missing something.\rSo, I’ll reread the documentation to make sure that I understand the definitions of arr_time, dep_time, and\rair_time.\rThe documentation contains a link to the source of the flights data, https://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236.\rThis documentation shows that the flights data does not contain the variables TaxiIn, TaxiOff, WheelsIn, and WheelsOff.\rIt appears that the air_time variable refers to flight time, which is defined as the time between wheels-off (take-off) and wheels-in (landing).\rBut the flight time does not include time spent on the runway taxiing to and from gates.\rWith this new understanding of the data, I now know that the relationship between air_time, arr_time, and dep_time is air_time \u0026lt;= arr_time - dep_time, supposing that the time zones of arr_time and dep_time are in the same time zone.\n\r\rExercise 5.6.7\r\rBrainstorm at least 5 different ways to assess the typical delay characteristics of a group of flights.\rConsider the following scenarios:\n\rA flight is 15 minutes early 50% of the time, and 15 minutes late 50% of the time.\rA flight is always 10 minutes late.\rA flight is 30 minutes early 50% of the time, and 30 minutes late 50% of the time.\r99% of the time a flight is on time. 1% of the time it’s 2 hours late.\r\rWhich is more important: arrival delay or departure delay?\n\r\rWhat this question gets at is a fundamental question of data analysis: the cost function.\rAs analysts, the reason we are interested in flight delay because it is costly to passengers.\rBut it is worth thinking carefully about how it is costly and use that information in ranking and measuring these scenarios.\nIn many scenarios, arrival delay is more important.\rIn most cases, being arriving late is more costly to the passenger since it could disrupt the next stages of their travel, such as connecting flights or scheduled meetings.\nIf a departure is delayed without affecting the arrival time, this delay will not have those affects plans nor does it affect the total time spent traveling.\rThis delay could be beneficial, if less time is spent in the cramped confines of the airplane itself, or a negative, if that delayed time is still spent in the cramped confines of the airplane on the runway.\nVariation in arrival time is worse than consistency.\rIf a flight is always 30 minutes late and that delay is known, then it is as if the arrival time is that delayed time.\rThe traveler could easily plan for this.\rBut higher variation in flight times makes it harder to plan.\n5.6.7.2\r\rCome up with another approach that will give you the same output as not_cancelled %\u0026gt;% count(dest) and not_cancelled %\u0026gt;% count(tailnum, wt = distance) (without using count()).\n\r\rnot_cancelled \u0026lt;- flights %\u0026gt;%\rfilter(!is.na(dep_delay), !is.na(arr_delay))\rThe first expression is the following.\nnot_cancelled %\u0026gt;% count(dest)\r## # A tibble: 104 x 2\r## dest n\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 ABQ 254\r## 2 ACK 264\r## 3 ALB 418\r## 4 ANC 8\r## 5 ATL 16837\r## 6 AUS 2411\r## 7 AVL 261\r## 8 BDL 412\r## 9 BGR 358\r## 10 BHM 269\r## # ... with 94 more rows\rThe count() function counts the number of instances within each group of variables.\rInstead of using the count() function, we can combine the group_by() and summarise() verbs.\nnot_cancelled %\u0026gt;%\rgroup_by(dest) %\u0026gt;%\rsummarise(n = length(dest))\r## # A tibble: 104 x 2\r## dest n\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 ABQ 254\r## 2 ACK 264\r## 3 ALB 418\r## 4 ANC 8\r## 5 ATL 16837\r## 6 AUS 2411\r## 7 AVL 261\r## 8 BDL 412\r## 9 BGR 358\r## 10 BHM 269\r## # ... with 94 more rows\rAn alternative method for getting the number of observations in a data frame is the function n().\nnot_cancelled %\u0026gt;%\rgroup_by(dest) %\u0026gt;%\rsummarise(n = n())\r## # A tibble: 104 x 2\r## dest n\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 ABQ 254\r## 2 ACK 264\r## 3 ALB 418\r## 4 ANC 8\r## 5 ATL 16837\r## 6 AUS 2411\r## 7 AVL 261\r## 8 BDL 412\r## 9 BGR 358\r## 10 BHM 269\r## # ... with 94 more rows\rAnother alternative to count() is to use group_by() followed by tally().\rIn fact, count() is effectively a short-cut for group_by() followed by tally().\nnot_cancelled %\u0026gt;%\rgroup_by(tailnum) %\u0026gt;%\rtally()\r## # A tibble: 4,037 x 2\r## tailnum n\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 D942DN 4\r## 2 N0EGMQ 352\r## 3 N10156 145\r## 4 N102UW 48\r## 5 N103US 46\r## 6 N104UW 46\r## 7 N10575 269\r## 8 N105UW 45\r## 9 N107US 41\r## 10 N108UW 60\r## # ... with 4,027 more rows\rThe second expression also uses the count() function, but adds a wt argument.\nnot_cancelled %\u0026gt;% count(tailnum, wt = distance)\r## # A tibble: 4,037 x 2\r## tailnum n\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 D942DN 3418\r## 2 N0EGMQ 239143\r## 3 N10156 109664\r## 4 N102UW 25722\r## 5 N103US 24619\r## 6 N104UW 24616\r## 7 N10575 139903\r## 8 N105UW 23618\r## 9 N107US 21677\r## 10 N108UW 32070\r## # ... with 4,027 more rows\rAs before, we can replicate count() by combining the group_by() and summarise() verbs.\rBut this time instead of using length(), we will use sum() with the weighting variable.\nnot_cancelled %\u0026gt;%\rgroup_by(tailnum) %\u0026gt;%\rsummarise(n = sum(distance))\r## # A tibble: 4,037 x 2\r## tailnum n\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 D942DN 3418\r## 2 N0EGMQ 239143\r## 3 N10156 109664\r## 4 N102UW 25722\r## 5 N103US 24619\r## 6 N104UW 24616\r## 7 N10575 139903\r## 8 N105UW 23618\r## 9 N107US 21677\r## 10 N108UW 32070\r## # ... with 4,027 more rows\rLike the previous example, we can also use the combination group_by() and tally().\rAny arguments to tally() are summed.\nnot_cancelled %\u0026gt;%\rgroup_by(tailnum) %\u0026gt;%\rtally(distance)\r## # A tibble: 4,037 x 2\r## tailnum n\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 D942DN 3418\r## 2 N0EGMQ 239143\r## 3 N10156 109664\r## 4 N102UW 25722\r## 5 N103US 24619\r## 6 N104UW 24616\r## 7 N10575 139903\r## 8 N105UW 23618\r## 9 N107US 21677\r## 10 N108UW 32070\r## # ... with 4,027 more rows\r\r\r5.6.7.3\r\rOur definition of cancelled flights (is.na(dep_delay) | is.na(arr_delay)) is slightly suboptimal.\rWhy?\rWhich is the most important column?\n\r\rIf a flight never departs, then it won’t arrive.\rA flight could also depart and not arrive if it crashes, or if it is redirected and lands in an airport other than its intended destination.\rSo the most important column is arr_delay, which indicates the amount of delay in arrival.\nfilter(flights, !is.na(dep_delay), is.na(arr_delay)) %\u0026gt;%\rselect(dep_time, arr_time, sched_arr_time, dep_delay, arr_delay)\r## # A tibble: 1,175 x 5\r## dep_time arr_time sched_arr_time dep_delay arr_delay\r## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1525 1934 1805 -5 NA\r## 2 1528 2002 1647 29 NA\r## 3 1740 2158 2020 -5 NA\r## 4 1807 2251 2103 29 NA\r## 5 1939 29 2151 59 NA\r## 6 1952 2358 2207 22 NA\r## 7 2016 NA 2220 46 NA\r## 8 905 1313 1045 43 NA\r## 9 1125 1445 1146 120 NA\r## 10 1848 2333 2151 8 NA\r## # ... with 1,165 more rows\rIn this data dep_time can be non-missing and arr_delay missing but arr_time not missing.\rSome further research found that these rows correspond to diverted flights.\rThe BTS database that is the source for the flights table contains additional information for diverted flights that is not included in the nycflights13 data.\rThe source contains a column DivArrDelay with the description:\n\rDifference in minutes between scheduled and actual arrival time for a diverted flight reaching scheduled destination.\rThe ArrDelay column remains NULL for all diverted flights.\n\r\r\r5.6.7.4\r\rLook at the number of cancelled flights per day.\rIs there a pattern?\rIs the proportion of cancelled flights related to the average delay?\n\r\rOne pattern in cancelled flights per day is that the number of cancelled flights increases with the total number of flights per day.\rThe proportion of cancelled flights increases with the average delay of flights.\nTo answer these questions, use definition of cancelled used in the\rchapter Section 5.6.3 and the\rrelationship !(is.na(arr_delay) \u0026amp; is.na(dep_delay)) is equal to\r!is.na(arr_delay) | !is.na(dep_delay) by De Morgan’s law.\nThe first part of the question asks for any pattern in the number of cancelled flights per day.\rI’ll look at the relationship between the number of cancelled flights per day and the total number of flights in a day.\rThere should be an increasing relationship for two reasons.\rFirst, if all flights are equally likely to be cancelled, then days with more flights should have a higher number of cancellations.\rSecond, it is likely that days with more flights would have a higher probability of cancellations because congestion itself can cause delays and any delay would affect more flights, and large delays can lead to cancellations.\ncancelled_per_day \u0026lt;- flights %\u0026gt;%\rmutate(cancelled = (is.na(arr_delay) | is.na(dep_delay))) %\u0026gt;%\rgroup_by(year, month, day) %\u0026gt;%\rsummarise(\rcancelled_num = sum(cancelled),\rflights_num = n(),\r)\r## `summarise()` has grouped output by \u0026#39;year\u0026#39;, \u0026#39;month\u0026#39;. You can override using the `.groups` argument.\rPlotting flights_num against cancelled_num shows that the number of flights\rcancelled increases with the total number of flights.\nggplot(cancelled_per_day) +\rgeom_point(aes(x = flights_num, y = cancelled_num)) \rThe second part of the question asks whether there is a relationship between the proportion of flights cancelled and the average departure delay.\rI implied this in my answer to the first part of the question, when I noted that increasing delays could result in increased cancellations.\rThe question does not specify which delay, so I will show the relationship for both.\ncancelled_and_delays \u0026lt;- flights %\u0026gt;%\rmutate(cancelled = (is.na(arr_delay) | is.na(dep_delay))) %\u0026gt;%\rgroup_by(year, month, day) %\u0026gt;%\rsummarise(\rcancelled_prop = mean(cancelled),\ravg_dep_delay = mean(dep_delay, na.rm = TRUE),\ravg_arr_delay = mean(arr_delay, na.rm = TRUE)\r) %\u0026gt;%\rungroup()\r## `summarise()` has grouped output by \u0026#39;year\u0026#39;, \u0026#39;month\u0026#39;. You can override using the `.groups` argument.\rThere is a strong increasing relationship between both average departure delay and\nand average arrival delay and the proportion of cancelled flights.\nggplot(cancelled_and_delays) +\rgeom_point(aes(x = avg_dep_delay, y = cancelled_prop))\rggplot(cancelled_and_delays) +\rgeom_point(aes(x = avg_arr_delay, y = cancelled_prop))\r\r\r5.6.7.5\r\rWhich carrier has the worst delays?\rChallenge: can you disentangle the effects of bad airports vs. bad carriers?\rWhy/why not?\r(Hint: think about flights %\u0026gt;% group_by(carrier, dest) %\u0026gt;% summarise(n()))\n\r\rflights %\u0026gt;%\rgroup_by(carrier) %\u0026gt;%\rsummarise(arr_delay = mean(arr_delay, na.rm = TRUE)) %\u0026gt;%\rarrange(desc(arr_delay))\r## # A tibble: 16 x 2\r## carrier arr_delay\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 F9 21.9 ## 2 FL 20.1 ## 3 EV 15.8 ## 4 YV 15.6 ## 5 OO 11.9 ## 6 MQ 10.8 ## 7 WN 9.65 ## 8 B6 9.46 ## 9 9E 7.38 ## 10 UA 3.56 ## 11 US 2.13 ## 12 VX 1.76 ## 13 DL 1.64 ## 14 AA 0.364\r## 15 HA -6.92 ## 16 AS -9.93\rWhat airline corresponds to the \"F9\" carrier code?\nfilter(airlines, carrier == \u0026quot;F9\u0026quot;)\r## # A tibble: 1 x 2\r## carrier name ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 F9 Frontier Airlines Inc.\rYou can get part of the way to disentangling the effects of airports versus bad carriers by comparing the average delay of each carrier to the average delay of flights within a route (flights from the same origin to the same destination).\rComparing delays between carriers and within each route disentangles the effect of carriers and airports.\rA better analysis would compare the average delay of a carrier’s flights to the average delay of all other carrier’s flights within a route.\nflights %\u0026gt;%\rfilter(!is.na(arr_delay)) %\u0026gt;%\r# Total delay by carrier within each origin, dest\rgroup_by(origin, dest, carrier) %\u0026gt;%\rsummarise(\rarr_delay = sum(arr_delay),\rflights = n()\r) %\u0026gt;%\r# Total delay within each origin dest\rgroup_by(origin, dest) %\u0026gt;%\rmutate(\rarr_delay_total = sum(arr_delay),\rflights_total = sum(flights)\r) %\u0026gt;%\r# average delay of each carrier - average delay of other carriers\rungroup() %\u0026gt;%\rmutate(\rarr_delay_others = (arr_delay_total - arr_delay) /\r(flights_total - flights),\rarr_delay_mean = arr_delay / flights,\rarr_delay_diff = arr_delay_mean - arr_delay_others\r) %\u0026gt;%\r# remove NaN values (when there is only one carrier)\rfilter(is.finite(arr_delay_diff)) %\u0026gt;%\r# average over all airports it flies to\rgroup_by(carrier) %\u0026gt;%\rsummarise(arr_delay_diff = mean(arr_delay_diff)) %\u0026gt;%\rarrange(desc(arr_delay_diff))\r## `summarise()` has grouped output by \u0026#39;origin\u0026#39;, \u0026#39;dest\u0026#39;. You can override using the `.groups` argument.\r## # A tibble: 15 x 2\r## carrier arr_delay_diff\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 OO 27.3 ## 2 F9 17.3 ## 3 EV 11.0 ## 4 B6 6.41 ## 5 FL 2.57 ## 6 VX -0.202\r## 7 AA -0.970\r## 8 WN -1.27 ## 9 UA -1.86 ## 10 MQ -2.48 ## 11 YV -2.81 ## 12 9E -3.54 ## 13 US -4.14 ## 14 DL -10.2 ## 15 AS -15.8\rThere are more sophisticated ways to do this analysis, however comparing the delay of flights within each route goes a long ways toward disentangling airport and carrier effects.\rTo see a more complete example of this analysis, see this FiveThirtyEight piece.\n\r\r5.6.7.6\r\rWhat does the sort argument to count() do?\rWhen might you use it?\n\r\rThe sort argument to count() sorts the results in order of n.\rYou could use this anytime you would run count() followed by arrange().\nFor example, the following expression counts the number of flights to a destination and sorts the returned data from highest to lowest.\nflights %\u0026gt;%\rcount(dest, sort = TRUE)\r## # A tibble: 105 x 2\r## dest n\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 ORD 17283\r## 2 ATL 17215\r## 3 LAX 16174\r## 4 BOS 15508\r## 5 MCO 14082\r## 6 CLT 14064\r## 7 SFO 13331\r## 8 FLL 12055\r## 9 MIA 11728\r## 10 DCA 9705\r## # ... with 95 more rows\r\r\r\rExercise 5.7.1\r\rRefer back to the lists of useful mutate and filtering functions.\rDescribe how each operation changes when you combine it with grouping.\n\r\rSummary functions (mean()), offset functions (lead(), lag()), ranking functions (min_rank(), row_number()), operate within each group when used with group_by() in\rmutate() or filter().\rArithmetic operators (+, -), logical operators (\u0026lt;, ==), modular arithmetic operators (%%, %/%), logarithmic functions (log) are not affected by group_by.\nSummary functions like mean(), median(), sum(), std() and others covered\rin the section Useful Summary Functions\rcalculate their values within each group when used with mutate() or filter() and group_by().\ntibble(x = 1:9,\rgroup = rep(c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;), each = 3)) %\u0026gt;%\rmutate(x_mean = mean(x)) %\u0026gt;%\rgroup_by(group) %\u0026gt;%\rmutate(x_mean_2 = mean(x))\r## # A tibble: 9 x 4\r## # Groups: group [3]\r## x group x_mean x_mean_2\r## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 a 5 2\r## 2 2 a 5 2\r## 3 3 a 5 2\r## 4 4 b 5 5\r## 5 5 b 5 5\r## 6 6 b 5 5\r## 7 7 c 5 8\r## 8 8 c 5 8\r## 9 9 c 5 8\rArithmetic operators +, -, *, /, ^ are not affected by group_by().\ntibble(x = 1:9,\rgroup = rep(c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;), each = 3)) %\u0026gt;%\rmutate(y = x + 2) %\u0026gt;%\rgroup_by(group) %\u0026gt;%\rmutate(z = x + 2)\r## # A tibble: 9 x 4\r## # Groups: group [3]\r## x group y z\r## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 a 3 3\r## 2 2 a 4 4\r## 3 3 a 5 5\r## 4 4 b 6 6\r## 5 5 b 7 7\r## 6 6 b 8 8\r## 7 7 c 9 9\r## 8 8 c 10 10\r## 9 9 c 11 11\rThe modular arithmetic operators %/% and %% are not affected by group_by()\ntibble(x = 1:9,\rgroup = rep(c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;), each = 3)) %\u0026gt;%\rmutate(y = x %% 2) %\u0026gt;%\rgroup_by(group) %\u0026gt;%\rmutate(z = x %% 2)\r## # A tibble: 9 x 4\r## # Groups: group [3]\r## x group y z\r## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 a 1 1\r## 2 2 a 0 0\r## 3 3 a 1 1\r## 4 4 b 0 0\r## 5 5 b 1 1\r## 6 6 b 0 0\r## 7 7 c 1 1\r## 8 8 c 0 0\r## 9 9 c 1 1\rThe logarithmic functions log(), log2(), and log10() are not affected by\rgroup_by().\ntibble(x = 1:9,\rgroup = rep(c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;), each = 3)) %\u0026gt;%\rmutate(y = log(x)) %\u0026gt;%\rgroup_by(group) %\u0026gt;%\rmutate(z = log(x))\r## # A tibble: 9 x 4\r## # Groups: group [3]\r## x group y z\r## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 a 0 0 ## 2 2 a 0.693 0.693\r## 3 3 a 1.10 1.10 ## 4 4 b 1.39 1.39 ## 5 5 b 1.61 1.61 ## 6 6 b 1.79 1.79 ## 7 7 c 1.95 1.95 ## 8 8 c 2.08 2.08 ## 9 9 c 2.20 2.20\rThe offset functions lead() and lag() respect the groupings in group_by().\rThe functions lag() and lead() will only return values within each group.\ntibble(x = 1:9,\rgroup = rep(c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;), each = 3)) %\u0026gt;%\rgroup_by(group) %\u0026gt;%\rmutate(lag_x = lag(x),\rlead_x = lead(x))\r## # A tibble: 9 x 4\r## # Groups: group [3]\r## x group lag_x lead_x\r## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r## 1 1 a NA 2\r## 2 2 a 1 3\r## 3 3 a 2 NA\r## 4 4 b NA 5\r## 5 5 b 4 6\r## 6 6 b 5 NA\r## 7 7 c NA 8\r## 8 8 c 7 9\r## 9 9 c 8 NA\rThe cumulative and rolling aggregate functions cumsum(), cumprod(), cummin(), cummax(), and cummean() calculate values within each group.\ntibble(x = 1:9,\rgroup = rep(c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;), each = 3)) %\u0026gt;%\rmutate(x_cumsum = cumsum(x)) %\u0026gt;%\rgroup_by(group) %\u0026gt;%\rmutate(x_cumsum_2 = cumsum(x))\r## # A tibble: 9 x 4\r## # Groups: group [3]\r## x group x_cumsum x_cumsum_2\r## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r## 1 1 a 1 1\r## 2 2 a 3 3\r## 3 3 a 6 6\r## 4 4 b 10 4\r## 5 5 b 15 9\r## 6 6 b 21 15\r## 7 7 c 28 7\r## 8 8 c 36 15\r## 9 9 c 45 24\rLogical comparisons, \u0026lt;, \u0026lt;=, \u0026gt;, \u0026gt;=, !=, and == are not affected by group_by().\ntibble(x = 1:9,\ry = 9:1,\rgroup = rep(c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;), each = 3)) %\u0026gt;%\rmutate(x_lte_y = x \u0026lt;= y) %\u0026gt;%\rgroup_by(group) %\u0026gt;%\rmutate(x_lte_y_2 = x \u0026lt;= y)\r## # A tibble: 9 x 5\r## # Groups: group [3]\r## x y group x_lte_y x_lte_y_2\r## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;lgl\u0026gt; ## 1 1 9 a TRUE TRUE ## 2 2 8 a TRUE TRUE ## 3 3 7 a TRUE TRUE ## 4 4 6 b TRUE TRUE ## 5 5 5 b TRUE TRUE ## 6 6 4 b FALSE FALSE ## 7 7 3 c FALSE FALSE ## 8 8 2 c FALSE FALSE ## 9 9 1 c FALSE FALSE\rRanking functions like min_rank() work within each group when used with group_by().\ntibble(x = 1:9,\rgroup = rep(c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;), each = 3)) %\u0026gt;%\rmutate(rnk = min_rank(x)) %\u0026gt;%\rgroup_by(group) %\u0026gt;%\rmutate(rnk2 = min_rank(x))\r## # A tibble: 9 x 4\r## # Groups: group [3]\r## x group rnk rnk2\r## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r## 1 1 a 1 1\r## 2 2 a 2 2\r## 3 3 a 3 3\r## 4 4 b 4 1\r## 5 5 b 5 2\r## 6 6 b 6 3\r## 7 7 c 7 1\r## 8 8 c 8 2\r## 9 9 c 9 3\rThough not asked in the question, note that arrange() ignores groups when sorting values.\ntibble(x = runif(9),\rgroup = rep(c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;), each = 3)) %\u0026gt;%\rgroup_by(group) %\u0026gt;%\rarrange(x)\r## # A tibble: 9 x 2\r## # Groups: group [3]\r## x group\r## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt;\r## 1 0.0941 a ## 2 0.170 b ## 3 0.606 a ## 4 0.729 c ## 5 0.744 b ## 6 0.874 b ## 7 0.897 c ## 8 0.932 a ## 9 0.956 c\rHowever, the order of values from arrange() can interact with groups when\rused with functions that rely on the ordering of elements, such as lead(), lag(),\ror cumsum().\ntibble(group = rep(c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;), each = 3), x = runif(9)) %\u0026gt;%\rgroup_by(group) %\u0026gt;%\rarrange(x) %\u0026gt;%\rmutate(lag_x = lag(x))\r## # A tibble: 9 x 3\r## # Groups: group [3]\r## group x lag_x\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 a 0.00549 NA ## 2 c 0.109 NA ## 3 a 0.152 0.00549\r## 4 a 0.351 0.152 ## 5 c 0.552 0.109 ## 6 b 0.576 NA ## 7 b 0.768 0.576 ## 8 c 0.810 0.552 ## 9 b 0.885 0.768\r\r5.7.1.2\r\rWhich plane (tailnum) has the worst on-time record?\n\r\rThe question does not define a way to measure on-time record, so I will consider two metrics:\nproportion of flights not delayed or cancelled, and\rmean arrival delay.\r\rThe first metric is the proportion of not-cancelled and on-time flights.\rI use the presence of an arrival time as an indicator that a flight was not cancelled.\rHowever, there are many planes that have never flown an on-time flight.\rAdditionally, many of the planes that have the lowest proportion of on-time flights have only flown a small number of flights.\nflights %\u0026gt;%\rfilter(!is.na(tailnum)) %\u0026gt;%\rmutate(on_time = !is.na(arr_time) \u0026amp; (arr_delay \u0026lt;= 0)) %\u0026gt;%\rgroup_by(tailnum) %\u0026gt;%\rsummarise(on_time = mean(on_time), n = n()) %\u0026gt;%\rfilter(min_rank(on_time) == 1)\r## # A tibble: 110 x 3\r## tailnum on_time n\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt;\r## 1 N121DE 0 2\r## 2 N136DL 0 1\r## 3 N143DA 0 1\r## 4 N17627 0 2\r## 5 N240AT 0 5\r## 6 N26906 0 1\r## 7 N295AT 0 4\r## 8 N302AS 0 1\r## 9 N303AS 0 1\r## 10 N32626 0 1\r## # ... with 100 more rows\rSo, I will remove planes that flew at least 20 flights.\rThe choice of 20 was chosen because it round number near the first quartile of the number of flights by plane.12\nquantile(count(flights, tailnum)$n)\r## 0% 25% 50% 75% 100% ## 1 23 54 110 2512\rThe plane with the worst on time record that flew at least 20 flights is:\nflights %\u0026gt;%\rfilter(!is.na(tailnum), is.na(arr_time) | !is.na(arr_delay)) %\u0026gt;%\rmutate(on_time = !is.na(arr_time) \u0026amp; (arr_delay \u0026lt;= 0)) %\u0026gt;%\rgroup_by(tailnum) %\u0026gt;%\rsummarise(on_time = mean(on_time), n = n()) %\u0026gt;%\rfilter(n \u0026gt;= 20) %\u0026gt;%\rfilter(min_rank(on_time) == 1)\r## # A tibble: 1 x 3\r## tailnum on_time n\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt;\r## 1 N988AT 0.189 37\rThere are cases where arr_delay is missing but arr_time is not missing.\rI have not debugged the cause of this bad data, so these rows are dropped for\rthe purposes of this exercise.\nThe second metric is the mean minutes delayed.\rAs with the previous metric, I will only consider planes which flew least 20 flights.\rA different plane has the worst on-time record when measured as average minutes delayed.\nflights %\u0026gt;%\rfilter(!is.na(arr_delay)) %\u0026gt;%\rgroup_by(tailnum) %\u0026gt;%\rsummarise(arr_delay = mean(arr_delay), n = n()) %\u0026gt;%\rfilter(n \u0026gt;= 20) %\u0026gt;%\rfilter(min_rank(desc(arr_delay)) == 1)\r## # A tibble: 1 x 3\r## tailnum arr_delay n\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt;\r## 1 N203FR 59.1 41\r\r\r5.7.1.3\r\rWhat time of day should you fly if you want to avoid delays as much as possible?\n\r\rLet’s group by the hour of the flight.\rThe earlier the flight is scheduled, the lower its expected delay.\rThis is intuitive as delays will affect later flights.\rMorning flights have fewer (if any) previous flights that can delay them.\nflights %\u0026gt;%\rgroup_by(hour) %\u0026gt;%\rsummarise(arr_delay = mean(arr_delay, na.rm = TRUE)) %\u0026gt;%\rarrange(arr_delay)\r## # A tibble: 20 x 2\r## hour arr_delay\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 7 -5.30 ## 2 5 -4.80 ## 3 6 -3.38 ## 4 9 -1.45 ## 5 8 -1.11 ## 6 10 0.954\r## 7 11 1.48 ## 8 12 3.49 ## 9 13 6.54 ## 10 14 9.20 ## 11 23 11.8 ## 12 15 12.3 ## 13 16 12.6 ## 14 18 14.8 ## 15 22 16.0 ## 16 17 16.0 ## 17 19 16.7 ## 18 20 16.7 ## 19 21 18.4 ## 20 1 NaN\r\r\r5.7.1.4\r\rFor each destination, compute the total minutes of delay.\rFor each flight, compute the proportion of the total delay for its destination.\n\r\rThe key to answering this question is to only include delayed flights when calculating the total delay and proportion of delay.\nflights %\u0026gt;%\rfilter(arr_delay \u0026gt; 0) %\u0026gt;%\rgroup_by(dest) %\u0026gt;%\rmutate(\rarr_delay_total = sum(arr_delay),\rarr_delay_prop = arr_delay / arr_delay_total\r) %\u0026gt;%\rselect(dest, month, day, dep_time, carrier, flight,\rarr_delay, arr_delay_prop) %\u0026gt;%\rarrange(dest, desc(arr_delay_prop))\r## # A tibble: 133,004 x 8\r## # Groups: dest [103]\r## dest month day dep_time carrier flight arr_delay arr_delay_prop\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 ABQ 7 22 2145 B6 1505 153 0.0341\r## 2 ABQ 12 14 2223 B6 65 149 0.0332\r## 3 ABQ 10 15 2146 B6 65 138 0.0308\r## 4 ABQ 7 23 2206 B6 1505 137 0.0305\r## 5 ABQ 12 17 2220 B6 65 136 0.0303\r## 6 ABQ 7 10 2025 B6 1505 126 0.0281\r## 7 ABQ 7 30 2212 B6 1505 118 0.0263\r## 8 ABQ 7 28 2038 B6 1505 117 0.0261\r## 9 ABQ 12 8 2049 B6 65 114 0.0254\r## 10 ABQ 9 2 2212 B6 1505 109 0.0243\r## # ... with 132,994 more rows\rThere is some ambiguity in the meaning of the term flights in the question.\rThe first example defined a flight as a row in the flights table, which is a trip by an aircraft from an airport at a particular date and time.\rHowever, flight could also refer to the flight number, which is the code a carrier uses for an airline service of a route.\rFor example, AA1 is the flight number of the 09:00 American Airlines flight between JFK and LAX.\rThe flight number is contained in the flights$flight column, though what is called a “flight” is a combination of the flights$carrier and flights$flight columns.\nflights %\u0026gt;%\rfilter(arr_delay \u0026gt; 0) %\u0026gt;%\rgroup_by(dest, origin, carrier, flight) %\u0026gt;%\rsummarise(arr_delay = sum(arr_delay)) %\u0026gt;%\rgroup_by(dest) %\u0026gt;%\rmutate(\rarr_delay_prop = arr_delay / sum(arr_delay)\r) %\u0026gt;%\rarrange(dest, desc(arr_delay_prop)) %\u0026gt;%\rselect(carrier, flight, origin, dest, arr_delay_prop)\r## `summarise()` has grouped output by \u0026#39;dest\u0026#39;, \u0026#39;origin\u0026#39;, \u0026#39;carrier\u0026#39;. You can override using the `.groups` argument.\r## # A tibble: 8,834 x 5\r## # Groups: dest [103]\r## carrier flight origin dest arr_delay_prop\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 B6 1505 JFK ABQ 0.567 ## 2 B6 65 JFK ABQ 0.433 ## 3 B6 1191 JFK ACK 0.475 ## 4 B6 1491 JFK ACK 0.414 ## 5 B6 1291 JFK ACK 0.0898\r## 6 B6 1195 JFK ACK 0.0208\r## 7 EV 4309 EWR ALB 0.174 ## 8 EV 4271 EWR ALB 0.137 ## 9 EV 4117 EWR ALB 0.0951\r## 10 EV 4088 EWR ALB 0.0865\r## # ... with 8,824 more rows\r\r\r5.7.1.5\r\rDelays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later \u0026gt;flights are delayed to allow earlier flights to leave. Using lag() explore how the delay of a flight is related to the \u0026gt;delay of the immediately preceding flight.\n\r\rThis calculates the departure delay of the preceding flight from the same airport.\nlagged_delays \u0026lt;- flights %\u0026gt;%\rarrange(origin, month, day, dep_time) %\u0026gt;%\rgroup_by(origin) %\u0026gt;%\rmutate(dep_delay_lag = lag(dep_delay)) %\u0026gt;%\rfilter(!is.na(dep_delay), !is.na(dep_delay_lag))\rThis plots the relationship between the mean delay of a flight for all values of the previous flight.\rFor delays less than two hours, the relationship between the delay of the preceding flight and the current flight is nearly a line.\rAfter that the relationship becomes more variable, as long-delayed flights are interspersed with flights leaving on-time.\rAfter about 8-hours, a delayed flight is likely to be followed by a flight leaving on time.\nlagged_delays %\u0026gt;%\rgroup_by(dep_delay_lag) %\u0026gt;%\rsummarise(dep_delay_mean = mean(dep_delay)) %\u0026gt;%\rggplot(aes(y = dep_delay_mean, x = dep_delay_lag)) +\rgeom_point() +\rscale_x_continuous(breaks = seq(0, 1500, by = 120)) +\rlabs(y = \u0026quot;Departure Delay\u0026quot;, x = \u0026quot;Previous Departure Delay\u0026quot;)\rThe overall relationship looks similar in all three origin airports.\nlagged_delays %\u0026gt;%\rgroup_by(origin, dep_delay_lag) %\u0026gt;%\rsummarise(dep_delay_mean = mean(dep_delay)) %\u0026gt;%\rggplot(aes(y = dep_delay_mean, x = dep_delay_lag)) +\rgeom_point() +\rfacet_wrap(~ origin, ncol=1) +\rlabs(y = \u0026quot;Departure Delay\u0026quot;, x = \u0026quot;Previous Departure Delay\u0026quot;)\r## `summarise()` has grouped output by \u0026#39;origin\u0026#39;. You can override using the `.groups` argument.\r\r\r5.7.1.6\r\rLook at each destination. Can you find flights that are suspiciously fast?\r(i.e. flights that represent a potential data entry error).\rCompute the air time of a flight relative to the shortest flight to that destination.\rWhich flights were most delayed in the air?\n\r\rWhen calculating this answer we should only compare flights within the same (origin, destination) pair.\nTo find unusual observations, we need to first put them on the same scale.\rI will standardize\rvalues by subtracting the mean from each and then dividing each by the standard deviation.\r\\[\r\\mathsf{standardized}(x) = \\frac{x - \\mathsf{mean}(x)}{\\mathsf{sd}(x)} .\r\\]\rA standardized variable is often called a \\(z\\)-score.\rThe units of the standardized variable are standard deviations from the mean.\rThis will put the flight times from different routes on the same scale.\rThe larger the magnitude of the standardized variable for an observation, the more unusual the observation is.\rFlights with negative values of the standardized variable are faster than the\rmean flight for that route, while those with positive values are slower than\rthe mean flight for that route.\nstandardized_flights \u0026lt;- flights %\u0026gt;%\rfilter(!is.na(air_time)) %\u0026gt;%\rgroup_by(dest, origin) %\u0026gt;%\rmutate(\rair_time_mean = mean(air_time),\rair_time_sd = sd(air_time),\rn = n()\r) %\u0026gt;%\rungroup() %\u0026gt;%\rmutate(air_time_standard = (air_time - air_time_mean) / (air_time_sd + 1))\rI add 1 to the denominator and numerator to avoid dividing by zero.\rNote that the ungroup() here is not necessary. However, I will be using\rthis data frame later. Through experience, I have found that I have fewer bugs\rwhen I keep a data frame grouped for only those verbs that need it.\rIf I did not ungroup() this data frame, the arrange() used later would\rnot work as expected. It is better to err on the side of using ungroup()\rwhen unnecessary.\nThe distribution of the standardized air flights has long right tail.\nggplot(standardized_flights, aes(x = air_time_standard)) +\rgeom_density()\r## Warning: Removed 4 rows containing non-finite values (stat_density).\rUnusually fast flights are those flights with the smallest standardized values.\nstandardized_flights %\u0026gt;%\rarrange(air_time_standard) %\u0026gt;%\rselect(\rcarrier, flight, origin, dest, month, day,\rair_time, air_time_mean, air_time_standard\r) %\u0026gt;%\rhead(10) %\u0026gt;%\rprint(width = Inf)\r## # A tibble: 10 x 9\r## carrier flight origin dest month day air_time air_time_mean\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 DL 1499 LGA ATL 5 25 65 114. ## 2 EV 4667 EWR MSP 7 2 93 151. ## 3 EV 4292 EWR GSP 5 13 55 93.2\r## 4 EV 3805 EWR BNA 3 23 70 115. ## 5 EV 4687 EWR CVG 9 29 62 96.1\r## 6 B6 2002 JFK BUF 11 10 38 57.1\r## 7 DL 1902 LGA PBI 1 12 105 146. ## 8 DL 161 JFK SEA 7 3 275 329. ## 9 EV 5486 LGA PIT 4 28 40 57.7\r## 10 B6 30 JFK ROC 3 25 35 51.9\r## air_time_standard\r## \u0026lt;dbl\u0026gt;\r## 1 -4.56\r## 2 -4.46\r## 3 -4.20\r## 4 -3.73\r## 5 -3.60\r## 6 -3.38\r## 7 -3.34\r## 8 -3.34\r## 9 -3.15\r## 10 -3.10\rI used width = Inf to ensure that all columns will be printed.\nThe fastest flight is DL1499 from LGA to\rATL which departed on\r2013-05-25 at 17:09.\rIt has an air time of 65 minutes, compared to an average\rflight time of 114 minutes for its route.\rThis is 4.6 standard deviations below\rthe average flight on its route.\nIt is important to note that this does not necessarily imply that there was a data entry error.\rWe should check these flights to see whether there was some reason for the difference.\rIt may be that we are missing some piece of information that explains these unusual times.\nA potential issue with the way that we standardized the flights is that the mean and standard deviation used to calculate are sensitive to outliers and outliers is what we are looking for.\rInstead of standardizing variables with the mean and variance, we could use the median\ras a measure of central tendency and the interquartile range (IQR) as a measure of spread.\rThe median and IQR are more resistant to outliers than the mean and standard deviation.\rThe following method uses the median and inter-quartile range, which are less sensitive to outliers.\nstandardized_flights2 \u0026lt;- flights %\u0026gt;%\rfilter(!is.na(air_time)) %\u0026gt;%\rgroup_by(dest, origin) %\u0026gt;%\rmutate(\rair_time_median = median(air_time),\rair_time_iqr = IQR(air_time),\rn = n(),\rair_time_standard = (air_time - air_time_median) / air_time_iqr)\rThe distribution of the standardized air flights using this new definition\ralso has long right tail of slow flights.\nggplot(standardized_flights2, aes(x = air_time_standard)) +\rgeom_density()\r## Warning: Removed 4 rows containing non-finite values (stat_density).\rUnusually fast flights are those flights with the smallest standardized values.\nstandardized_flights2 %\u0026gt;%\rarrange(air_time_standard) %\u0026gt;%\rselect(\rcarrier, flight, origin, dest, month, day, air_time,\rair_time_median, air_time_standard\r) %\u0026gt;%\rhead(10) %\u0026gt;%\rprint(width = Inf)\r## # A tibble: 10 x 9\r## # Groups: dest, origin [10]\r## carrier flight origin dest month day air_time air_time_median\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 EV 4667 EWR MSP 7 2 93 149\r## 2 DL 1499 LGA ATL 5 25 65 112\r## 3 US 2132 LGA BOS 3 2 21 37\r## 4 B6 30 JFK ROC 3 25 35 51\r## 5 B6 2002 JFK BUF 11 10 38 57\r## 6 EV 4292 EWR GSP 5 13 55 92\r## 7 EV 4249 EWR SYR 3 15 30 39\r## 8 EV 4580 EWR BTV 6 29 34 46\r## 9 EV 3830 EWR RIC 7 2 35 53\r## 10 EV 4687 EWR CVG 9 29 62 95\r## air_time_standard\r## \u0026lt;dbl\u0026gt;\r## 1 -3.5 ## 2 -3.36\r## 3 -3.2 ## 4 -3.2 ## 5 -3.17\r## 6 -3.08\r## 7 -3 ## 8 -3 ## 9 -3 ## 10 -3\rAll of these answers have relied only on using a distribution of comparable observations to find unusual observations.\rIn this case, the comparable observations were flights from the same origin to the same destination.\rApart from our knowledge that flights from the same origin to the same destination should have similar air times, we have not used any other domain-specific knowledge.\rBut we know much more about this problem.\rThe most obvious piece of knowledge we have is that we know that flights cannot travel back in time, so there should never be a flight with a negative airtime.\rBut we also know that aircraft have maximum speeds.\rWhile different aircraft have different cruising speeds, commercial airliners\rtypically cruise at air speeds around 547–575 mph.\rCalculating the ground speed of aircraft is complicated by the way in which winds, especially the influence of wind, especially jet streams, on the ground-speed of flights.\rA strong tailwind can increase ground-speed of the aircraft by 200 mph.\rApart from the retired Concorde.\rFor example, in 2018, a transatlantic flight\rtraveled at 770 mph due to a strong jet stream tailwind.\rThis means that any flight traveling at speeds greater than 800 mph is implausible,\rand it may be worth checking flights traveling at greater than 600 or 700 mph.\rGround speed could also be used to identify aircraft flying implausibly slow.\rJoining flights data with the air craft type in the planes table and getting\rinformation about typical or top speeds of those aircraft could provide a more\rdetailed way to identify implausibly fast or slow flights.\rAdditional data on high altitude wind speeds at the time of the flight would further help.\nKnowing the substance of the data analysis at hand is one of the most important\rtools of a data scientist. The tools of statistics are a complement, not a\rsubstitute, for that knowledge.\nWith that in mind, Let’s plot the distribution of the ground speed of flights.\rThe modal flight in this data has a ground speed of between 400 and 500 mph.\rThe distribution of ground speeds has a large left tail of slower flights below\r400 mph constituting the majority.\rThere are very few flights with a ground speed over 500 mph.\nflights %\u0026gt;%\rmutate(mph = distance / (air_time / 60)) %\u0026gt;%\rggplot(aes(x = mph)) +\rgeom_histogram(binwidth = 10)\r## Warning: Removed 9430 rows containing non-finite values (stat_bin).\rThe fastest flight is the same one identified as the largest outlier earlier.\rIts ground speed was 703 mph.\rThis is fast for a commercial jet, but not impossible.\nflights %\u0026gt;%\rmutate(mph = distance / (air_time / 60)) %\u0026gt;%\rarrange(desc(mph)) %\u0026gt;%\rselect(mph, flight, carrier, flight, month, day, dep_time) %\u0026gt;%\rhead(5)\r## # A tibble: 5 x 6\r## mph flight carrier month day dep_time\r## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r## 1 703. 1499 DL 5 25 1709\r## 2 650. 4667 EV 7 2 1558\r## 3 648 4292 EV 5 13 2040\r## 4 641. 3805 EV 3 23 1914\r## 5 591. 1902 DL 1 12 1559\rOne explanation for unusually fast flights is that they are “making up time” in the air by flying faster.\rCommercial aircraft do not fly at their top speed since the airlines are also concerned about fuel consumption.\rBut, if a flight is delayed on the ground, it may fly faster than usual in order to avoid a late arrival.\rSo, I would expect that some of the unusually fast flights were delayed on departure.\nflights %\u0026gt;%\rmutate(mph = distance / (air_time / 60)) %\u0026gt;%\rarrange(desc(mph)) %\u0026gt;%\rselect(\rorigin, dest, mph, year, month, day, dep_time, flight, carrier,\rdep_delay, arr_delay\r)\r## # A tibble: 336,776 x 11\r## origin dest mph year month day dep_time flight carrier dep_delay\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 LGA ATL 703. 2013 5 25 1709 1499 DL 9\r## 2 EWR MSP 650. 2013 7 2 1558 4667 EV 45\r## 3 EWR GSP 648 2013 5 13 2040 4292 EV 15\r## 4 EWR BNA 641. 2013 3 23 1914 3805 EV 4\r## 5 LGA PBI 591. 2013 1 12 1559 1902 DL -1\r## 6 JFK SJU 564 2013 11 17 650 315 DL -5\r## 7 JFK SJU 557. 2013 2 21 2355 707 B6 -3\r## 8 JFK STT 556. 2013 11 17 759 936 AA -1\r## 9 JFK SJU 554. 2013 11 16 2003 347 DL 38\r## 10 JFK SJU 554. 2013 11 16 2349 1503 B6 -10\r## # ... with 336,766 more rows, and 1 more variable: arr_delay \u0026lt;dbl\u0026gt;\rhead(5)\r## [1] 5\rFive of the top ten flights had departure delays, and three of those were\rable to make up that time in the air and arrive ahead of schedule.\nOverall, there were a few flights that seemed unusually fast, but they all\rfall into the realm of plausibility and likely are not data entry problems.\r[Ed. Please correct me if I am missing something]\nThe second part of the question asks us to compare flights to the fastest flight\ron a route to find the flights most delayed in the air. I will calculate the\ramount a flight is delayed in air in two ways.\rThe first is the absolute delay, defined as the number of minutes longer than the fastest flight on that route,air_time - min(air_time).\rThe second is the relative delay, which is the percentage increase in air time relative to the time of the fastest flight\ralong that route, (air_time - min(air_time)) / min(air_time) * 100.\nair_time_delayed \u0026lt;-\rflights %\u0026gt;%\rgroup_by(origin, dest) %\u0026gt;%\rmutate(\rair_time_min = min(air_time, na.rm = TRUE),\rair_time_delay = air_time - air_time_min,\rair_time_delay_pct = air_time_delay / air_time_min * 100\r)\r## Warning in min(air_time, na.rm = TRUE): no non-missing arguments to min;\r## returning Inf\rThe most delayed flight in air in minutes was DL841\rfrom JFK to SFO which departed on\r2013-07-28 at 17:27. It took\r189 minutes longer than the flight with the shortest\rair time on its route.\nair_time_delayed %\u0026gt;%\rarrange(desc(air_time_delay)) %\u0026gt;%\rselect(\rair_time_delay, carrier, flight,\rorigin, dest, year, month, day, dep_time,\rair_time, air_time_min\r) %\u0026gt;%\rhead() %\u0026gt;%\rprint(width = Inf)\r## # A tibble: 6 x 11\r## # Groups: origin, dest [5]\r## air_time_delay carrier flight origin dest year month day dep_time air_time\r## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 189 DL 841 JFK SFO 2013 7 28 1727 490\r## 2 165 DL 426 JFK LAX 2013 11 22 1812 440\r## 3 163 AA 575 JFK EGE 2013 1 28 1806 382\r## 4 147 DL 17 JFK LAX 2013 7 10 1814 422\r## 5 145 UA 745 LGA DEN 2013 9 10 1513 331\r## 6 143 UA 587 EWR LAS 2013 11 22 2142 399\r## air_time_min\r## \u0026lt;dbl\u0026gt;\r## 1 301\r## 2 275\r## 3 219\r## 4 275\r## 5 186\r## 6 256\rThe most delayed flight in air as a percentage of the fastest flight along that\rroute was US2136\rfrom LGA to BOS departing on 2013-06-17 at 16:52.\rIt took 410% longer than the\rflight with the shortest air time on its route.\nair_time_delayed %\u0026gt;%\rarrange(desc(air_time_delay)) %\u0026gt;%\rselect(\rair_time_delay_pct, carrier, flight,\rorigin, dest, year, month, day, dep_time,\rair_time, air_time_min\r) %\u0026gt;%\rhead() %\u0026gt;%\rprint(width = Inf)\r## # A tibble: 6 x 11\r## # Groups: origin, dest [5]\r## air_time_delay_pct carrier flight origin dest year month day dep_time\r## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r## 1 62.8 DL 841 JFK SFO 2013 7 28 1727\r## 2 60 DL 426 JFK LAX 2013 11 22 1812\r## 3 74.4 AA 575 JFK EGE 2013 1 28 1806\r## 4 53.5 DL 17 JFK LAX 2013 7 10 1814\r## 5 78.0 UA 745 LGA DEN 2013 9 10 1513\r## 6 55.9 UA 587 EWR LAS 2013 11 22 2142\r## air_time air_time_min\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 490 301\r## 2 440 275\r## 3 382 219\r## 4 422 275\r## 5 331 186\r## 6 399 256\r\r\r5.7.1.7\r\rFind all destinations that are flown by at least two carriers.\rUse that information to rank the carriers.\n\r\rTo restate this question, we are asked to rank airlines by the number of destinations that they fly to, considering only those airports that are flown to by two or more airlines.\rThere are two steps to calculating this ranking.\rFirst, find all airports serviced by two or more carriers.\rThen, rank carriers by the number of those destinations that they service.\nflights %\u0026gt;%\r# find all airports with \u0026gt; 1 carrier\rgroup_by(dest) %\u0026gt;%\rmutate(n_carriers = n_distinct(carrier)) %\u0026gt;%\rfilter(n_carriers \u0026gt; 1) %\u0026gt;%\r# rank carriers by numer of destinations\rgroup_by(carrier) %\u0026gt;%\rsummarize(n_dest = n_distinct(dest)) %\u0026gt;%\rarrange(desc(n_dest))\r## # A tibble: 16 x 2\r## carrier n_dest\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 EV 51\r## 2 9E 48\r## 3 UA 42\r## 4 DL 39\r## 5 B6 35\r## 6 AA 19\r## 7 MQ 19\r## 8 WN 10\r## 9 OO 5\r## 10 US 5\r## 11 VX 4\r## 12 YV 3\r## 13 FL 2\r## 14 AS 1\r## 15 F9 1\r## 16 HA 1\rThe carrier \"EV\" flies to the most destinations, considering only airports flown to by two or more carriers. What airline does the \"EV\" carrier code correspond to?\nfilter(airlines, carrier == \u0026quot;EV\u0026quot;)\r## # A tibble: 1 x 2\r## carrier name ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 EV ExpressJet Airlines Inc.\rUnless you know the airplane industry, it is likely that you don’t recognize ExpressJet; I certainly didn’t.\rIt is a regional airline that partners with major airlines to fly from hubs (larger airports) to smaller airports.\rThis means that many of the shorter flights of major carriers are operated by ExpressJet.\rThis business model explains why ExpressJet services the most destinations.\nAmong the airlines that fly to only one destination from New York are Alaska Airlines\rand Hawaiian Airlines.\nfilter(airlines, carrier %in% c(\u0026quot;AS\u0026quot;, \u0026quot;F9\u0026quot;, \u0026quot;HA\u0026quot;))\r## # A tibble: 3 x 2\r## carrier name ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 AS Alaska Airlines Inc. ## 2 F9 Frontier Airlines Inc.\r## 3 HA Hawaiian Airlines Inc.\r\r\r5.7.1.8\rFor each plane, count the number of flights before the first delay of greater than 1 hour.\n\rThe question does not specify arrival or departure delay.\rI consider dep_delay in this answer, though similar code could be used for arr_delay.\nflights %\u0026gt;%\r# sort in increasing order\rselect(tailnum, year, month,day, dep_delay) %\u0026gt;%\rfilter(!is.na(dep_delay)) %\u0026gt;%\rarrange(tailnum, year, month, day) %\u0026gt;%\rgroup_by(tailnum) %\u0026gt;%\r# cumulative number of flights delayed over one hour\rmutate(cumulative_hr_delays = cumsum(dep_delay \u0026gt; 60)) %\u0026gt;%\r# count the number of flights == 0\rsummarise(total_flights = sum(cumulative_hr_delays \u0026lt; 1)) %\u0026gt;%\rarrange(total_flights)\r## # A tibble: 4,037 x 2\r## tailnum total_flights\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 D942DN 0\r## 2 N10575 0\r## 3 N11106 0\r## 4 N11109 0\r## 5 N11187 0\r## 6 N11199 0\r## 7 N12967 0\r## 8 N13550 0\r## 9 N136DL 0\r## 10 N13903 0\r## # ... with 4,027 more rows\r\r\r\r\r\rWe could address this issue using a statistical model, but that is outside\rthe scope of this text.↩︎\n\rThe count() function is introduced in Chapter 5.6. It returns the count of\rrows by group. In this case, the number of rows in flights for each\rtailnum. The data frame that count() returns has columns for the\rgroups, and a column n, which contains that count.↩︎\n\r\r\r","date":1616889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616889600,"objectID":"2515718a77b481a9c3cfbf7ee522d159","permalink":"https://ianadamsresearch.com/courses/pubpl-6002/week-10/","publishdate":"2021-03-28T00:00:00Z","relpermalink":"/courses/pubpl-6002/week-10/","section":"courses","summary":"So you want to be a data scientist\rThis week can feel like a bit of a doozy, as the difficulty really ramps up. Something to remember - most data scientists spend most of their time cleaning, transforming, and tidying their data.","tags":["6002"],"title":"Week 10 Solutions","type":"docs"},{"authors":[],"categories":[],"content":"\r\rSynthetic control methodologies come in many flavors. Most commonly, Scott Mourtgos and I use “Bayesian Structural Time Series,” but there are others. One exciting new package brings synth models to the tidyverse. Eric Dunford just released his new package tidysynth, and I wanted to give it a spin.\nTo demonstrate the package, the vignette uses data from Abadie et al. (2010), which tests the effects of an anti-smoking proposition on cigarette consumption.\nlibrary(tidyverse)\rlibrary(tidysynth)\rdata(\u0026quot;smoking\u0026quot;)\rsmoking %\u0026gt;% glimpse()\r## Rows: 1,209\r## Columns: 7\r## $ state \u0026lt;chr\u0026gt; \u0026quot;Rhode Island\u0026quot;, \u0026quot;Tennessee\u0026quot;, \u0026quot;Indiana\u0026quot;, \u0026quot;Nevada\u0026quot;, \u0026quot;Louisi...\r## $ year \u0026lt;dbl\u0026gt; 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 197...\r## $ cigsale \u0026lt;dbl\u0026gt; 123.9, 99.8, 134.6, 189.5, 115.9, 108.4, 265.7, 93.8, 100...\r## $ lnincome \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N...\r## $ beer \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N...\r## $ age15to24 \u0026lt;dbl\u0026gt; 0.1831579, 0.1780438, 0.1765159, 0.1615542, 0.1851852, 0....\r## $ retprice \u0026lt;dbl\u0026gt; 39.3, 39.9, 30.6, 38.9, 34.3, 38.4, 31.4, 37.3, 36.7, 28....\rThe main computations happen within just a few pipes (ahh, the beauty of tidy!).\nsmoking_out \u0026lt;-\rsmoking %\u0026gt;%\r# initial the synthetic control object\rsynthetic_control(outcome = cigsale, # outcome\runit = state, # unit index in the panel data\rtime = year, # time index in the panel data\ri_unit = \u0026quot;California\u0026quot;, # unit where the intervention occurred\ri_time = 1988, # time period when the intervention occurred\rgenerate_placebos=T # generate placebo synthetic controls (for inference)\r) %\u0026gt;%\r# Generate the aggregate predictors used to fit the weights\r# average log income, retail price of cigarettes, and proportion of the\r# population between 15 and 24 years of age from 1980 - 1988\rgenerate_predictor(time_window = 1980:1988,\rln_income = mean(lnincome, na.rm = T),\rret_price = mean(retprice, na.rm = T),\ryouth = mean(age15to24, na.rm = T)) %\u0026gt;%\r# average beer consumption in the donor pool from 1984 - 1988\rgenerate_predictor(time_window = 1984:1988,\rbeer_sales = mean(beer, na.rm = T)) %\u0026gt;%\r# Lagged cigarette sales generate_predictor(time_window = 1975,\rcigsale_1975 = cigsale) %\u0026gt;%\rgenerate_predictor(time_window = 1980,\rcigsale_1980 = cigsale) %\u0026gt;%\rgenerate_predictor(time_window = 1988,\rcigsale_1988 = cigsale) %\u0026gt;%\r# Generate the fitted weights for the synthetic control\rgenerate_weights(optimization_window = 1970:1988, # time to use in the optimization task\rmargin_ipop = .02,sigf_ipop = 7,bound_ipop = 6 # optimizer options\r) %\u0026gt;%\r# Generate the synthetic control\rgenerate_control()\rIf everything is working like it should, the synthetic control should closely match the observed trend in the pre-intervention period.\nsmoking_out %\u0026gt;% plot_trends()\rOne can easily see that the post-propostion period is deviating downward. But to capture the actual quantitative differences between the observed and synthetic models, we can use plot_differences().\nsmoking_out %\u0026gt;% plot_differences()\rWe might also want to know which (and how) units and variables were weighted by the model.\nsmoking_out %\u0026gt;% plot_weights()\rSmooth! The package also includes a host of grab_ functions to quickly retrieve parts of the tidy output. For example, a balance table can show us how comparable the synthetic control is to the observed covariates of the treated unit.\nsmoking_out %\u0026gt;% grab_balance_table()\r## # A tibble: 7 x 4\r## variable California synthetic_California donor_sample\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 ln_income 10.1 9.84 9.83 ## 2 ret_price 89.4 89.4 87.3 ## 3 youth 0.174 0.174 0.173\r## 4 beer_sales 24.3 24.3 23.7 ## 5 cigsale_1975 127. 127. 137. ## 6 cigsale_1980 120. 120. 138. ## 7 cigsale_1988 90.1 90.8 114.\rEverything looks good there. One of the main uses for synthetic control models is for inference. In other words, can we infer causality for the somewhat dramatic change between the pre- and post-intervention periods? The package developer has included some very nice features to help with inference. From the package developer:\n\rFor inference, the method relies on repeating the method for every donor in the donor pool exactly as was done for the treated unit — i.e. generating placebo synthetic controls. By setting generate_placebos = TRUE when initializing the synth pipeline with synthetic_control(), placebo cases are automatically generated when constructing the synthetic control of interest. This makes it easy to explore how unique difference between the observed and synthetic unit is when compared to the placebos.\n\rsmoking_out %\u0026gt;% plot_placebos()\rYou might wonder why the plot above is only plotting a few of the donor cases. This is because the plain function of plot_placebos() automatically drops those cases where the data has a poor fit to the model. This is a fairly large difference between this package, which uses a frequentist approach, and BSTS, which obviously uses a Bayesian approach. Still, the package developer has a prune = FALSE argument you can use to see all the cases, regardless of data –\u0026gt; model fit.\n\rNote that the plot_placebos() function automatically prunes any placebos that poorly fit the data in the pre-intervention period. The reason for doing so is purely visual: those units tend to throw off the scale when plotting the placebos. To prune, the function looks at the pre-intervention period mean squared prediction error (MSPE) (i.e. a metric that reflects how well the synthetic control maps to the observed outcome time series in pre-intervention period). If a placebo control has a MSPE that is two times beyond the target case (e.g. “California”), then it’s dropped. To turn off this behavior, set prune = FALSE.\n\rsmoking_out %\u0026gt;% plot_placebos(prune = FALSE)\rSome researchers prefer a frequentist approach, and one of the advantages of this approach is that we can derive Fisher’s Exact P-values based on work from Abadie et al., (2010). Interpretation is straightforward:\n\rIf the intervention had no effect, then the post-period and pre-period should continue to map onto one another fairly well, yielding a ratio close to 1. If the placebo units fit the data similarly, then we can’t reject the hull hypothesis that there is no effect brought about by the intervention.\n\r\rThis ratio can be easily plotted using plot_mspe_ratio(), offering insight into the rarity of the case where the intervention actually occurred.\n\rsmoking_out %\u0026gt;% plot_mspe_ratio()\rFor those who want to publish their results, reviewers and readers are going to want a table of results. The tidysynth package has you covered.\nsmoking_out %\u0026gt;% grab_signficance()\r## # A tibble: 39 x 8\r## unit_name type pre_mspe post_mspe mspe_ratio rank fishers_exact_p~ z_score\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 California Trea~ 3.94 390. 99.0 1 0.0256 5.13 ## 2 Georgia Donor 3.48 174. 49.8 2 0.0513 2.33 ## 3 Virginia Donor 5.86 171. 29.2 3 0.0769 1.16 ## 4 Indiana Donor 18.4 415. 22.6 4 0.103 0.787 ## 5 West Virg~ Donor 14.3 287. 20.1 5 0.128 0.646 ## 6 Connectic~ Donor 27.3 335. 12.3 6 0.154 0.202 ## 7 Nebraska Donor 6.47 54.3 8.40 7 0.179 -0.0189\r## 8 Missouri Donor 9.19 77.0 8.38 8 0.205 -0.0199\r## 9 Texas Donor 24.5 160. 6.54 9 0.231 -0.125 ## 10 Idaho Donor 53.2 340. 6.39 10 0.256 -0.133 ## # ... with 29 more rows\rI really appreciate the work that’s gone into this package. It highlights the value of the tidyverse with human-readable code, and straightforward piping to make for very functional analysis with limited work. Great job to the developer!\nI suggest reading further, as the package is apparently under active development. I also don’t go into the many grab_ functions that allow for the researcher to quickly “grab” elements of the model.\n## # A tibble: 8 x 2\r## Function Description ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 grab_outcome() Extract the outcome variable generated by synthetic_cont~\r## 2 grab_predictors() Extract the aggregate-level covariates generated by gene~\r## 3 grab_unit_weights() Extract the unit weights generated by generate_weights().\r## 4 grab_predictor_weig~ Extract the predictor variable weights generated by gene~\r## 5 grab_loss() Extract the RMSE loss of the optimized weights generated~\r## 6 grab_synthetic_cont~ Extract the synthetic control generated using generate_c~\r## 7 grab_signficance() Generate inferential statistics comparing the rarity of ~\r## 8 grab_balance_table() Compare the distributions of the aggregate-level predict~\r","date":1612752983,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612752983,"objectID":"003fc60128862e20399335ab0c1eadf9","permalink":"https://ianadamsresearch.com/post/2021-02-07-tidysynth-demonstration/","publishdate":"2021-02-07T19:56:23-07:00","relpermalink":"/post/2021-02-07-tidysynth-demonstration/","section":"post","summary":"Synthetic control methodologies come in many flavors. Most commonly, Scott Mourtgos and I use “Bayesian Structural Time Series,” but there are others. One exciting new package brings synth models to the tidyverse.","tags":[],"title":"Tidysynth Demonstration","type":"post"},{"authors":[],"categories":[],"content":"I\u0026rsquo;ve struggled mightily with Github over the years. I kind of got the idea, but the practice of it always left me frustrated, and the time-sink necessary to figure it all out was always just out of reach.\nI\u0026rsquo;ve always manually deployed this website through Netlify. That actually works just fine, but it\u0026rsquo;s easy to lose a lot of time as you deploy \u0026ndash;\u0026gt; find a mistake \u0026ndash;\u0026gt; back to Rstudio to fix things \u0026ndash;\u0026gt; deploy, and on and on.\nI have finally figured it out, at least enough to deploy consistently and safely. One of the big hangups I found was that Rstudio\u0026rsquo;s git commit does not work well with bigger sized files. When dealing with blogdown, the \u0026ldquo;public\u0026rdquo; folder that gets built, and that you use to manually deploy, was a problem. Upon initial commit, it was hanging Rstudio and would not proceed. That would lead to a \u0026ldquo;lock\u0026rdquo; file in the git process, leaving me unable to proceed. It took a while to figure out that using the Github Desktop app actually works a lot smoother. For small commits I still use Rstudio, but if I\u0026rsquo;m forking over a larger respository and building a local public folder, I\u0026rsquo;ll use Github Desktop to take care of the commit and push.\nBut really, this whole post is just a breadcrumb for myself to remember when I switched over to deploying the website from Github. It\u0026rsquo;s such a nice quality of life improvement!\n","date":1611286577,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611286577,"objectID":"aad8746245e9729f89c3f4c78ce5fcb8","permalink":"https://ianadamsresearch.com/post/2021-01-21-git-marker/","publishdate":"2021-01-21T20:36:17-07:00","relpermalink":"/post/2021-01-21-git-marker/","section":"post","summary":"I\u0026rsquo;ve struggled mightily with Github over the years. I kind of got the idea, but the practice of it always left me frustrated, and the time-sink necessary to figure it all out was always just out of reach.","tags":[],"title":"Git Marker","type":"post"},{"authors":[],"categories":[],"content":"\r\rAs I was building a recent preprint, and trying to translate a long Bayesian formula (courtesy the big brain of Scott Mourtgos) into properly specified LaTeX, I thought there has to be a better way. As usual, my decision to follow Andrew Heiss’ github paid off, as I saw he has been authoring the equatiomatic package. The project is maintained by Daniel Anderson, and you can check it out yourself here.\nThe beauty of equatiomatic is clear - it takes your model object in R and translates it into beautifully rendered LaTeX equations.\nThought I’d quickly demo the package using some easy data I had laying around.\nWalking Through equatiomatic\rFirst get the package installed and loaded:\n# package install\r# install.packages(\u0026quot;equatiomatic\u0026quot;, repos = \u0026quot;http://cran.us.r-project.org\u0026quot;)\r# load up\rlibrary(equatiomatic)\rI’m using some data from my most recent publication in Public Administration Review, which tests competing theories of body-worn camera (BWC) activation. We ask: Is variation in BWC activations more explained by officer attitudes towards the cameras, by officer demographics, or by job function. I won’t repeat the whole analysis here, but you can find out by visiting the article!\nlibrary(tidyverse)\ractivations \u0026lt;- read_csv(\u0026quot;activations.csv\u0026quot;)\rhead(activations)\r## # A tibble: 6 x 26\r## totalactivations Activ_Plus_One_~ years_LEO Female rank BWC_time forcecount\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 54 4.01 4 0 1 5 0\r## 2 79 4.38 10 0 1 3 0\r## 3 138 4.93 4 0 1 5 1\r## 4 11 2.48 11 0 1 5 0\r## 5 148 5.00 20 0 1 5 0\r## 6 198 5.29 3 0 1 4 1\r## # ... with 19 more variables: totalprimarycalls \u0026lt;dbl\u0026gt;, arrests \u0026lt;dbl\u0026gt;,\r## # Line_Officer \u0026lt;dbl\u0026gt;, BWCapproval_new \u0026lt;dbl\u0026gt;, POS_LATENT \u0026lt;dbl\u0026gt;,\r## # BWC_understand \u0026lt;dbl\u0026gt;, BWC_freedom \u0026lt;dbl\u0026gt;, BWC_decision \u0026lt;dbl\u0026gt;,\r## # BWC_manipulate \u0026lt;dbl\u0026gt;, BWC_modify \u0026lt;dbl\u0026gt;, BWC_lessforce \u0026lt;dbl\u0026gt;,\r## # BWC_assault \u0026lt;dbl\u0026gt;, BWC_complaint \u0026lt;dbl\u0026gt;, BWC_personal \u0026lt;dbl\u0026gt;,\r## # BWC_embarrass \u0026lt;dbl\u0026gt;, BWC_hatred \u0026lt;dbl\u0026gt;, BWC_fair \u0026lt;dbl\u0026gt;, BWC_protect \u0026lt;dbl\u0026gt;,\r## # BWC_wellbeing \u0026lt;dbl\u0026gt;\rLet’s build a quick (and misspecified here) version of one of the main models of interest in the paper:\njob_function \u0026lt;- lm(totalactivations ~ forcecount + totalprimarycalls + arrests + Line_Officer, activations)\rNow, we give the results of that model to equatiomatic and let it extract and build:\nequatiomatic::extract_eq(job_function,\rwrap = TRUE, # Long equation needs to wrap terms_per_line = 2) # Max two equation terms per line\r## $$\r## \\begin{aligned}\r## \\operatorname{totalactivations} \u0026amp;= \\alpha + \\beta_{1}(\\operatorname{forcecount})\\ + \\\\\r## \u0026amp;\\quad \\beta_{2}(\\operatorname{totalprimarycalls}) + \\beta_{3}(\\operatorname{arrests})\\ + \\\\\r## \u0026amp;\\quad \\beta_{4}(\\operatorname{Line\\_Officer}) + \\epsilon\r## \\end{aligned}\r## $$\rWe can take the output directly to Rmarkdown using the given LaTeX!\n\\[\r\\begin{aligned}\r\\operatorname{totalactivations} \u0026amp;= \\alpha + \\beta_{1}(\\operatorname{forcecount})\\ + \\\\\r\u0026amp;\\quad \\beta_{2}(\\operatorname{totalprimarycalls}) + \\beta_{3}(\\operatorname{arrests})\\ + \\\\\r\u0026amp;\\quad \\beta_{4}(\\operatorname{Line\\_Officer}) + \\epsilon\r\\end{aligned}\r\\]\nAbsolutely gorgeous! But it gets better, we can include the coefficients instead of funny Greek letters!\nequatiomatic::extract_eq(job_function,\ruse_coefs = TRUE, # Use coefficients instead of beta\rwrap = TRUE, # Long equation needs to wrap terms_per_line = 2) # Max two equation terms per line\r## $$\r## \\begin{aligned}\r## \\operatorname{totalactivations} \u0026amp;= 6.19 + 10.66(\\operatorname{forcecount})\\ + \\\\\r## \u0026amp;\\quad 0.65(\\operatorname{totalprimarycalls}) + 3.91(\\operatorname{arrests})\\ + \\\\\r## \u0026amp;\\quad 18.78(\\operatorname{Line\\_Officer}) + \\epsilon\r## \\end{aligned}\r## $$\rAgain, copy/paste over the LaTeX given by equatiomatic, and:\n\\[\r\\begin{aligned}\r\\operatorname{totalactivations} \u0026amp;= 6.19 + 10.66(\\operatorname{forcecount})\\ + \\\\\r\u0026amp;\\quad 0.65(\\operatorname{totalprimarycalls}) + 3.91(\\operatorname{arrests})\\ + \\\\\r\u0026amp;\\quad 18.78(\\operatorname{Line\\_Officer}) + \\epsilon\r\\end{aligned}\r\\]\nBy the way, the package isn’t limited to linear regressions, and already has support for logistic and probit regressions with glm(), and ordered logistic regressions. Hit up the package home to follow development.\nI am completely impressed by this young package so far, and can’t wait to see what else is coming!\n\r","date":1611023238,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611023238,"objectID":"af20108142e51f83e4dc56a2cf27b893","permalink":"https://ianadamsresearch.com/post/2021-01-18-latex-equations-from-model-objects-the-equatiomatic-package/","publishdate":"2021-01-18T19:27:18-07:00","relpermalink":"/post/2021-01-18-latex-equations-from-model-objects-the-equatiomatic-package/","section":"post","summary":"As I was building a recent preprint, and trying to translate a long Bayesian formula (courtesy the big brain of Scott Mourtgos) into properly specified LaTeX, I thought there has to be a better way.","tags":[],"title":"Latex Equations From Model Objects the Equatiomatic Package","type":"post"},{"authors":null,"categories":["stats"],"content":"\r\r\rIntroduction\rThis is a very constrained, simple table to help students decide what type of statistical modeling is appropriate for their research question and data set.\n\rTable 1: Which Model Do I Use?\r\r\r\rComparing\r\rDependent (outcome) Variable\r\rIndependent (explanatory) variable\r\rParametric Test (normally distributed data)\r\rNon-parametric test (ordinal or skewed data)\r\r\r\r\rSingle Comparison Tests\r\r\r\rAverages of two independent groups\r\rScale\r\rNominal (Binary)\r\rIndependent t-test\r\rMann-Whitney test/ Wilcoxon rank sum\r\r\r\rAverages of 3+ independent groups\r\rScale\r\rNominal\r\rOne-way ANOVA\r\rKruskal-Wallis test\r\r\r\rThe average difference between paired (matched) samples e.g. test scores before and after a class\r\rScale\r\rTime or Condition variable\r\rPaired t-test\r\rWilcoxon signed rank test\r\r\r\rThe 3+ measurements on the same subject\r\rScale\r\rTime or Condition variable\r\rRepeated measures ANOVA\r\rFriedman test\r\r\rAssociation Tests\r\r\r\rRelationship between 2 continuous variables\r\rScale\r\rScale\r\rPearson’s Correlation Coefficient\r\rSpearman’s Correlation Coefficient\r\r\r\rWhat is value of DV when value of IV changes?\r\rScale\r\rAny\r\rSimple Linear Regression\r\rTransform the data\r\r\r\rWhat is value of DV when value of IV changes?\r\rNominal (Binary)\r\rAny\r\rLogistic regression\r\rNA\r\r\r\rWhat is value of DV when value of IV changes?\r\rNominal (Count)\r\rAny\r\rPoisson Regression\r\rNA\r\r\r\rWhat is value of DV when value of IV changes?\r\rNominal (Count, overdispersed)\r\rAny\r\rNegative Binomial Regression\r\rNA\r\r\r\rAssessing the relationship between two categorical variables\r\rCategorical\r\rCategorical\r\rNA\r\rChi-squared test\r\r\r\r\r\r","date":1610928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610928000,"objectID":"f5cc4b298fc59b254a61d94a9d8302c0","permalink":"https://ianadamsresearch.com/courses/pubpl-6002/which-model/","publishdate":"2021-01-18T00:00:00Z","relpermalink":"/courses/pubpl-6002/which-model/","section":"courses","summary":"Introduction\rThis is a very constrained, simple table to help students decide what type of statistical modeling is appropriate for their research question and data set.\n\rTable 1: Which Model Do I Use?","tags":["6002","stats"],"title":"Which Model do I use?","type":"docs"},{"authors":[],"categories":[],"content":"\r\rGeneral Data Sources\rThe questions posed in public policy research are astoundingly varied. Crime, medicine, environmental, and political questions are all valid domains. Given that variety, it is no surprise that the data sources tapped by public policy scholars are similarly varied.\nWith that in mind, the following list should be considered only a start. All of the sources below have been used by scholars and practitioners to investigate interesting questions from across public policy.\nUS Census\rThe Census Bureau’s mission is to serve as the nation’s leading provider of quality data about its people and economy.\nLINK\n\rInter-University Consortium for Political and Social Research (ICPSR)\r(Personal favorite!) An international consortium of more than 750 academic institutions and research organizations, Inter-university Consortium for Political and Social Research (ICPSR) provides leadership and training in data access, curation, and methods of analysis for the social science research community.\nICPSR maintains a data archive of more than 250,000 files of research in the social and behavioral sciences. It hosts 21 specialized collections of data in education, aging, criminal justice, substance abuse, terrorism, and other fields.\nLINK\n\rWorld Development Indicators (WDI) Online\rThe primary World Bank collection of development indicators, compiled from officially-recognized international sources. It presents the most current and accurate global development data available, and includes national, regional and global estimates\nLINK\n\rCouncil of European Social Science Data Archives (CESSDA) Portal\rResearch data and metadata, including sociological surveys, election studies, longitudinal studies, opinion polls, and census data.\nLINK\n\rGoogle Database Search\rDataset Search is a search engine for datasets.Using a simple keyword search, users can discover datasets hosted in thousands of repositories across the Web. The nice thing about this one is it can find data in many of the others listed here!\nLINK\n\rWorld Bank Research Data Sets\rDatasets for the World bank. Free and open access to global development data.\nLINK\n\rUS Government “Data.gov”\rData, tools, and resources to conduct research, develop web and mobile applications, design data visualizations, and more.\nLINK\n\rIQSS (Institute for Qualitative Social Science at Harvard) Dataverse Network\rAlso known as the “Harvard Dataverse,” and is a repository for research data.\nLINK\n\rAmerican National Election Studies (ANES)\rData on voting, public opinion, and political participation\nLINK\n\rGeneral Social Survey (GSS)\rThe GSS has been a reliable source of data to help researchers, students, and journalists monitor and explain trends in American behaviors, demographics, and opinions.\nLINK\n\rPew Research Center for People and the Press\rPew Research Center is a nonpartisan fact tank that informs the public about the issues, attitudes and trends shaping the world.\nLINK\n\rState Data Center (SDC) Program from US Census\rThe State Data Center (SDC) program is one of the Census Bureau’s longest and most successful partnerships. The partnership was created to make data available locally.\nLINK\n\r\r","date":1610473997,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610473997,"objectID":"d92c619caadfbd07f29336a399ef12f1","permalink":"https://ianadamsresearch.com/post/2021-01-12-public-policy-data-sources/","publishdate":"2021-01-12T10:53:17-07:00","relpermalink":"/post/2021-01-12-public-policy-data-sources/","section":"post","summary":"General Data Sources\rThe questions posed in public policy research are astoundingly varied. Crime, medicine, environmental, and political questions are all valid domains. Given that variety, it is no surprise that the data sources tapped by public policy scholars are similarly varied.","tags":["research","public policy"],"title":"Public Policy Data Sources","type":"post"},{"authors":[],"categories":["R"],"content":"\r\rI saw the scholar package has a new maintainer on Github, so thought I’d do a quick run through of what’s available in the vignette. I was happy to see some updates, I think this is one of those fun but useful packages for people to learn about. Plus, it lets everyone make a little study of their favorite little solipsistic research subjects.\nMake sure you have the updated version of scholar.\n# install.package(\u0026quot;scholar\u0026quot;)\rlibrary(scholar)\rSetup the basic information by changing up the id argument, just visit Google Scholar, click on the “My Profile” link, and copy the last character string from the url (minus everything after the \u0026amp; sign). Paste that to the id object below!\n# Define the id for author\rid \u0026lt;- \u0026#39;g9lY5RUAAAAJ\u0026#39;\r# Get profile and print name\rl \u0026lt;- get_profile(id)\rl$name \r## [1] \u0026quot;Ian T. Adams\u0026quot;\r# Get his citation history, i.e. citations to his work in a given year get_citation_history(id)\r## year cites\r## 1 2018 5\r## 2 2019 40\r## 3 2020 73\r## 4 2021 1\r# Get his publications (a large data frame)\rget_publications(id)\r## title\r## 1 Police body-worn cameras: Effects on officers’ burnout and perceived organizational support\r## 2 Visibility is a trap: The ethics of police body-worn cameras and control\r## 3 Is emotional labor easier in collectivist or individualist cultures? An east–west comparison\r## 4 “That’s What the Money’s for”: Alienation and Emotional Labor in Public Service\r## 5 Police body-worn cameras: development of the perceived intensity of monitoring scale\r## 6 Assessing public perceptions of police use-of-force: Legal reasonableness and community standards\r## 7 Understanding emotional labor at the cultural level\r## 8 The rhetoric of de-policing: Evaluating open-ended survey responses from police officers with machine learning-based structural topic modeling\r## 9 It\u0026#39;s not depersonalization, It\u0026#39;s emotional labor: Examining surface acting and use-of-force with evidence from the US\r## 10 Hidden in plain sight\r## 11 Hidden in Plain Sight: Contrasting Emotional Labor and Burnout in Civilian and Sworn Law Enforcement Employees\r## 12 The Effect of Prosecutorial Actions on Deterrence: A County-Level Analysis\r## 13 Contrasting Emotional Labor and Burnout in Civilian and Sworn Law Enforcement Personnel\r## 14 Emotional Labor in Emergency Dispatch: Gauging Effects of Training Protocols\r## 15 Andrew G. Ferguson, The Rise of Big Data Policing: Surveillance, Race, and the Future of Law Enforcement (New York, NY: NYU Press, 2017). 272 pp. $28.00 (hardcover), ISBN …\r## 16 The Rhetoric of De-Policing\r## 17 How Values Shape Program Perceptions: The “Organic Ethos” and Producers’ Assessments of US Organic Policy Impacts\r## 18 Emotional labour in non-governmental organisations: narrative analysis and theory expansion\r## 19 The UK\r## 20 Trending Interconnectedness: The Value of Comparative Analysis\r## 21 High-Stakes Administrative Discretion: What Drives Body-Worn Camera Activations?\r## 22 International Journal of Law, Crime and Justice\r## author\r## 1 I Adams, S Mastracci\r## 2 I Adams, S Mastracci\r## 3 S Mastracci, I Adams\r## 4 S Mastracci, I Adams\r## 5 I Adams, S Mastracci\r## 6 SM Mourtgos, IT Adams\r## 7 SH Mastracci, IT Adams\r## 8 SM Mourtgos, IT Adams\r## 9 SH Mastracci, IT Adams\r## 10 IT Adams, SH Mastracci\r## 11 IT Adams, SH Mastracci\r## 12 SM Mourtgos, IT Adams\r## 13 IT Adams, SH Mastracci\r## 14 SH Mastracci, IT Adams\r## 15 IT Adams\r## 16 SM Mourtgos, IT Adams\r## 17 DP Carter, SL Mosier, IT Adams\r## 18 SH Mastracci, IT Adams\r## 19 SH Mastracci, IT Adams, N Kang\r## 20 SH Mastracci, IT Adams\r## 21 IT Adams, SM Mourtgos, SH Mastracci\r## 22 SH Mastracci, IT Adams\r## journal\r## 1 Police quarterly\r## 2 Administrative Theory \u0026amp; Praxis\r## 3 Public Personnel Management\r## 4 Administrative Theory \u0026amp; Praxis\r## 5 Criminal Justice Review\r## 6 Justice Quarterly\r## 7 The Palgrave Handbook of Global Perspectives on Emotional Labor in Public …\r## 8 Journal of Criminal Justice\r## 9 International Journal of Law, Crime and Justice\r## 10 Emotional Labour in Criminal Justice and Criminology\r## 11 Emotional Labour in Criminal Justice and Criminology\r## 12 Criminal Justice Policy Review\r## 13 Policing: An International Journal\r## 14 Annals of Emergency Dispatch \u0026amp; Response\r## 15 Public Administration Review\r## 16 ## 17 Review of Policy Research\r## 18 International Journal of Work Organisation and Emotion\r## 19 The Palgrave Handbook of Global Perspectives on Emotional Labor in Public …\r## 20 The Palgrave Handbook of Global Perspectives on Emotional Labor in Public …\r## 21 Public Administration Review\r## 22 ## number cites year cid pubid\r## 1 22 (1), 5-30 46 2019 8532835669277965898 RHpTSmoSYBkC\r## 2 39 (4), 313-328 28 2017 11458448965364077389 ZeXyd9-uunAC\r## 3 48 (3), 325-344 16 2019 3113275916335416875 BqipwSGYUEgC\r## 4 40 (4), 304-319 9 2018 17350818719678078979 M3NEmzRMIkIC\r## 5 44 (3), 386-405 7 2019 17636589692170586414 J_g5lzvAfSwC\r## 6 37 (5), 869-899 6 2020 9895552701426227567 g5m5HwL7SMYC\r## 7 4 2019 1546338871969866120 2P1L_qKh6hAC\r## 8 64 (C), 1-1 3 2019 12823963095676900727 35N4QoGY0k4C\r## 9 61, 100358 1 2020 13462011810362789692 HoB7MX3m0LUC\r## 10 185 0 2020 \u0026lt;NA\u0026gt; yD5IFk8b50cC\r## 11 185-195 0 2020 \u0026lt;NA\u0026gt; dfsIfKJdRG4C\r## 12 31 (4), 479-499 0 2020 \u0026lt;NA\u0026gt; ns9cj8rnVeAC\r## 13 0 2020 \u0026lt;NA\u0026gt; 3s1wT3WcHBgC\r## 14 7 (3), 5-10 0 2020 \u0026lt;NA\u0026gt; zA6iFVUQeVQC\r## 15 79 (5), 791-793 0 2019 \u0026lt;NA\u0026gt; lSLTfruPkqcC\r## 16 0 2019 \u0026lt;NA\u0026gt; pqnbT2bcN3wC\r## 17 36 (3), 296-317 0 2019 \u0026lt;NA\u0026gt; RGFaLdJalmkC\r## 18 10 (1), 1-18 0 2019 \u0026lt;NA\u0026gt; u_35RYKgDlwC\r## 19 0 2019 \u0026lt;NA\u0026gt; M05iB0D1s5AC\r## 20 0 2019 \u0026lt;NA\u0026gt; ldfaerwXgEUC\r## 21 0 NA \u0026lt;NA\u0026gt; a0OBvERweLwC\r## 22 0 NA \u0026lt;NA\u0026gt; 4OULZ7Gr8RgC\r# Get number of articles\rget_num_articles(id)\r## [1] 22\r# Number of different journals published in\rget_num_distinct_journals(id)\r## [1] 16\r# Retrieve year of oldest publication\rget_oldest_article(id)\r## [1] 2017\r# Number of publications in \u0026quot;top\u0026quot; journals\rget_num_top_journals(id)\r## [1] 0\rNeed a little dose of imposter syndrome today? Dr. Hawking had 497 citations in his first career year. You can compare scholars based on their id.\n# Compare yourself and Stephen Hawking\rids \u0026lt;- c(\u0026#39;g9lY5RUAAAAJ\u0026#39;, \u0026#39;qj74uXkAAAAJ\u0026#39;) # Get a data frame comparing the number of citations to their work in\r# a given year compare_scholars(ids)\r## id year cites total name\r## 1 g9lY5RUAAAAJ 2017 28 28 Ian T. Adams\r## 2 g9lY5RUAAAAJ 2018 9 37 Ian T. Adams\r## 3 g9lY5RUAAAAJ 2019 76 113 Ian T. Adams\r## 4 g9lY5RUAAAAJ 2020 7 120 Ian T. Adams\r## 5 g9lY5RUAAAAJ NA 0 120 Ian T. Adams\r## 6 qj74uXkAAAAJ 1970 1991 1991 Stephen Hawking\r## 7 qj74uXkAAAAJ 1971 1182 3173 Stephen Hawking\r## 8 qj74uXkAAAAJ 1972 1393 4566 Stephen Hawking\r## 9 qj74uXkAAAAJ 1973 16927 21493 Stephen Hawking\r## 10 qj74uXkAAAAJ 1974 6809 28302 Stephen Hawking\r## 11 qj74uXkAAAAJ 1976 6502 34804 Stephen Hawking\r## 12 qj74uXkAAAAJ 1977 4804 39608 Stephen Hawking\r## 13 qj74uXkAAAAJ 1982 2299 41907 Stephen Hawking\r## 14 qj74uXkAAAAJ 1983 9915 51822 Stephen Hawking\r## 15 qj74uXkAAAAJ 2009 7949 59771 Stephen Hawking\r## 16 qj74uXkAAAAJ 2010 3538 63309 Stephen Hawking\r# Compare their career trajectories, based on year of first citation\rcompare_scholar_careers(ids)\r## id year cites career_year name\r## 1 g9lY5RUAAAAJ 2018 5 0 Ian T. Adams\r## 2 g9lY5RUAAAAJ 2019 40 1 Ian T. Adams\r## 3 g9lY5RUAAAAJ 2020 73 2 Ian T. Adams\r## 4 g9lY5RUAAAAJ 2021 1 3 Ian T. Adams\r## 5 qj74uXkAAAAJ 1982 497 0 Stephen Hawking\r## 6 qj74uXkAAAAJ 1983 608 1 Stephen Hawking\r## 7 qj74uXkAAAAJ 1984 803 2 Stephen Hawking\r## 8 qj74uXkAAAAJ 1985 837 3 Stephen Hawking\r## 9 qj74uXkAAAAJ 1986 953 4 Stephen Hawking\r## 10 qj74uXkAAAAJ 1987 836 5 Stephen Hawking\r## 11 qj74uXkAAAAJ 1988 872 6 Stephen Hawking\r## 12 qj74uXkAAAAJ 1989 1205 7 Stephen Hawking\r## 13 qj74uXkAAAAJ 1990 1145 8 Stephen Hawking\r## 14 qj74uXkAAAAJ 1991 1228 9 Stephen Hawking\r## 15 qj74uXkAAAAJ 1992 1290 10 Stephen Hawking\r## 16 qj74uXkAAAAJ 1993 1612 11 Stephen Hawking\r## 17 qj74uXkAAAAJ 1994 1667 12 Stephen Hawking\r## 18 qj74uXkAAAAJ 1995 1708 13 Stephen Hawking\r## 19 qj74uXkAAAAJ 1996 1764 14 Stephen Hawking\r## 20 qj74uXkAAAAJ 1997 1645 15 Stephen Hawking\r## 21 qj74uXkAAAAJ 1998 2135 16 Stephen Hawking\r## 22 qj74uXkAAAAJ 1999 2160 17 Stephen Hawking\r## 23 qj74uXkAAAAJ 2000 2113 18 Stephen Hawking\r## 24 qj74uXkAAAAJ 2001 2118 19 Stephen Hawking\r## 25 qj74uXkAAAAJ 2002 2386 20 Stephen Hawking\r## 26 qj74uXkAAAAJ 2003 2446 21 Stephen Hawking\r## 27 qj74uXkAAAAJ 2004 2491 22 Stephen Hawking\r## 28 qj74uXkAAAAJ 2005 2751 23 Stephen Hawking\r## 29 qj74uXkAAAAJ 2006 2877 24 Stephen Hawking\r## 30 qj74uXkAAAAJ 2007 3221 25 Stephen Hawking\r## 31 qj74uXkAAAAJ 2008 3444 26 Stephen Hawking\r## 32 qj74uXkAAAAJ 2009 3539 27 Stephen Hawking\r## 33 qj74uXkAAAAJ 2010 3590 28 Stephen Hawking\r## 34 qj74uXkAAAAJ 2011 4096 29 Stephen Hawking\r## 35 qj74uXkAAAAJ 2012 4067 30 Stephen Hawking\r## 36 qj74uXkAAAAJ 2013 4144 31 Stephen Hawking\r## 37 qj74uXkAAAAJ 2014 4627 32 Stephen Hawking\r## 38 qj74uXkAAAAJ 2015 4394 33 Stephen Hawking\r## 39 qj74uXkAAAAJ 2016 4611 34 Stephen Hawking\r## 40 qj74uXkAAAAJ 2017 5023 35 Stephen Hawking\r## 41 qj74uXkAAAAJ 2018 5530 36 Stephen Hawking\r## 42 qj74uXkAAAAJ 2019 5812 37 Stephen Hawking\r## 43 qj74uXkAAAAJ 2020 6206 38 Stephen Hawking\r## 44 qj74uXkAAAAJ 2021 96 39 Stephen Hawking\rWant to feel bad about yourself (part 2)? Use the prediction algorithm from Acuna et al. to see where you’ll be when you finally land that adjunct position!\n## Predict h-index of original method author, Daniel Acuna\rid \u0026lt;- \u0026#39;g9lY5RUAAAAJ\u0026#39;\rpredict \u0026lt;- predict_h_index(id)\rplot(predict)\r","date":1610323200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610323200,"objectID":"f0b625c423dbf6ccd27c644540b1f852","permalink":"https://ianadamsresearch.com/post/2021-01-11-fun-with-the-scholar-package/","publishdate":"2021-01-11T00:00:00Z","relpermalink":"/post/2021-01-11-fun-with-the-scholar-package/","section":"post","summary":"Playing around with the updated `scholar` package.","tags":[],"title":"Fun With the Scholar Package","type":"post"},{"authors":[],"categories":["website","R"],"content":"\r\rThis is a post to breadcrumb the migration of the previous website. Turns out that updating the hugo package, the blogdown package, the academic theme, and installing the Rstudio preview build all in the same day was a good way to blow up the previous iteration.\nNote that because of a change in the file structure under the new hugo, the image links from previous posts are going to break. When I have the time and interest, I’ll go back and fix.\nThe ease with which I can blow up a hugo/academic/blogdown website is equalled only by the utter incompetence I am capable of when trying to fix it.\n\u0026mdash; Ian T. Adams (@ian_t_adams) December 30, 2020  ","date":1609286400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609349931,"objectID":"a2f1400234c8362ed11ca25b8bdc1450","permalink":"https://ianadamsresearch.com/post/website-migrated/","publishdate":"2020-12-30T00:00:00Z","relpermalink":"/post/website-migrated/","section":"post","summary":"This is a post to breadcrumb the migration of the previous website. Turns out that updating the hugo package, the blogdown package, the academic theme, and installing the Rstudio preview build all in the same day was a good way to blow up the previous iteration.","tags":["test"],"title":"Website migrated","type":"post"},{"authors":null,"categories":["R"],"content":"\r\rOne of the wonderful features of the R statistical universe is the number of free, high-quality instructional materials available. Throughout the remainder of the course, we will be learning and working from the R for Data Science book (you’ll see this book shorthanded as R4DS in this course and all over the web), by two giants of the R space: Hadley Wickham and Garrett Grolemund . This is a free book, available here . If you prefer a hardcopy, you can order one from a number of sources as well, including at Amazon.\nAs you read through the assigned chapters, the authors have included a variety of practical exercises to introduce you to R, with a focus on the “tidyverse,” a collection of statistical packages that make exploring, analyzing, visualizing, and communicating data a more enjoyable, replicable, and easy-to-learn process. Some of these exercises will be assigned, but we also highly recommend you complete the exercises as you read through the book! These exercises are quite short, and usually have the coded answers built right into the book, so you can copy/paste right into RStudio and see the solution.\nData Visualization Exercises\rAlways remember to load your tidyverse library!\nlibrary(tidyverse)\r## -- Attaching packages --------------------------------------- tidyverse 1.3.0 --\r## v ggplot2 3.3.2 v purrr 0.3.4\r## v tibble 3.0.4 v dplyr 1.0.2\r## v tidyr 1.1.2 v stringr 1.4.0\r## v readr 1.4.0 v forcats 0.5.0\r## -- Conflicts ------------------------------------------ tidyverse_conflicts() --\r## x dplyr::filter() masks stats::filter()\r## x dplyr::lag() masks stats::lag()\r\rFirst steps\rExercise 3.2.1\rRun ggplot(data = mpg) what do you see?\n\rggplot(data = mpg)\rThis code creates an empty plot. The ggplot() function creates the background of the plot, but since no layers were specified with geom function, nothing is drawn.\n\r\rExercise 3.2.2\rHow many rows are in mpg? How many columns?\n\rThere are 234 rows and 11 columns in the mpg data frame.\nnrow(mpg)\r## [1] 234\rncol(mpg)\r## [1] 11\rThe glimpse() function also displays the number of rows and columns in a data frame.\nglimpse(mpg)\r## Rows: 234\r## Columns: 11\r## $ manufacturer \u0026lt;chr\u0026gt; \u0026quot;audi\u0026quot;, \u0026quot;audi\u0026quot;, \u0026quot;audi\u0026quot;, \u0026quot;audi\u0026quot;, \u0026quot;audi\u0026quot;, \u0026quot;audi\u0026quot;, \u0026quot;audi\u0026quot;...\r## $ model \u0026lt;chr\u0026gt; \u0026quot;a4\u0026quot;, \u0026quot;a4\u0026quot;, \u0026quot;a4\u0026quot;, \u0026quot;a4\u0026quot;, \u0026quot;a4\u0026quot;, \u0026quot;a4\u0026quot;, \u0026quot;a4\u0026quot;, \u0026quot;a4 quattro\u0026quot;...\r## $ displ \u0026lt;dbl\u0026gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0,...\r## $ year \u0026lt;int\u0026gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, ...\r## $ cyl \u0026lt;int\u0026gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, ...\r## $ trans \u0026lt;chr\u0026gt; \u0026quot;auto(l5)\u0026quot;, \u0026quot;manual(m5)\u0026quot;, \u0026quot;manual(m6)\u0026quot;, \u0026quot;auto(av)\u0026quot;, \u0026quot;a...\r## $ drv \u0026lt;chr\u0026gt; \u0026quot;f\u0026quot;, \u0026quot;f\u0026quot;, \u0026quot;f\u0026quot;, \u0026quot;f\u0026quot;, \u0026quot;f\u0026quot;, \u0026quot;f\u0026quot;, \u0026quot;f\u0026quot;, \u0026quot;4\u0026quot;, \u0026quot;4\u0026quot;, \u0026quot;4\u0026quot;, \u0026quot;4\u0026quot;,...\r## $ cty \u0026lt;int\u0026gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17...\r## $ hwy \u0026lt;int\u0026gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25...\r## $ fl \u0026lt;chr\u0026gt; \u0026quot;p\u0026quot;, \u0026quot;p\u0026quot;, \u0026quot;p\u0026quot;, \u0026quot;p\u0026quot;, \u0026quot;p\u0026quot;, \u0026quot;p\u0026quot;, \u0026quot;p\u0026quot;, \u0026quot;p\u0026quot;, \u0026quot;p\u0026quot;, \u0026quot;p\u0026quot;, \u0026quot;p\u0026quot;,...\r## $ class \u0026lt;chr\u0026gt; \u0026quot;compact\u0026quot;, \u0026quot;compact\u0026quot;, \u0026quot;compact\u0026quot;, \u0026quot;compact\u0026quot;, \u0026quot;compact\u0026quot;,...\r\r\rExercise 3.2.3\rWhat does the drv variable describe? Read the help for ?mpg to find out.\n\rThe drv variable is a categorical variable which categorizes cars into front-wheels, rear-wheels, or four-wheel drive.\n\r\rValue\rDescription\r\r\r\r\"f\"\rfront-wheel drive\r\r\"r\"\rrear-wheel drive\r\r\"4\"\rfour-wheel drive\r\r\r\r\r\rExercise 3.2.4\rMake a scatter plot of hwy vs. cyl.\n\rggplot(mpg, aes(x = cyl, y = hwy)) +\rgeom_point()\r\r\rExercise 3.2.5\rWhat happens if you make a scatter plot of class vs drv? Why is the plot not useful?\n\rThe resulting scatterplot has only a few points.\nggplot(mpg, aes(x = class, y = drv)) +\rgeom_point()\rA scatter plot is not a useful display of these variables since both drv and class are categorical variables. Since categorical variables typically take a small number of values, there are a limited number of unique combinations of (x, y) values that can be displayed. In this data, drv takes 3 values and class takes 7 values, meaning that there are only 21 values that could be plotted on a scatterplot of drv vs. class. In this data, there 12 values of (drv, class) are observed.\ncount(mpg, drv, class)\r## # A tibble: 12 x 3\r## drv class n\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 4 compact 12\r## 2 4 midsize 3\r## 3 4 pickup 33\r## 4 4 subcompact 4\r## 5 4 suv 51\r## 6 f compact 35\r## 7 f midsize 38\r## 8 f minivan 11\r## 9 f subcompact 22\r## 10 r 2seater 5\r## 11 r subcompact 9\r## 12 r suv 11\rA simple scatter plot does not show how many observations there are for each (x, y) value. As such, scatterplots work best for plotting a continuous x and a continuous y variable, and when all (x, y) values are unique.\nWarning: The following code uses functions introduced in a later section. Come back to this after reading section 7.5.2, which introduces methods for plotting two categorical variables. The first is geom_count() which is similar to a scatterplot but uses the size of the points to show the number of observations at an (x, y) point.\nggplot(mpg, aes(x = class, y = drv)) +\rgeom_count()\rThe second is geom_tile() which uses a color scale to show the number of observations with each (x, y) value.\nmpg %\u0026gt;%\rcount(class, drv) %\u0026gt;%\rggplot(aes(x = class, y = drv)) +\rgeom_tile(mapping = aes(fill = n))\rIn the previous plot, there are many missing tiles. These missing tiles represent unobserved combinations of class and drv values. These missing values are not unknown, but represent values of (class, drv) where n = 0. The complete() function in the tidyr package adds new rows to a data frame for missing combinations of columns. The following code adds rows for missing combinations of class and drv and uses the fill argument to set n = 0 for those new rows.\nmpg %\u0026gt;%\rcount(class, drv) %\u0026gt;%\rcomplete(class, drv, fill = list(n = 0)) %\u0026gt;%\rggplot(aes(x = class, y = drv)) +\rgeom_tile(mapping = aes(fill = n))\r\r\r\r3.3 Aesthetic mappings\rExercise 3.3.1\rWhat’s gone wrong with this code? Why are the points not blue?\nggplot(data = mpg) +\rgeom_point(mapping = aes(x = displ, y = hwy, colour = \u0026quot;blue\u0026quot;))\r\rThe argumentcolour = \"blue\" is included within the mapping argument, and as such, it is treated as an aesthetic, which is a mapping between a variable and a value. In the expression, colour = \"blue\", \"blue\" is interpreted as a categorical variable which only takes a single value \"blue\". If this is confusing, consider how colour = 1:234 and colour = 1 are interpreted by aes().\nThe following code does produces the expected result.\nggplot(data = mpg) +\rgeom_point(mapping = aes(x = displ, y = hwy), colour = \u0026quot;blue\u0026quot;)\r\r\rExercise 3.3.2\rWhich variables in mpg are categorical? Which variables are continuous? (Hint: type ?mpg to read the documentation for the dataset). How can you see this information when you run mpg?\n\rThe following list contains the categorical variables in mpg:\n\rmanufacturer\rmodel\rtrans\rdrv\rfl\rclass\r\rThe following list contains the continuous variables in mpg:\n\rdispl\ryear\rcyl\rcty\rhwy\r\rIn the printed data frame, angled brackets at the top of each column provide type of each variable.\nmpg\r## # A tibble: 234 x 11\r## manufacturer model displ year cyl trans drv cty hwy fl class\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt;\r## 1 audi a4 1.8 1999 4 auto(l~ f 18 29 p comp~\r## 2 audi a4 1.8 1999 4 manual~ f 21 29 p comp~\r## 3 audi a4 2 2008 4 manual~ f 20 31 p comp~\r## 4 audi a4 2 2008 4 auto(a~ f 21 30 p comp~\r## 5 audi a4 2.8 1999 6 auto(l~ f 16 26 p comp~\r## 6 audi a4 2.8 1999 6 manual~ f 18 26 p comp~\r## 7 audi a4 3.1 2008 6 auto(a~ f 18 27 p comp~\r## 8 audi a4 quat~ 1.8 1999 4 manual~ 4 18 26 p comp~\r## 9 audi a4 quat~ 1.8 1999 4 auto(l~ 4 16 25 p comp~\r## 10 audi a4 quat~ 2 2008 4 manual~ 4 20 28 p comp~\r## # ... with 224 more rows\rThose with \u0026lt;chr\u0026gt; above their columns are categorical, while those with \u0026lt;dbl\u0026gt; or \u0026lt;int\u0026gt; are continuous. The exact meaning of these types will be discussed in “Chapter 15: Vectors”.\nglimpse() is another function that concisely displays the type of each column in the data frame:\nglimpse(mpg)\r## Rows: 234\r## Columns: 11\r## $ manufacturer \u0026lt;chr\u0026gt; \u0026quot;audi\u0026quot;, \u0026quot;audi\u0026quot;, \u0026quot;audi\u0026quot;, \u0026quot;audi\u0026quot;, \u0026quot;audi\u0026quot;, \u0026quot;audi\u0026quot;, \u0026quot;audi\u0026quot;...\r## $ model \u0026lt;chr\u0026gt; \u0026quot;a4\u0026quot;, \u0026quot;a4\u0026quot;, \u0026quot;a4\u0026quot;, \u0026quot;a4\u0026quot;, \u0026quot;a4\u0026quot;, \u0026quot;a4\u0026quot;, \u0026quot;a4\u0026quot;, \u0026quot;a4 quattro\u0026quot;...\r## $ displ \u0026lt;dbl\u0026gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0,...\r## $ year \u0026lt;int\u0026gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, ...\r## $ cyl \u0026lt;int\u0026gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, ...\r## $ trans \u0026lt;chr\u0026gt; \u0026quot;auto(l5)\u0026quot;, \u0026quot;manual(m5)\u0026quot;, \u0026quot;manual(m6)\u0026quot;, \u0026quot;auto(av)\u0026quot;, \u0026quot;a...\r## $ drv \u0026lt;chr\u0026gt; \u0026quot;f\u0026quot;, \u0026quot;f\u0026quot;, \u0026quot;f\u0026quot;, \u0026quot;f\u0026quot;, \u0026quot;f\u0026quot;, \u0026quot;f\u0026quot;, \u0026quot;f\u0026quot;, \u0026quot;4\u0026quot;, \u0026quot;4\u0026quot;, \u0026quot;4\u0026quot;, \u0026quot;4\u0026quot;,...\r## $ cty \u0026lt;int\u0026gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17...\r## $ hwy \u0026lt;int\u0026gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25...\r## $ fl \u0026lt;chr\u0026gt; \u0026quot;p\u0026quot;, \u0026quot;p\u0026quot;, \u0026quot;p\u0026quot;, \u0026quot;p\u0026quot;, \u0026quot;p\u0026quot;, \u0026quot;p\u0026quot;, \u0026quot;p\u0026quot;, \u0026quot;p\u0026quot;, \u0026quot;p\u0026quot;, \u0026quot;p\u0026quot;, \u0026quot;p\u0026quot;,...\r## $ class \u0026lt;chr\u0026gt; \u0026quot;compact\u0026quot;, \u0026quot;compact\u0026quot;, \u0026quot;compact\u0026quot;, \u0026quot;compact\u0026quot;, \u0026quot;compact\u0026quot;,...\rFor those lists, I considered any variable that was non-numeric was considered categorical and any variable that was numeric was considered continuous. This largely corresponds to the heuristics ggplot() uses for will interpreting variables as discrete or continuous.\nHowever, this definition of continuous vs. categorical misses several important cases. Of the numeric variables, year and cyl (cylinders) clearly take on discrete values. The variables cty and hwy are stored as integers (int) so they only take on a discrete values. Even though displ has In some sense, due to measurement and computational constraints all numeric variables are discrete (). But unlike the categorical variables, it is possible to add and subtract these numeric variables in a meaningful way. The typology of levels of measurement is one such typology of data types.\nIn this case the R data types largely encode the semantics of the variables; e.g. integer variables are stored as integers, categorical variables with no order are stored as character vectors and so on. However, that is not always the case. Instead, the data could have stored the categorical class variable as an integer with values 1–7, where the documentation would note that 1 = “compact”, 2 = “midsize”, and so on.1 Even though this integer vector could be added, multiplied, subtracted, and divided, those operations would be meaningless.\nFundamentally, categorizing variables as “discrete”, “continuous”, “ordinal”, “nominal”, “categorical”, etc. is about specifying what operations can be performed on the variables. Discrete variables support counting and calculating the mode. Variables with an ordering support sorting and calculating quantiles. Variables that have an interval scale support addition and subtraction and operations such as taking the mean that rely on these primitives. In this way, the types of data or variables types is an information class system, something that is beyond the scope of R4DS but discussed in Advanced R.\n\r\rExercise 3.3.3\rMap a continuous variable to color, size, and shape. How do these aesthetics behave differently for categorical vs. continuous variables?\n\rThe variable cty, city highway miles per gallon, is a continuous variable.\nggplot(mpg, aes(x = displ, y = hwy, colour = cty)) +\rgeom_point()\rInstead of using discrete colors, the continuous variable uses a scale that varies from a light to dark blue color.\nggplot(mpg, aes(x = displ, y = hwy, size = cty)) +\rgeom_point()\rWhen mapped to size, the sizes of the points vary continuously as a function of their size.\nggplot(mpg, aes(x = displ, y = hwy, shape = cty)) +\rgeom_point()\r## Error: A continuous variable can not be mapped to shape\rWhen a continuous value is mapped to shape, it gives an error. Though we could split a continuous variable into discrete categories and use a shape aesthetic, this would conceptually not make sense. A numeric variable has an order, but shapes do not. It is clear that smaller points correspond to smaller values, or once the color scale is given, which colors correspond to larger or smaller values. But it is not clear whether a square is greater or less than a circle.\n\r\rExercise 3.3.4\rWhat happens if you map the same variable to multiple aesthetics?\n\rggplot(mpg, aes(x = displ, y = hwy, colour = hwy, size = displ)) +\rgeom_point()\rIn the above plot, hwy is mapped to both location on the y-axis and color, and displ is mapped to both location on the x-axis and size. The code works and produces a plot, even if it is a bad one. Mapping a single variable to multiple aesthetics is redundant. Because it is redundant information, in most cases avoid mapping a single variable to multiple aesthetics.\n\r\rExercise 3.3.5\rWhat does the stroke aesthetic do? What shapes does it work with? (Hint: use ?geom_point)\n\rStroke changes the size of the border for shapes (21-25). These are filled shapes in which the color and size of the border can differ from that of the filled interior of the shape.\nFor example\nggplot(mtcars, aes(wt, mpg)) +\rgeom_point(shape = 21, colour = \u0026quot;black\u0026quot;, fill = \u0026quot;white\u0026quot;, size = 5, stroke = 5)\r\r\rExercise 3.3.6\rWhat happens if you map an aesthetic to something other than a variable name, like aes(colour = displ \u0026lt; 5)?\n\rggplot(mpg, aes(x = displ, y = hwy, colour = displ \u0026lt; 5)) +\rgeom_point()\rAesthetics can also be mapped to expressions like displ \u0026lt; 5. The ggplot() function behaves as if a temporary variable was added to the data with values equal to the result of the expression. In this case, the result of displ \u0026lt; 5 is a logical variable which takes values of TRUE or FALSE.\nThis also explains why, in Exercise 3.3.1, the expression colour = \"blue\" created a categorical variable with only one category: “blue”.\n\r\rFacets\rExercise 3.5.1\rWhat happens if you facet on a continuous variable?\n\rLet’s see.\nggplot(mpg, aes(x = displ, y = hwy)) +\rgeom_point() +\rfacet_grid(. ~ cty)\rThe continuous variable is converted to a categorical variable, and the plot contains a facet for each distinct value.\n\r\rExercise 3.5.2\rWhat do the empty cells in plot with facet_grid(drv ~ cyl) mean? How do they relate to this plot?\nggplot(data = mpg) +\rgeom_point(mapping = aes(x = drv, y = cyl))\r\rggplot(data = mpg) +\rgeom_point(mapping = aes(x = hwy, y = cty)) +\rfacet_grid(drv ~ cyl)\rThe empty cells (facets) in this plot are combinations of drv and cyl that have no observations. These are the same locations in the scatter plot of drv and cyl that have no points.\nggplot(data = mpg) +\rgeom_point(mapping = aes(x = drv, y = cyl))\r\r\rExercise 3.5.3\rWhat plots does the following code make? What does . do?\n\rThe symbol . ignores that dimension when faceting. For example, drv ~ . facet by values of drv on the y-axis.\nggplot(data = mpg) +\rgeom_point(mapping = aes(x = displ, y = hwy)) +\rfacet_grid(drv ~ .)\rWhile, . ~ cyl will facet by values of cyl on the x-axis.\nggplot(data = mpg) +\rgeom_point(mapping = aes(x = displ, y = hwy)) +\rfacet_grid(. ~ cyl)\r\r\rExercise 3.5.4\rTake the first faceted plot in this section:\nggplot(data = mpg) +\rgeom_point(mapping = aes(x = displ, y = hwy)) +\rfacet_wrap(~class, nrow = 2)\rWhat are the advantages to using faceting instead of the colour aesthetic? What are the disadvantages? How might the balance change if you had a larger dataset?\n\rIn the following plot the class variable is mapped to color.\nggplot(data = mpg) +\rgeom_point(mapping = aes(x = displ, y = hwy, color = class))\rAdvantages of encoding class with facets instead of color include the ability to encode more distinct categories. For me, it is difficult to distinguish between the colors of \"midsize\" and \"minivan\".\nGiven human visual perception, the max number of colors to use when encoding unordered categorical (qualitative) data is nine, and in practice, often much less than that. Displaying observations from different categories on different scales makes it difficult to directly compare values of observations across categories. However, it can make it easier to compare the shape of the relationship between the x and y variables across categories.\nDisadvantages of encoding the class variable with facets instead of the color aesthetic include the difficulty of comparing the values of observations between categories since the observations for each category are on different plots. Using the same x- and y-scales for all facets makes it easier to compare values of observations across categories, but it is still more difficult than if they had been displayed on the same plot. Since encoding class within color also places all points on the same plot, it visualizes the unconditional relationship between the x and y variables; with facets, the unconditional relationship is no longer visualized since the points are spread across multiple plots.\nThe benefit of encoding a variable with facetting over encoding it with color increase in both the number of points and the number of categories. With a large number of points, there is often overlap. It is difficult to handle overlapping points with different colors color. Jittering will still work with color. But jittering will only work well if there are few points and the classes do not overlap much, otherwise, the colors of areas will no longer be distinct, and it will be hard to pick out the patterns of different categories visually. Transparency (alpha) does not work well with colors since the mixing of overlapping transparent colors will no longer represent the colors of the categories. Binning methods already use color to encode the density of points in the bin, so color cannot be used to encode categories.\nAs the number of categories increases, the difference between colors decreases, to the point that the color of categories will no longer be visually distinct.\n\r\rExercise 3.5.5\rRead ?facet_wrap. What does nrow do? What does ncol do? What other options control the layout of the individual panels? Why doesn’t facet_grid() have nrow and ncol variables?\n\rThe arguments nrow (ncol) determines the number of rows (columns) to use when laying out the facets. It is necessary since facet_wrap() only facets on one variable.\nThe nrow and ncol arguments are unnecessary for facet_grid() since the number of unique values of the variables specified in the function determines the number of rows and columns.\n\r\rExercise 3.5.6\rWhen using facet_grid() you should usually put the variable with more unique levels in the columns. Why?\n\rThere will be more space for columns if the plot is laid out horizontally (landscape).\n\r\r\rGeometric objects\rExercise 3.6.1\rWhat geom would you use to draw a line chart? A boxplot? A histogram? An area chart?\n\r\rline chart: geom_line()\rboxplot: geom_boxplot()\rhistogram: geom_histogram()\rarea chart: geom_area()\r\r\r\rExercise 3.6.2\rRun this code in your head and predict what the output will look like. Then, run the code in R and check your predictions.\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, colour = drv)) +\rgeom_point() +\rgeom_smooth(se = FALSE)\r## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39;\r\rThis code produces a scatter plot with displ on the x-axis, hwy on the y-axis, and the points colored by drv. There will be a smooth line, without standard errors, fit through each drv group.\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, colour = drv)) +\rgeom_point() +\rgeom_smooth(se = FALSE)\r## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39;\r\r\rExercise 3.6.3\rWhat does show.legend = FALSE do? What happens if you remove it? Why do you think I used it earlier in the chapter?\n\rThe theme option show.legend = FALSE hides the legend box.\nConsider this example earlier in the chapter.\nggplot(data = mpg) +\rgeom_smooth(\rmapping = aes(x = displ, y = hwy, colour = drv),\rshow.legend = FALSE\r)\r## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39;\rIn that plot, there is no legend. Removing the show.legend argument or setting show.legend = TRUE will result in the plot having a legend displaying the mapping between colors and drv.\nggplot(data = mpg) +\rgeom_smooth(mapping = aes(x = displ, y = hwy, colour = drv))\r## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39;\rIn the chapter, the legend is suppressed because with three plots, adding a legend to only the last plot would make the sizes of plots different. Different sized plots would make it more difficult to see how arguments change the appearance of the plots. The purpose of those plots is to show the difference between no groups, using a group aesthetic, and using a color aesthetic, which creates implicit groups. In that example, the legend isn’t necessary since looking up the values associated with each color isn’t necessary to make that point.\n\r\rExercise 3.6.4\rWhat does the se argument to geom_smooth() do?\n\rIt adds standard error bands to the lines.\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, colour = drv)) +\rgeom_point() +\rgeom_smooth(se = TRUE)\r## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39;\rBy default se = TRUE:\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, colour = drv)) +\rgeom_point() +\rgeom_smooth()\r## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39;\r\r\rExercise 3.6.5\rWill these two graphs look different? Why/why not?\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) +\rgeom_point() +\rgeom_smooth()\rggplot() +\rgeom_point(data = mpg, mapping = aes(x = displ, y = hwy)) +\rgeom_smooth(data = mpg, mapping = aes(x = displ, y = hwy))\r\rNo. Because both geom_point() and geom_smooth() will use the same data and mappings. They will inherit those options from the ggplot() object, so the mappings don’t need to specified again.\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) +\rgeom_point() +\rgeom_smooth()\r## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39;\rggplot() +\rgeom_point(data = mpg, mapping = aes(x = displ, y = hwy)) +\rgeom_smooth(data = mpg, mapping = aes(x = displ, y = hwy))\r## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39;\r\r\rExercise 3.6.6\rRecreate the R code necessary to generate the following graphs.\n\r\rinsert the figures from the book later\n\r\r\rThe following code will generate those plots.\nggplot(mpg, aes(x = displ, y = hwy)) +\rgeom_point() +\rgeom_smooth(se = FALSE)\rggplot(mpg, aes(x = displ, y = hwy)) +\rgeom_smooth(mapping = aes(group = drv), se = FALSE) +\rgeom_point()\rggplot(mpg, aes(x = displ, y = hwy, colour = drv)) +\rgeom_point() +\rgeom_smooth(se = FALSE)\rggplot(mpg, aes(x = displ, y = hwy)) +\rgeom_point(aes(colour = drv)) +\rgeom_smooth(se = FALSE)\rggplot(mpg, aes(x = displ, y = hwy)) +\rgeom_point(aes(colour = drv)) +\rgeom_smooth(aes(linetype = drv), se = FALSE)\r## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39;\rggplot(mpg, aes(x = displ, y = hwy)) +\rgeom_point(size = 4, color = \u0026quot;white\u0026quot;) +\rgeom_point(aes(colour = drv))\r\r\r\rStatistical transformations\rExercise 3.7.1\rWhat is the default geom associated with stat_summary()? How could you rewrite the previous plot to use that geom function instead of the stat function?\n\rThe “previous plot” referred to in the question is the following.\nggplot(data = diamonds) +\rstat_summary(\rmapping = aes(x = cut, y = depth),\rfun.min = min,\rfun.max = max,\rfun = median\r)\rThe arguments fun.ymin, fun.ymax, and fun.y have been deprecated and replaced with fun.min, fun.max, and fun in ggplot2 v 3.3.0.\nThe default geom for stat_summary() is geom_pointrange(). The default stat for geom_pointrange() is identity() but we can add the argument stat = \"summary\" to use stat_summary() instead of stat_identity().\nggplot(data = diamonds) +\rgeom_pointrange(\rmapping = aes(x = cut, y = depth),\rstat = \u0026quot;summary\u0026quot;\r)\r## No summary function supplied, defaulting to `mean_se()`\rThe resulting message says that stat_summary() uses the mean and sd to calculate the middle point and endpoints of the line. However, in the original plot the min and max values were used for the endpoints. To recreate the original plot we need to specify values for fun.min, fun.max, and fun.\nggplot(data = diamonds) +\rgeom_pointrange(\rmapping = aes(x = cut, y = depth),\rstat = \u0026quot;summary\u0026quot;,\rfun.min = min,\rfun.max = max,\rfun = median\r)\r\r\rExercise 3.7.2\rWhat does geom_col() do? How is it different to geom_bar()?\n\rThe geom_col() function has different default stat than geom_bar(). The default stat of geom_col() is stat_identity(), which leaves the data as is. The geom_col() function expects that the data contains x values and y values which represent the bar height.\nThe default stat of geom_bar() is stat_count(). The geom_bar() function only expects an x variable. The stat, stat_count(), preprocesses input data by counting the number of observations for each value of x. The y aesthetic uses the values of these counts.\n\r\rExercise 3.7.3\rMost geoms and stats come in pairs that are almost always used in concert. Read through the documentation and make a list of all the pairs. What do they have in common?\n\rThe following tables lists the pairs of geoms and stats that are almost always used in concert.\n\rComplementary geoms and stats\r\rgeom\rstat\r\r\r\rgeom_bar()\rstat_count()\r\rgeom_bin2d()\rstat_bin_2d()\r\rgeom_boxplot()\rstat_boxplot()\r\rgeom_contour_filled()\rstat_contour_filled()\r\rgeom_contour()\rstat_contour()\r\rgeom_count()\rstat_sum()\r\rgeom_density_2d()\rstat_density_2d()\r\rgeom_density()\rstat_density()\r\rgeom_dotplot()\rstat_bindot()\r\rgeom_function()\rstat_function()\r\rgeom_sf()\rstat_sf()\r\rgeom_sf()\rstat_sf()\r\rgeom_smooth()\rstat_smooth()\r\rgeom_violin()\rstat_ydensity()\r\rgeom_hex()\rstat_bin_hex()\r\rgeom_qq_line()\rstat_qq_line()\r\rgeom_qq()\rstat_qq()\r\rgeom_quantile()\rstat_quantile()\r\r\r\rThese pairs of geoms and stats tend to have their names in common, such stat_smooth() and geom_smooth() and be documented on the same help page. The pairs of geoms and stats that are used in concert often have each other as the default stat (for a geom) or geom (for a stat).\nThe following tables contain the geoms and stats in ggplot2 and their defaults as of version 3.3.0. Many geoms have stat_identity() as the default stat.\n\rggplot2 geom layers and their default stats.\r\rgeom\rdefault stat\rshared docs\r\r\r\rgeom_abline()\rstat_identity()\r\r\rgeom_area()\rstat_identity()\r\r\rgeom_bar()\rstat_count()\rx\r\rgeom_bin2d()\rstat_bin_2d()\rx\r\rgeom_blank()\rNone\r\r\rgeom_boxplot()\rstat_boxplot()\rx\r\rgeom_col()\rstat_identity()\r\r\rgeom_count()\rstat_sum()\rx\r\rgeom_countour_filled()\rstat_countour_filled()\rx\r\rgeom_countour()\rstat_countour()\rx\r\rgeom_crossbar()\rstat_identity()\r\r\rgeom_curve()\rstat_identity()\r\r\rgeom_density_2d_filled()\rstat_density_2d_filled()\rx\r\rgeom_density_2d()\rstat_density_2d()\rx\r\rgeom_density()\rstat_density()\rx\r\rgeom_dotplot()\rstat_bindot()\rx\r\rgeom_errorbar()\rstat_identity()\r\r\rgeom_errorbarh()\rstat_identity()\r\r\rgeom_freqpoly()\rstat_bin()\rx\r\rgeom_function()\rstat_function()\rx\r\rgeom_hex()\rstat_bin_hex()\rx\r\rgeom_histogram()\rstat_bin()\rx\r\rgeom_hline()\rstat_identity()\r\r\rgeom_jitter()\rstat_identity()\r\r\rgeom_label()\rstat_identity()\r\r\rgeom_line()\rstat_identity()\r\r\rgeom_linerange()\rstat_identity()\r\r\rgeom_map()\rstat_identity()\r\r\rgeom_path()\rstat_identity()\r\r\rgeom_point()\rstat_identity()\r\r\rgeom_pointrange()\rstat_identity()\r\r\rgeom_polygon()\rstat_identity()\r\r\rgeom_qq_line()\rstat_qq_line()\rx\r\rgeom_qq()\rstat_qq()\rx\r\rgeom_quantile()\rstat_quantile()\rx\r\rgeom_raster()\rstat_identity()\r\r\rgeom_rect()\rstat_identity()\r\r\rgeom_ribbon()\rstat_identity()\r\r\rgeom_rug()\rstat_identity()\r\r\rgeom_segment()\rstat_identity()\r\r\rgeom_sf_label()\rstat_sf_coordinates()\rx\r\rgeom_sf_text()\rstat_sf_coordinates()\rx\r\rgeom_sf()\rstat_sf()\rx\r\rgeom_smooth()\rstat_smooth()\rx\r\rgeom_spoke()\rstat_identity()\r\r\rgeom_step()\rstat_identity()\r\r\rgeom_text()\rstat_identity()\r\r\rgeom_tile()\rstat_identity()\r\r\rgeom_violin()\rstat_ydensity()\rx\r\rgeom_vline()\rstat_identity()\r\r\r\r\r\rggplot2 stat layers and their default geoms.\r\rstat\rdefault geom\rshared docs\r\r\r\rstat_bin_2d()\rgeom_tile()\r\r\rstat_bin_hex()\rgeom_hex()\rx\r\rstat_bin()\rgeom_bar()\rx\r\rstat_boxplot()\rgeom_boxplot()\rx\r\rstat_count()\rgeom_bar()\rx\r\rstat_countour_filled()\rgeom_contour_filled()\rx\r\rstat_countour()\rgeom_contour()\rx\r\rstat_density_2d_filled()\rgeom_density_2d()\rx\r\rstat_density_2d()\rgeom_density_2d()\rx\r\rstat_density()\rgeom_area()\r\r\rstat_ecdf()\rgeom_step()\r\r\rstat_ellipse()\rgeom_path()\r\r\rstat_function()\rgeom_function()\rx\r\rstat_function()\rgeom_path()\r\r\rstat_identity()\rgeom_point()\r\r\rstat_qq_line()\rgeom_path()\r\r\rstat_qq()\rgeom_point()\r\r\rstat_quantile()\rgeom_quantile()\rx\r\rstat_sf_coordinates()\rgeom_point()\r\r\rstat_sf()\rgeom_rect()\r\r\rstat_smooth()\rgeom_smooth()\rx\r\rstat_sum()\rgeom_point()\r\r\rstat_summary_2d()\rgeom_tile()\r\r\rstat_summary_bin()\rgeom_pointrange()\r\r\rstat_summary_hex()\rgeom_hex()\r\r\rstat_summary()\rgeom_pointrange()\r\r\rstat_unique()\rgeom_point()\r\r\rstat_ydensity()\rgeom_violin()\rx\r\r\r\r\r\rExercise 3.7.4\rWhat variables does stat_smooth() compute? What parameters control its behavior?\n\rThe function stat_smooth() calculates the following variables:\n\ry: predicted value\rymin: lower value of the confidence interval\rymax: upper value of the confidence interval\rse: standard error\r\rThe “Computed Variables” section of the stat_smooth() documentation contains these variables.\nThe parameters that control the behavior of stat_smooth() include:\n\rmethod: This is the method used to compute the smoothing line. If NULL, a default method is used based on the sample size: stats::loess() when there are less than 1,000 observations in a group, and mgcv::gam() with formula = y ~ s(x, bs = \"CS) otherwise. Alternatively, the user can provide a character vector with a function name, e.g. \"lm\", \"loess\", or a function, e.g. MASS::rlm.\n\rformula: When providing a custom method argument, the formula to use. The default is y ~ x. For example, to use the line implied by lm(y ~ x + I(x ^ 2) + I(x ^ 3)), use method = \"lm\" or method = lm and formula = y ~ x + I(x ^ 2) + I(x ^ 3).\n\rmethod.arg(): Arguments other than than the formula, which is already specified in the formula argument, to pass to the function inmethod`.\n\rse: If TRUE, display standard error bands, if FALSE only display the line.\n\rna.rm: If FALSE, missing values are removed with a warning, if TRUE the are silently removed. The default is FALSE in order to make debugging easier. If missing values are known to be in the data, then can be ignored, but if missing values are not anticipated this warning can help catch errors.\n\r\rTODO: Plots with examples illustrating the uses of these arguments.\n\r\rExercise 3.7.5\rIn our proportion bar chart, we need to set group = 1 Why? In other words, what is the problem with these two graphs?\n\rIf group = 1 is not included, then all the bars in the plot will have the same height, a height of 1. The function geom_bar() assumes that the groups are equal to the x values since the stat computes the counts within the group.\nggplot(data = diamonds) +\rgeom_bar(mapping = aes(x = cut, y = ..prop..))\rThe problem with these two plots is that the proportions are calculated within the groups.\nggplot(data = diamonds) +\rgeom_bar(mapping = aes(x = cut, y = ..prop..))\rggplot(data = diamonds) +\rgeom_bar(mapping = aes(x = cut, fill = color, y = ..prop..))\rThe following code will produce the intended stacked bar charts for the case with no fill aesthetic.\nggplot(data = diamonds) +\rgeom_bar(mapping = aes(x = cut, y = ..prop.., group = 1))\rWith the fill aesthetic, the heights of the bars need to be normalized.\nggplot(data = diamonds) + geom_bar(aes(x = cut, y = ..count.. / sum(..count..), fill = color))\r\r\r\rPosition adjustments\rExercise 3.8.1\rWhat is the problem with this plot? How could you improve it?\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +\rgeom_point()\r\rThere is overplotting because there are multiple observations for each combination of cty and hwy values.\nI would improve the plot by using a jitter position adjustment to decrease overplotting.\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +\rgeom_point(position = \u0026quot;jitter\u0026quot;)\rThe relationship between cty and hwy is clear even without jittering the points but jittering shows the locations where there are more observations.\n\r\rExercise 3.8.2\rWhat parameters to geom_jitter() control the amount of jittering?\n\rFrom the geom_jitter() documentation, there are two arguments to jitter:\n\rwidth controls the amount of horizontal displacement, and\rheight controls the amount of vertical displacement.\r\rThe defaults values of width and height will introduce noise in both directions. Here is what the plot looks like with the default values of height and width.\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +\rgeom_point(position = position_jitter())\rHowever, we can change these parameters. Here are few a examples to understand how these parameters affect the amount of jittering. Whenwidth = 0 there is no horizontal jitter.\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +\rgeom_jitter(width = 0)\rWhen width = 20, there is too much horizontal jitter.\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +\rgeom_jitter(width = 20)\rWhen height = 0, there is no vertical jitter.\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +\rgeom_jitter(height = 0)\rWhen height = 15, there is too much vertical jitter.\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +\rgeom_jitter(height = 15)\rWhen width = 0 and height = 0, there is neither horizontal or vertical jitter, and the plot produced is identical to the one produced with geom_point().\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +\rgeom_jitter(height = 0, width = 0)\rNote that the height and width arguments are in the units of the data. Thus height = 1 (width = 1) corresponds to different relative amounts of jittering depending on the scale of the y (x) variable. The default values of height and width are defined to be 80% of the resolution() of the data, which is the smallest non-zero distance between adjacent values of a variable. When x and y are discrete variables, their resolutions are both equal to 1, and height = 0.4 and width = 0.4 since the jitter moves points in both positive and negative directions.\nThe default values of height and width in geom_jitter() are non-zero, so unless both height and width are explicitly set set 0, there will be some jitter.\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +\rgeom_jitter()\r\r\rExercise 3.8.3\rCompare and contrast geom_jitter() with geom_count().\n\rThe geom geom_jitter() adds random variation to the locations points of the graph. In other words, it “jitters” the locations of points slightly. This method reduces overplotting since two points with the same location are unlikely to have the same random variation.\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +\rgeom_jitter()\rHowever, the reduction in overlapping comes at the cost of slightly changing the x and y values of the points.\nThe geom geom_count() sizes the points relative to the number of observations. Combinations of (x, y) values with more observations will be larger than those with fewer observations.\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +\rgeom_count()\rThe geom_count() geom does not change x and y coordinates of the points. However, if the points are close together and counts are large, the size of some points can itself create overplotting. For example, in the following example, a third variable mapped to color is added to the plot. In this case, geom_count() is less readable than geom_jitter() when adding a third variable as a color aesthetic.\nggplot(data = mpg, mapping = aes(x = cty, y = hwy, color = class)) +\rgeom_jitter()\rggplot(data = mpg, mapping = aes(x = cty, y = hwy, color = class)) +\rgeom_count()\rCombining geom_count() with jitter, which is specified with the position argument to geom_count() rather than its own geom, helps overplotting a little.\nggplot(data = mpg, mapping = aes(x = cty, y = hwy, color = class)) +\rgeom_count(position = \u0026quot;jitter\u0026quot;)\rBut as this example shows, unfortunately, there is no universal solution to overplotting. The costs and benefits of different approaches will depend on the structure of the data and the goal of the data scientist.\n\r\rExercise 3.8.4\rWhat’s the default position adjustment for geom_boxplot()? Create a visualization of the mpg dataset that demonstrates it.\n\rThe default position for geom_boxplot() is \"dodge2\", which is a shortcut for position_dodge2. This position adjustment does not change the vertical position of a geom but moves the geom horizontally to avoid overlapping other geoms. See the documentation for position_dodge2() for additional discussion on how it works.\nWhen we add colour = class to the box plot, the different levels of the drv variable are placed side by side, i.e., dodged.\nggplot(data = mpg, aes(x = drv, y = hwy, colour = class)) +\rgeom_boxplot()\rIf position_identity() is used the boxplots overlap.\nggplot(data = mpg, aes(x = drv, y = hwy, colour = class)) +\rgeom_boxplot(position = \u0026quot;identity\u0026quot;)\r\r\r\rCoordinate systems\rExercise 3.9.1\rTurn a stacked bar chart into a pie chart using coord_polar().\n\rA pie chart is a stacked bar chart with the addition of polar coordinates. Take this stacked bar chart with a single category.\nggplot(mpg, aes(x = factor(1), fill = drv)) +\rgeom_bar()\rNow add coord_polar(theta=\"y\") to create pie chart.\nggplot(mpg, aes(x = factor(1), fill = drv)) +\rgeom_bar(width = 1) +\rcoord_polar(theta = \u0026quot;y\u0026quot;)\rThe argument theta = \"y\" maps y to the angle of each section. If coord_polar() is specified without theta = \"y\", then the resulting plot is called a bulls-eye chart.\nggplot(mpg, aes(x = factor(1), fill = drv)) +\rgeom_bar(width = 1) +\rcoord_polar()\r\r\rExercise 3.9.2\rWhat does labs() do? Read the documentation.\n\rThe labs function adds axis titles, plot titles, and a caption to the plot.\nggplot(data = mpg, mapping = aes(x = class, y = hwy)) +\rgeom_boxplot() +\rcoord_flip() +\rlabs(y = \u0026quot;Highway MPG\u0026quot;,\rx = \u0026quot;Class\u0026quot;,\rtitle = \u0026quot;Highway MPG by car class\u0026quot;,\rsubtitle = \u0026quot;1999-2008\u0026quot;,\rcaption = \u0026quot;Source: http://fueleconomy.gov\u0026quot;)\rThe arguments to labs() are optional, so you can add as many or as few of these as are needed.\nggplot(data = mpg, mapping = aes(x = class, y = hwy)) +\rgeom_boxplot() +\rcoord_flip() +\rlabs(y = \u0026quot;Highway MPG\u0026quot;,\rx = \u0026quot;Year\u0026quot;,\rtitle = \u0026quot;Highway MPG by car class\u0026quot;)\rThe labs() function is not the only function that adds titles to plots. The xlab(), ylab(), and x- and y-scale functions can add axis titles. The ggtitle() function adds plot titles.\n\r\rExercise 3.9.3\rWhat’s the difference between coord_quickmap() and coord_map()?\n\rThe coord_map() function uses map projections to project the three-dimensional Earth onto a two-dimensional plane. By default, coord_map() uses the Mercator projection. This projection is applied to all the geoms in the plot. The coord_quickmap() function uses an approximate but faster map projection. This approximation ignores the curvature of Earth and adjusts the map for the latitude/longitude ratio. The coord_quickmap() project is faster than coord_map() both because the projection is computationally easier, and unlike coord_map(), the coordinates of the individual geoms do not need to be transformed.\nSee the coord_map() documentation for more information on these functions and some examples.\n\r\rExercise 3.9.4\rWhat does the plot below tell you about the relationship between city and highway mpg? Why is coord_fixed() important? What does geom_abline() do?\n\rThe function coord_fixed() ensures that the line produced by geom_abline() is at a 45-degree angle. A 45-degree line makes it easy to compare the highway and city mileage to the case in which city and highway MPG were equal.\np \u0026lt;- ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +\rgeom_point() +\rgeom_abline()\rp + coord_fixed()\rIf we didn’t include coord_fixed(), then the line would no longer have an angle of 45 degrees.\np\rOn average, humans are best able to perceive differences in angles relative to 45 degrees. See @Cleveland1993, @Cleveland1994,@Cleveland1993a, @ClevelandMcGillMcGill1988, @HeerAgrawala2006 for discussion on how the aspect ratio of a plot affects perception of the values it encodes, evidence that 45-degrees is generally the optimal aspect ratio, and methods to calculate the optimal aspect ratio of a plot. The function ggthemes::bank_slopes() will calculate the optimal aspect ratio to bank slopes to 45-degrees.\n\r\r\r\r\rInternally, this is what the factor class, which is covered in Chapter 15, does.↩︎\n\r\r\r","date":1609113600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609190558,"objectID":"01747ced7f1f2d91edf1c367da81002e","permalink":"https://ianadamsresearch.com/courses/pubpl-6002/chapter-3/","publishdate":"2020-12-28T00:00:00Z","relpermalink":"/courses/pubpl-6002/chapter-3/","section":"courses","summary":"One of the wonderful features of the R statistical universe is the number of free, high-quality instructional materials available. Throughout the remainder of the course, we will be learning and working from the R for Data Science book (you’ll see this book shorthanded as R4DS in this course and all over the web), by two giants of the R space: Hadley Wickham and Garrett Grolemund .","tags":["6002"],"title":"Chapter 3","type":"docs"},{"authors":null,"categories":[],"content":"If you are still not sure how to get started with R, Rstudio, and tidyverse, see if the following information helps. It was written by Dr. Jennifer (Jenny) Bryan for her STAT 545 class, and in addition to the following start-up information, you may want to bookmark her class page as a reference throughout the rest of the semester!\nR and RStudio   Install R, a free software environment for statistical computing and graphics from CRAN, the Comprehensive R Archive Network. I highly recommend you install a precompiled binary distribution for your operating system \u0026ndash; use the links up at the top of the CRAN page linked above!\n  Install RStudio\u0026rsquo;s IDE (stands for integrated development environment), a powerful user interface for R. Get the Open Source Edition of RStudio Desktop.\n I highly recommend you run the Preview version. I find these quite stable and you\u0026rsquo;ll get the cool new features! Update to new Preview versions often. Of course, there are also official releases available here. RStudio comes with a text editor, so there is no immediate need to install a separate stand-alone editor. RStudio can interface with Git(Hub). However, you must do all the Git(Hub) set up [described elsewhere][https://happygitwithr.com/] before you can take advantage of this.    If you have a pre-existing installation of R and/or RStudio, we highly recommend that you reinstall both and get as current as possible. It can be considerably harder to run old software than new.\n  If you upgrade R, you will need to update any packages you have installed. The command below should get you started, though you may need to specify more arguments if, e.g., you have been using a non-default library for your packages.\nupdate.packages(ask = FALSE, checkBuilt = TRUE)\r Note: this will only look for updates on CRAN. So if you use a package that lives only on GitHub or if you want a development version from GitHub, you will need to update manually, e.g. via devtools::install_github().\n  Testing testing   Do whatever is appropriate for your OS to launch RStudio. You should get a window similar to the screenshot you see here, but yours will be more boring because you haven\u0026rsquo;t written any code or made any figures yet!\n  Put your cursor in the pane labelled Console, which is where you interact with the live R process. Create a simple object with code like x \u0026lt;- 2 * 4 (followed by enter or return). Then inspect the x object by typing x followed by enter or return. You should see the value 8 print to screen. If yes, you\u0026rsquo;ve succeeded in installing R and RStudio.\n  Add-on packages R is an extensible system and many people share useful code they have developed as a package via CRAN and GitHub. To install a package from CRAN, for example the dplyr package for data manipulation, here is one way to do it in the R console (there are others).\ninstall.packages(\u0026quot;dplyr\u0026quot;, dependencies = TRUE)\r By including dependencies = TRUE, we are being explicit and extra-careful to install any additional packages the target package, dplyr in the example above, needs to have around.\nYou could use the above method to install the following packages, all of which we will use:\n tidyr package webpage ggplot2 package webpage  Note that if you install the tidyverse, it includes many of the other packages above!\ninstall.packages('tidyverse', dependencies = TRUE)\r Further resources The above will get your basic setup ready but here are some links if you are interested in reading a bit further.\n How to Use RStudio RStudio\u0026rsquo;s leads for learning R R FAQ R Installation and Administration More about add-on packages in the R Installation and Administration Manual  ","date":1609113600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609187857,"objectID":"1ccc9ec4894ff49f7e7e5ba274b83124","permalink":"https://ianadamsresearch.com/courses/pubpl-6002/intro-and-setup-materials/","publishdate":"2020-12-28T00:00:00Z","relpermalink":"/courses/pubpl-6002/intro-and-setup-materials/","section":"courses","summary":"If you are still not sure how to get started with R, Rstudio, and tidyverse, see if the following information helps. It was written by Dr. Jennifer (Jenny) Bryan for her STAT 545 class, and in addition to the following start-up information, you may want to bookmark her class page as a reference throughout the rest of the semester!","tags":[],"title":"Intro and Setup Materials","type":"docs"},{"authors":null,"categories":["SPSS"],"content":"\r\r\rSPSS Cheat Sheet\rThis contains some of the most common SPSS procedures for basic data analysis.\nData Cleaning\rMissing Data Counts\rAnalyse \u0026gt;\u0026gt; Descriptive Statistics \u0026gt;\u0026gt; Frequencies\rSelect the variable(s)\rClick Continue and then OK\r\rMissing data counts will be at the top of the resulting output.\n\rEdit Variable Name\rTransform \u0026gt;\u0026gt; Recode into Same Variables...\rSelect the variable to transform and move it into the right column.\rClick Old and New Values...\rUnder Old Value, enter either a specific value you would like to replace or a set of values you would\rlike to replace.\rUnder New Value, enter what the replacement value should be.\rClick Add under New Value.\rClick Continue and then OK.\r\r\rCreate a Variable\rTransform \u0026gt;\u0026gt; Compute Variable...\rClick Type and Label... to set the variable type, then click Continue.\rEnter the value for the variable. If it is a string, include the value in quotes.\rOr just enter a formula for the variable based on the existing variables.\rClick“OK.\r\r\rDelete a Variable\rRight-click on the column header\rClick Clear.\r\rNOTE: This does not produce a syntax in the Output window. The syntax for deleting a variable is here, in case you\rare saving your syntax:\n\rDELETE VARIABLES [list of variables, separated by spaces].\r\r\rDrop observations based on some condition (Keep observations meeting the opposite)\rData \u0026gt;\u0026gt; Select Cases... \u0026gt;\u0026gt; Select If condition is satisfied \u0026gt;\u0026gt; If...\rEnter the condition based on which observations you would like to keep, then click Continue.\rSelect Delete unselected cases.\rClick OK.\r\rYou can specify multiple conditions at the same time by separating them with AND or OR.\n\rMerging datasets\rData \u0026gt;\u0026gt; Merge Files \u0026gt;\u0026gt; Add Variables...\rNote that the datasets you are merging must already be saved as SPSS (.sav) format files. In addition,\rthe variables you are matching on must have the same name across datasets.\rSelect An external SPSS statistics data file, browse for your file, and select it.\rSelect Match cases on key variables, click on the matching variable, and add it to Key Variables.\rClick OK.\r\r\rAppending datasets\rData \u0026gt;\u0026gt; Merge Files \u0026gt;\u0026gt; Add Cases...\rNote that the datasets you are merging must already be saved as SPSS (.sav) format files. In addition,\rthe variables you are matching on must have the same name across datasets.\rSelect An external SPSS statistics data file, browse for your file, and select it.\rAll variables already in both datasets will appear in Variables in New Active Dataset, and variables\rnot in both datasets will be in Unpaired Variables. Move all unpaired variables you want into the\rright column.\rClick OK.\r\r\r\rDescriptive Statistics\rCentral tendency: mean, median, and mode (for continuous variable)\rAnalyze \u0026gt;\u0026gt; Descriptive Statistics \u0026gt;\u0026gt; Frequencies\rSelect the continuous variable(s)\rUncheck Display frequency tables\rClick Statistics... and check the desired central tendency measures\rClick Continue and then OK\r\r\rCentral tendency: mode and frequency table (for categorical variable)\rAnalyze \u0026gt;\u0026gt; Descriptive Statistics \u0026gt;\u0026gt; Frequencies\rSelect the categorical variable(s)\rCheck Display frequency tables\rClick Format and select Descending counts\rClick Continue and then OK\r\rThe top item in the frequency table is the mode. Note that if multiple categorical variables are selected, a\rseparate frequency table will be created for each variable.\n\rVariability: Standard deviation, variance, and range (for continuous variable)\rAnalyze \u0026gt;\u0026gt; Descriptive Statistics \u0026gt;\u0026gt; Descriptives\rSelect the continuous variable(s)\rClick Options and select the desired measures of spread\rClick Continue and then OK\r\r\r\rCommon Analyses\rCorrelations\rPearson correlation:\rAnalyze \u0026gt;\u0026gt; Correlate \u0026gt;\u0026gt; Bivariate\rSelect Pearson under Correlation Coefficients box, select the variables, click OK\r\r\rSpearman correlation coefficient:\rAnalyze \u0026gt;\u0026gt; Correlate \u0026gt;\u0026gt; Bivariate\rSelect Spearman under Correlation Coefficient box, select the variables, click OK\r\r\r\rLinear Regression\rSimple Linear Regression:\rAnalyze \u0026gt;\u0026gt; Regression \u0026gt;\u0026gt; Linear\rEnter IV and DV, Click OK\r\r\rMultiple Linear Regression:\rAnalyze \u0026gt;\u0026gt; Regression \u0026gt;\u0026gt; Linear\rSelect IVs and DV, Click OK\r\r\r\rT-Test\rSingle-Sample T-test:\rAnalyze \u0026gt;\u0026gt; Compare Means \u0026gt;\u0026gt; One-Sample T-test\rEnter variables, click OK\r\r\rIndependent Samples T-test:\rAnalyze \u0026gt;\u0026gt; Compare Means \u0026gt;\u0026gt; Independent Samples T test\rEnter DV (Test Variable) and IV (Grouping variable), Define Groups, and enter the values of the two levels of the IV, click continue, click OK\r\r\rPaired Samples T-Test:\rAnalyze \u0026gt;\u0026gt; Compare Means \u0026gt;\u0026gt; Paired Samples T Test\rClick on two paired variables to move to Current Selections area, then click right arrowto move to Paired Variables Section, Click OK\r\r\r\rANOVAs\rOneway ANOVA:\rAnalyze \u0026gt;\u0026gt; Compare Means \u0026gt;\u0026gt; One-Way ANOVA\rEnter IV in Factor box, Enter DV to Dependent List box, click Options \u0026gt;\u0026gt; Descriptive to get means in output Click continue, click OK\r\r\rFactorial Anova (2x2, 2x2x3, etc.):\rAnalyze \u0026gt;\u0026gt; General Linear Model \u0026gt;\u0026gt; Univariate\rSelect DV for Dependent Variable blank and IVs for the Fixed Factors box, click OK\r\r\rRepeated Measures ANOVA:\rAnalyze \u0026gt;\u0026gt; General Linear Model \u0026gt;\u0026gt; Repeated Measures\rEnter factors and number of levels \u0026gt;\u0026gt; click Add \u0026gt;\u0026gt; once all factors are entered click define\rDefine variables using the arrows, click OK\r\r\rMixed-Design ANOVA:\rFollow same steps as repeated measures\rAdd between-subjects factor to “Between-Subjects Factor” box, click OK\r\r\r\r\r\r","date":1609113600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610928000,"objectID":"69cd2136ec967bdeec0643a87aba0ecb","permalink":"https://ianadamsresearch.com/courses/pubpl-6002/spss-cheat/","publishdate":"2020-12-28T00:00:00Z","relpermalink":"/courses/pubpl-6002/spss-cheat/","section":"courses","summary":"SPSS Cheat Sheet\rThis contains some of the most common SPSS procedures for basic data analysis.\nData Cleaning\rMissing Data Counts\rAnalyse \u0026gt;\u0026gt; Descriptive Statistics \u0026gt;\u0026gt; Frequencies\rSelect the variable(s)\rClick Continue and then OK\r\rMissing data counts will be at the top of the resulting output.","tags":["6002"],"title":"SPSS Cheat Sheet","type":"docs"},{"authors":[],"categories":["R","website","Research"],"content":"\r\rThe Department of Political Science at the University of Utah periodically holds political research colloquiums (PRC) for graduate students and faculty to present early and current research findings. This is my presentation at the PRC held Oct. 30, 2020. I describe early findings from one of my dissertation chapters. This research uses a randomized vignette experiment to investigate the effects of various body-worn camera activation and review/audit policies on how officers perceive the fairness of monitoring.\nThis is an embedded Microsoft Office presentation, powered by Office.\r\r","date":1604016000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604080794,"objectID":"cb160c6f5f8b19a0552ab88a26a46ae1","permalink":"https://ianadamsresearch.com/post/powerpoint-embed-testing/","publishdate":"2020-10-30T00:00:00Z","relpermalink":"/post/powerpoint-embed-testing/","section":"post","summary":"Presented at the Political Research Colloquium, University of Utah, Oct. 30, 2020","tags":["BWC"],"title":"The Effect of Body-Worn Camera Activation and Auditing Policies on Perceptions of Monitoring Fairness","type":"post"},{"authors":[],"categories":["Research"],"content":"\r\rPresented as part of the “Uncovering the Hidden Curriculum” workshop, hosted by the Department of Political Science at the University of Utah, October 23, 2020. Thank you to Devon Cantwell for helping make this happen!\nThis is an embedded Microsoft Office presentation, powered by Office.\r\r","date":1604016000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604083070,"objectID":"780b8b28bde070fb51dfd1723a3cb028","permalink":"https://ianadamsresearch.com/post/2020-10-23-zotero-workflow-for-researchers/zotero-workflow-for-graduate-students-and-researchers/","publishdate":"2020-10-30T00:00:00Z","relpermalink":"/post/2020-10-23-zotero-workflow-for-researchers/zotero-workflow-for-graduate-students-and-researchers/","section":"post","summary":"Presented as part of the “Uncovering the Hidden Curriculum” workshop, hosted by the Department of Political Science at the University of Utah, October 23, 2020. Thank you to Devon Cantwell for helping make this happen!","tags":["Zotero"],"title":"Zotero Workflow for Graduate Students and Researchers","type":"post"},{"authors":[],"categories":["police"],"content":"\rLast week I saw a Vox article making the Twitter rounds. Written by Zach Beauchamp, the article purports to have the inside track on an “ideology of American policing,” one that “justifies racist violence” in the minds of police officers. Startling stuff.\nBefore you hear from me, go check out the article. First though, ask yourself: How many active police officers do you think the author spoke with, based on the headline?\nI initially looked forward to reading the article. I am a former officer, and now a scholar who devotes a great deal of time thinking about and researching police attitudes and beliefs. From what officers’ own words can tell us about their propensity to “de-police” to how officers perceive the “intensity” of monitoring from body-worn cameras, I approach officers as human beings with something to say, and with something worth hearing.\nMy initial hopes for the article were not fulfilled. The article begins with promises of a “deep dive” but ends up misjudging the depth of the pool. It soon became clear where the author went wrong: He forgot to listen to cops.\rWriting an article that promises insight into a group of human beings, but then not interviewing one person who is from that group, is almost guaranteed to result in a failure to communicate effectively about that group.\nBut don’t take my word for it, here’s how Beauchamp reports he conducted his research:\nDinkheller \u0026amp; Administrative Heat\rOne of the critical linchpins of Beauchamp’s case is that this “ideology of American policing” starts in the academy with a showing of the “Dinkheller” video. Being familiar with that video, and somewhat knowledgeable about how it can be framed for police recruits, Beauchamp’s reductionist vision of it was quite startling.\nI first encountered the Dinkheller video at the academy as a young recruit. The purpose as I remember it was in the context of how administrative “heat” can cause officers to hesitate when they absolutely should not. No one can perfectly attest to what caused Dinkheller to hesitate taking action even in the face of the most violent and predictable outcomes. Five separate law enforcement officers confirmed one version of events, centered on the idea that Dinkheller was distressed about his trouble with the Sheriff over a previous demeanor complaint on a traffic stop. Here is some support for that idea, from CNN’s coverage of the incident.\n\r“In the late ’90s, some rank-and-file deputies believed [Sheriff Webb] wouldn’t stand by them in a crisis. This was partly due to an incident that involved Kyle Dinkheller just a few months before he died.\rKyle was on his way to a crash scene with his lights flashing when he came upon a vehicle whose driver wouldn’t move aside. Because state law requires civilians to yield in those situations, Kyle spoke harshly to the man, who turned out to be a good friend of the sheriff. The man complained to Webb, who told Kyle to write a letter of apology. Kyle initially refused to write the letter. He told fellow deputies that the sheriff ordered him to write it or face severe consequences.\rKyle wrote the letter. His colleagues say the sheriff made him re-write it and deliver it by hand. The incident weighed on him. When he saw Deputy Skip Lowery at the gas pumps, Kyle would say things like,”Y’all write any letters of apology today?” Two days before he died, in a friendly visit to the home of Special Agent Alan Watson of the Georgia Bureau of Investigation, Kyle mentioned the apology. His colleagues are convinced it was somewhere in the back of his mind during his final traffic stop. He knew he was being watched, and he’d learned that being right was not always enough.”\n\r\rExperts and Practitioners\rWhy isn’t this theme reflected in the Vox article? What’s missing from Beauchamp’s reporting? Well…officers. Though the article describes “a deep dive into the motivations and beliefs of police,” a less flashy but more honest appraisal of the approach would be: a collection of academic opinions without the mess of dealing with the views of practitioners.\nTo his credit, Beauchamp calls on the words of highly respected academics, including Eugene Paoline, who, along with William Terrill, authored “Listen to Me! Police Officers Views of Appropriate Use of Force,” a compelling argument for listening to the voices of actual, working law enforcement officers. The article also includes thoughts from Peter Moskos, a former officer, current professor at Jon Jay College of Criminal Justice, and author of the exceptional Cop in the Hood (really, you should read it).\nSo what’s the complaint then? Didn’t Beauchamp do what he set out to do? He interviewed experts and former police. Doesn’t that get you “inside the ideology of American policing”? No, it doesn’t. Despite the excellent academic contributions, Beauchamp misses the mark fairly widely by not listening to officers who are actively serving their communities. It is clear what Beauchamp missed: the gap between expert opinion, and officers’ perceptions (if you bother to ask them directly). While I’m writing in response to a journalist, this is also a reminder to myself, and others who study policing - be wary of letting our own voices carry louder than those we report on and research. All the interesting variation is in the voices and experiences of those we study; our overlays of theory and interpretation strip out that variation.\n\rBeauchamp’s Claim\rBeauchamp spends ample time unpacking expert and academic views on the Dinkheller video. Like Professor Moskos correctly says, this is a video that most every officer knows, but not many outside policing or the study of it do. Warning – it’s not a pleasant video (contains disturbing audio and video of an officer being murdered): https://www.youtube.com/watch?v=mssNOhv1UMc. If the video is too much (and it is for me), you can read about the incident here.\nBeauchamp makes this claim about the purpose of showing the Dinkheller video to police recruits:\nFrom Beauchamp’s reporting, however, all we know is his interpretation, based on a handful of experts and former officers, but not a single, actual, bonafide officer. Is that what they think the purpose of the video is? The easiest way to find out is to ask. So I did.\n\rOfficers’ Views\rI called six current officers whom I know personally. They range from very close friends who would raise my children if needed, to professional contacts in other parts of the country. I promised them I wouldn’t use names, but just their rank, approximate years on the job, and current assignment. Here’s what they said about the Dinkheller video. I asked them if they remembered the video, and if so, what was the purpose of the video. This is obviously not a random sample.\nThese are their immediate, off-the-cuff responses that I typed directly into the document as we spoke:\nOfficer, ~20 years, narcotics officer: That video was about doing what you have to do. It was a constable, right? He was jammed up by his chief on something else, like a use-of-force. Then when it came time, he hesitated. The purpose I guess was to teach us not to let an administrator get us killed.\n\rOfficer, \u0026lt; 3 years, patrol officer: I actually don’t remember that video.\n\rOfficer, ~15 years, traffic enforcement: That was about not letting a dude do what he did. You don’t let a guy back into the car after all that. It ain’t gonna end well, ever.\n\rLieutenant, ~15 years, major crimes: Oh yeah, I remember it. It was about how you can’t let someone disobey your lawful commands over and over. If you do, there’s always going to be a bad outcome. Unfortunately for that officer, the outcome was death. Young officers need to know what to look out for, and take action when it’s necessary.\n\rMajor, ~25 years, training academy director: I don’t know if we still use that video, I’d have to check with my trainers to see. These days we introduce the recruits to those types of videos much later. That video is hard for even you and I to watch. When I did use that video in the past, it was for the flags. What flags can we point to where the officer could have made a different decision? That man struck the officer, and then he allowed him to get back into the truck and retrieve a rifle. That should not have happened. I want my recruits to have the confidence to stop him from getting back into the vehicle.\n\rOfficer, ~20 years, patrol and use-of-force trainer: Man, the Dinkheller video was horrific. The indicators of non-compliance are all there. You’ll see odd behaviors in the extreme. Most traffic stops are extremely predictable. When we see things outside of the norm, that’s when we need to pay attention. It would be foolish not to show officers what the out-of-the-ordinary looks like. The Dinkheller video is not an example of the normal traffic stop, it’s the video that shows what’s not normal, what an officer needs to pay attention to. When you see the indicators, the hands in the pockets, the strange behavior, the noncompliance, you need to immediately hop on that. When you disregard those, when you hesitate, when you ignore the physiological effects like the voice stress, the repetitive commands – that tells you Dinkheller has lost control. As a trainer I need to put people into stressful scenarios. For me as an officer and a trainer, I need to recognize “this is not good, this is waaaay outside the bounds of good, I need to do something.” Teaching recruits how to deal with fear is imperative. Clearly recognizing the warning signs is part of that. That only comes from training and experience. There’s no way around that.\n\r\r\rThemes from Police\rSo, what do police actually believe about the Dinkheller video? We have four themes:\nNot exposed to the video, or video not in use. This is a particular problem for Beauchamp’s thesis that the Dinkheller video is forming an ideology that “justifies racist violence.”\n\rThe purpose of the video is to teach recruits the “flags” that might mean their lives are in danger. Refusing to remove his hands from his pockets [0:22], dancing around yelling “Fuck you Goddamnit, here I am, shoot my fucking ass!” [0:29], shouting “Fucking do it man!” [0:46], physically attacking the officer [1:00], getting back into the vehicle [1:10], retrieving an M1 rifle from the truck [1:20], aiming the rifle at the officer [1:20 – 1:55], and finally firing at the officer beginning at [1:56].\n\rThe purpose of the video was to reinforce the danger of not enforcing an officer’s legal commands. Repeated indifference to an officer’s commands is a danger signal that officers should pay attention to. Act when your eyes are telling you there is a problem.\n\rThe purpose of the video was to instill in officers that they should not let previous administrative burdens become a death sentence. It is fascinating that multiple officers, from different parts of the country, remember this theme from their training days. The gap in trust between officers and administrators runs deep, and this is some evidence that even recruits are exposed to the distrust from a very early stage in their careers. In some ways this would support Beauchamp’s conclusions, in that there are important exposures in academy training that shape officers’ views of policing. However, amending Beauchamp’s thesis in this post-hoc manner is speculative at best, because his reporting does not reflect this important bit of information.\n\r\rWhat’s striking about these themes taken together is the notable absence of the animating thesis for Beauchamp’s article, which is: “The purpose of the Dinkheller video…is to teach officers that any situation could escalate to violence. Cop killers lurk around every corner.” Dramatic language, but not language supported by officers’ words, either in the article (because they weren’t asked), or in my quick sample. Quite contrary to the article’s thesis that the Dinkheller video teaches officers that unexpected danger is around every corner, officers see the video as teaching them expected dangers are right in front of you. That is some valuable insight, and was entirely available to Beauchamp, had he bothered to ask.\nThe other obvious deficiency that these six officers expose in Beauchamp’s reporting is that there is no singular ideology. This should not be surprising. With at least 800,000 officers in the US, and 18,000 law enforcement agencies, ascribing a singular ideology is tenuous at best. Doing so contributes to visions of law enforcement officers as less than fully human, as robotic and featureless cogs in a unitary system. Anyone who has researched officers knows that’s an unfair conclusion. Even worse, in the context of reporting, it is an inaccurate one. One way to avoid that kind of caricature is to spend time - a lot of time - with the people on whom you report.\n\rDoing Better\rFormer officers and scholars, myself included, are valuable resources for insights about how officers view the world. The trap that Beauchamp fell into is assuming those insights are the same as officers’ own. This is a pernicious problem in journalism about police. Check your favorite paper and count how many times some version of “Police say…” is used. How many were followed with the views of 1) line officers, who were 2) not representing the department as a public information officer or spokesperson?\nDoing better means listening to more people. Not experts on people, but the people themselves. Doing better allows you to avoid misjudging the depth of an issue, and revealing variation you could not otherwise have accessed. I only made a handful of calls, and Beauchamp only spoke with “more than a dozen” sources, but the variety of interesting themes is already too complicated and interesting to reduce down to an “ideology of American policing” as claimed in the Vox piece. Variation is good, rough reductionism helps no one.\nJournalists and journalism generally, and Vox specifically, can do better. I enjoy the policy dives that Vox provides in all kinds of areas. Like many, I rely on their reportage to get an accurate picture of worlds I don’t otherwise have access to. How accurate would an article be titled “What Teachers Really Believe,” if we found out not a single teacher was used? I suspect some teacher’s might object, especially if their actual views were not represented, and they would be right to do so.\n\r","date":1594684800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594759436,"objectID":"d6e7e8a9259318df9227e0795cd0ca95","permalink":"https://ianadamsresearch.com/post/what-police-believe/what-police-believe/","publishdate":"2020-07-14T00:00:00Z","relpermalink":"/post/what-police-believe/what-police-believe/","section":"post","summary":"The article begins with promises of a “deep dive” but ends up misjudging the depth of the pool. It soon became clear where the author went wrong: He forgot to listen to cops.","tags":["police","vox","Dinkheller"],"title":"What Police Believe","type":"post"},{"authors":null,"categories":["R","research"],"content":"\r\rLibraries\rlibrary(tidytext)\rlibrary(ggthemes)\rlibrary(tidyverse)\rlibrary(ggplot2)\rlibrary(dplyr)\rlibrary(scales)\r\rLoad Previous STM Objects\rI have previously run stm models for topics ranging from 3 to 25. Based on the fit indices, a six-topic model was selected. I am not showing that analysis here, but instead loading the saved stm objects (the model and the ‘out’ object).\n\rTidy the stm model\rI am using the tidytext package from Julia Silge, which she demonstrates in a blog post here.\ntd_beta \u0026lt;- tidy(modelfit6)\rtd_beta\r## # A tibble: 1,416 x 3\r## topic term beta\r## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 abil 9.13e-61\r## 2 2 abil 6.07e-55\r## 3 3 abil 1.51e- 2\r## 4 4 abil 2.98e-54\r## 5 5 abil 3.66e-63\r## 6 6 abil 2.39e-58\r## 7 1 account 1.57e-91\r## 8 2 account 1.37e-25\r## 9 3 account 4.28e-66\r## 10 4 account 6.47e-63\r## # ... with 1,406 more rows\rtd_gamma \u0026lt;- tidy(modelfit6, matrix = \u0026quot;gamma\u0026quot;,\rdocument_names = rownames(out$meta))\rtd_gamma\r## # A tibble: 1,266 x 3\r## document topic gamma\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 1 0.00343\r## 2 2 1 0.00462\r## 3 3 1 0.0861 ## 4 4 1 0.0467 ## 5 5 1 0.0106 ## 6 6 1 0.00306\r## 7 7 1 0.00419\r## 8 8 1 0.0511 ## 9 9 1 0.00356\r## 10 10 1 0.144 ## # ... with 1,256 more rows\r\rPlot Topic Relevance\rtop_terms \u0026lt;- td_beta %\u0026gt;%\rarrange(beta) %\u0026gt;%\rgroup_by(topic) %\u0026gt;%\rtop_n(7, beta) %\u0026gt;%\rarrange(-beta) %\u0026gt;%\rselect(topic, term) %\u0026gt;%\rsummarise(terms = list(term)) %\u0026gt;%\rmutate(terms = map(terms, paste, collapse = \u0026quot;, \u0026quot;)) %\u0026gt;% unnest()\rgamma_terms \u0026lt;- td_gamma %\u0026gt;%\rgroup_by(topic) %\u0026gt;%\rsummarise(gamma = mean(gamma)) %\u0026gt;%\rarrange(desc(gamma)) %\u0026gt;%\rleft_join(top_terms, by = \u0026quot;topic\u0026quot;) %\u0026gt;%\rmutate(topic = paste0(\u0026quot;Topic \u0026quot;, topic),\rtopic = reorder(topic, gamma))\rgamma_terms %\u0026gt;%\rtop_n(6, gamma) %\u0026gt;%\rggplot(aes(topic, gamma, label = terms, fill = topic)) +\rgeom_col(show.legend = FALSE) +\rgeom_text(hjust = 0.85, nudge_y = 0.0005, size = 3) +\rcoord_flip() +\rtheme_hc() +\rtheme(plot.title = element_text(size = 14)) +\rlabs(x = NULL, y = expression(gamma),\rtitle = \u0026quot;Top Six Topics by Prevalence in the Officer Responses\u0026quot;,\rsubtitle = \u0026quot;With the top words that contribute to each topic\u0026quot;)\r\rTable of Topic Proportions with Top Terms\rrequire(knitr)\rgamma_terms %\u0026gt;%\rselect(topic, gamma, terms) %\u0026gt;%\rkable(digits = 3, col.names = c(\u0026quot;Topic\u0026quot;, \u0026quot;Expected topic proportion\u0026quot;, \u0026quot;Top 6 terms\u0026quot;))\r\r\rTopic\rExpected topic proportion\rTop 6 terms\r\r\r\rTopic 6\r0.240\rcomplaint, protect, public, fals, make, tool, time\r\rTopic 5\r0.214\raffect, way, chang, made, turn, situat, also\r\rTopic 3\r0.192\rcamera, job, help, bodi, wear, captur, action\r\rTopic 4\r0.149\roffic, peopl, work, worri, one, say, good\r\rTopic 2\r0.117\ruse, video, will, call, without, review, someth\r\rTopic 1\r0.088\rknow, person, feel, wear, like, supervisor, think\r\r\r\r\r","date":1592870400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592935626,"objectID":"002d6e8a364d75f9e41d0c0a9523ef2f","permalink":"https://ianadamsresearch.com/post/tidy-stm/tidying-stm-with-tidytext/","publishdate":"2020-06-23T00:00:00Z","relpermalink":"/post/tidy-stm/tidying-stm-with-tidytext/","section":"post","summary":"Libraries\rlibrary(tidytext)\rlibrary(ggthemes)\rlibrary(tidyverse)\rlibrary(ggplot2)\rlibrary(dplyr)\rlibrary(scales)\r\rLoad Previous STM Objects\rI have previously run stm models for topics ranging from 3 to 25. Based on the fit indices, a six-topic model was selected.","tags":["police","R Markdown","STM","survey"],"title":"Tidying STM with tidytext","type":"post"},{"authors":[],"categories":["research"],"content":"\r\rIn survey research, we sometimes want to present varying conditions in a short descriptive text, often called a vignette, and measure the effects of those conditions on an outcome of interest.\nFor example, say we have been hired by DinoCreations Inc. to gauge the public’s willingness to spend tax dollars on our brand new Island Adventure theme park, complete with living dinosaurs created from ancient DNA held within amber! We want to know if varying the dinosaurs type and size in a vignette results in statistically significant differences in potential patrons willingness to visit. We have two types of dinosaurs, and they come in three sizes. Will our potential guests be more likely to purchase a vacation package dependent on the type and size of dinosaurs we advertise with?\nThere are multiple ways we could do this. For example, we could just type out each of the six vignette (2x3) and have those randomly display to respondents. However, some vignette designs have a lot more conditions to evaluate. Here, I’ll demonstrate a simple 2x3 vignette factorial design, but the method can easily be modified for larger designs. The survey platform Qualtrics makes this easy, using embedded data and the randomizer in survey flows.\nPlaceholder Descriptive Text\rFollowing months of research, we have determined that there are TWO types of dinosaurs (T-rex and Stegosaurus), and each dinosaur comes in three sizes (big, humongous, and ginormous). Lucky for us, this is a perfect opportunity to put a 2x3 factorial vignette design into play!\nFor now, let’s create a place holder question in the survey. Our vignette will be about dinosaurs, and we are interested in willingness to visit our new Dinocreations.\n\rBuilding the Randomizer\rNext we need to build a randomizer inside the survey flow. Once inside the survey flow, click the “add below” text inside the survey block, then click the “randomizer” tag.\nNow we want to add the type of information this randomizer will handle, in this case, “type” of dinosaur. Click “Add a new element here,” using the “embedded data” type, and assign “type” as the element name. Then assign a type of dinosaur, I chose “terrifying Tyrannasaurus Rex.” Then repeat this step, and assign a different type of dinosaur, in this case an “adorable Stegosaurus.” Finally, make sure you tell the randomizer to randomly present only 1 of these options. Click the “evenly present options” if you want there to be a randomized and equal chance for your survey respondents to see each choice (so here, a 1:2 chance, but for size, a 1:3 chance).\nNow we want to build another randomizer, this time to vary the size of the dinosaur in our vignette. Using the same steps as above, we will add a size randomizer that uses three elements of big, humongous, and ginormous, and then make sure only one of these options is presented.\nFor the final step in the survey flow, we want to make sure both randomizers are ABOVE the vignette question. This assures that the appropriate information is placed into our survey vignette question.\n\rBack to the vignette, and inserting piped text\rSave and close the survey flow interface. Now we need to use the “piped text” interface to get our vignette put together. We have decided to concentrate on customers who will be leaving on their honeymoon soon. The plain text in this box is text that EVERY survey respondent will see, while the SIZE and TYPE placeholders will need to be modified to import the embedded data we defined in the survey flow steps above.\nTo use the piped text funcionality, highlight SIZE, and then click the “piped text” tab directly above:\nThen, click the “embedded data” option, and type in “size” (without the quotation marks) and hit enter. Repeat this step for the TYPE placeholder, with the “type” embedded data. After you delete the placeholders, you should have something that looks like:\nNow, let’s see if our survey is working. Use the “preview” button at the top of the page. Check it a few times to make sure the randomization is working. Here, in two consecutive previews, I can already see the randomization is working!\nand\n\rConclusion\rAnalyzing the results is outside of the scope of this post, but there are plenty of guides for that. Now go forth and randomize your vignettes!\n\r","date":1590192000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590272368,"objectID":"b5b6312a88c7cbe8d73bffbf81a949f9","permalink":"https://ianadamsresearch.com/post/qualtrics/randomizing-vignettes-in-survey-research-with-qualtrics/","publishdate":"2020-05-23T00:00:00Z","relpermalink":"/post/qualtrics/randomizing-vignettes-in-survey-research-with-qualtrics/","section":"post","summary":"In survey research, we sometimes want to present varying conditions in a short descriptive text, often called a vignette, and measure the effects of those conditions on an outcome of interest.","tags":["survey","vignette","qualtrics"],"title":"Randomizing Vignette Factorial Designs in Survey Research with Qualtrics","type":"post"},{"authors":[],"categories":["R","website"],"content":"\r\rThis report was generated on 2020-12-30, as a demo of textclean from https://github.com/trinker/textclean#check-text\nThis is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code.\nFast example with created examples\rx \u0026lt;- c(\u0026quot;i like\u0026quot;, \u0026quot;\u0026lt;p\u0026gt;i want. \u0026lt;/p\u0026gt;. thet them ther .\u0026quot;, \u0026quot;I am ! that|\u0026quot;, \u0026quot;\u0026quot;, NA, \u0026quot;\u0026amp;quot;they\u0026amp;quot; they,were there\u0026quot;, \u0026quot;.\u0026quot;, \u0026quot; \u0026quot;, \u0026quot;?\u0026quot;, \u0026quot;3;\u0026quot;, \u0026quot;I like goud eggs!\u0026quot;, \u0026quot;bi\\xdfchen Z\\xfcrcher\u0026quot;, \u0026quot;i 4like...\u0026quot;, \u0026quot;\\\\tgreat\u0026quot;, \u0026quot;She said \\\u0026quot;yes\\\u0026quot;\u0026quot;)\rEncoding(x) \u0026lt;- \u0026quot;latin1\u0026quot;\rx \u0026lt;- as.factor(x)\rcheck_text(x)\r## ## =============\r## NON CHARACTER\r## =============\r## ## The text variable is not a character column (likely `factor`):\r## ## ## *Suggestion: Consider using `as.character` or `stringsAsFactors = FALSE` when reading in\r## Also, consider rerunning `check_text` after fixing\r## ## ## =====\r## DIGIT\r## =====\r## ## The following observations contain digits/numbers:\r## ## 10, 13\r## ## This issue affected the following text:\r## ## 10: 3;\r## 13: i 4like...\r## ## *Suggestion: Consider using `replace_number`\r## ## ## ========\r## EMOTICON\r## ========\r## ## The following observations contain emoticons:\r## ## 6\r## ## This issue affected the following text:\r## ## 6: \u0026amp;quot;they\u0026amp;quot; they,were there\r## ## *Suggestion: Consider using `replace_emoticons`\r## ## ## =====\r## EMPTY\r## =====\r## ## The following observations contain empty text cells (all white space):\r## ## 1\r## ## This issue affected the following text:\r## ## 1: i like\r## ## *Suggestion: Consider running `drop_empty_row`\r## ## ## =======\r## ESCAPED\r## =======\r## ## The following observations contain escaped back spaced characters:\r## ## 14\r## ## This issue affected the following text:\r## ## 14: \\tgreat\r## ## *Suggestion: Consider using `replace_white`\r## ## ## ====\r## HTML\r## ====\r## ## The following observations contain HTML markup:\r## ## 2, 6\r## ## This issue affected the following text:\r## ## 2: \u0026lt;p\u0026gt;i want. \u0026lt;/p\u0026gt;. thet them ther .\r## 6: \u0026amp;quot;they\u0026amp;quot; they,were there\r## ## *Suggestion: Consider running `replace_html`\r## ## ## ==========\r## INCOMPLETE\r## ==========\r## ## The following observations contain incomplete sentences (e.g., uses ending punctuation like \u0026#39;...\u0026#39;):\r## ## 13\r## ## This issue affected the following text:\r## ## 13: i 4like...\r## ## *Suggestion: Consider using `replace_incomplete`\r## ## ## =============\r## MISSING VALUE\r## =============\r## ## The following observations contain missing values:\r## ## 5\r## ## *Suggestion: Consider running `drop_NA`\r## ## ## ========\r## NO ALPHA\r## ========\r## ## The following observations contain elements with no alphabetic (a-z) letters:\r## ## 4, 7, 8, 9, 10\r## ## This issue affected the following text:\r## ## 4: ## 7: .\r## 8: ## 9: ?\r## 10: 3;\r## ## *Suggestion: Consider cleaning the raw text or running `filter_row`\r## ## ## ==========\r## NO ENDMARK\r## ==========\r## ## The following observations contain elements with missing ending punctuation:\r## ## 1, 3, 4, 6, 8, 10, 12, 14, 15\r## ## This issue affected the following text:\r## ## 1: i like\r## 3: I am ! that|\r## 4: ## 6: \u0026amp;quot;they\u0026amp;quot; they,were there\r## 8: ## 10: 3;\r## 12: bißchen Zürcher\r## 14: \\tgreat\r## 15: She said \u0026quot;yes\u0026quot;\r## ## *Suggestion: Consider cleaning the raw text or running `add_missing_endmark`\r## ## ## ====================\r## NO SPACE AFTER COMMA\r## ====================\r## ## The following observations contain commas with no space afterwards:\r## ## 6\r## ## This issue affected the following text:\r## ## 6: \u0026amp;quot;they\u0026amp;quot; they,were there\r## ## *Suggestion: Consider running `add_comma_space`\r## ## ## =========\r## NON ASCII\r## =========\r## ## The following observations contain non-ASCII text:\r## ## 12\r## ## This issue affected the following text:\r## ## 12: bißchen Zürcher\r## ## *Suggestion: Consider running `replace_non_ascii`\r## ## ## ==================\r## NON SPLIT SENTENCE\r## ==================\r## ## The following observations contain unsplit sentences (more than one sentence per element):\r## ## 2, 3\r## ## This issue affected the following text:\r## ## 2: \u0026lt;p\u0026gt;i want. \u0026lt;/p\u0026gt;. thet them ther .\r## 3: I am ! that|\r## ## *Suggestion: Consider running `textshape::split_sentence`\rAnd if all is well the user should be greeted by a cow:\ry \u0026lt;- c(\u0026quot;A valid sentence.\u0026quot;, \u0026quot;yet another!\u0026quot;)\rcheck_text(y)\r## ## ------------- ## No problems found!\r## This text is virtuosic! ## ---------------- ## \\ ^__^ ## \\ (oo)\\ ________ ## (__)\\ )\\ /\\ ## ||------w|\r## || ||\r\r\rRow Filtering\rIt is useful to drop/remove empty rows or unwanted rows (for example the researcher dialogue from a transcript). The drop_empty_row \u0026amp; drop_row do empty row do just this. First I’ll demo the removal of empty rows.\n## create a data set wit empty rows\r(dat \u0026lt;- rbind.data.frame(DATA[, c(1, 4)], matrix(rep(\u0026quot; \u0026quot;, 4), ncol =2, dimnames=list(12:13, colnames(DATA)[c(1, 4)]))))\r## person state\r## 1 sam Computer is fun. Not too fun.\r## 2 greg No it\u0026#39;s not, it\u0026#39;s dumb.\r## 3 teacher What should we do?\r## 4 sam You liar, it stinks!\r## 5 greg I am telling the truth!\r## 6 sally How can we be certain?\r## 7 greg There is no way.\r## 8 sam I distrust you.\r## 9 sally What are you talking about?\r## 10 researcher Shall we move on? Good then.\r## 11 greg I\u0026#39;m hungry. Let\u0026#39;s eat. You already?\r## 12 ## 13\rdrop_empty_row(dat)\r## person state\r## 1 sam Computer is fun. Not too fun.\r## 2 greg No it\u0026#39;s not, it\u0026#39;s dumb.\r## 3 teacher What should we do?\r## 4 sam You liar, it stinks!\r## 5 greg I am telling the truth!\r## 6 sally How can we be certain?\r## 7 greg There is no way.\r## 8 sam I distrust you.\r## 9 sally What are you talking about?\r## 10 researcher Shall we move on? Good then.\r## 11 greg I\u0026#39;m hungry. Let\u0026#39;s eat. You already?\rNext we drop out rows. The drop_row function takes a data set, a column (named or numeric position) and regex terms to search for. The terms argument takes regex(es) allowing for partial matching. terms is case sensitive but can be changed via the ignore.case argument.\ndrop_row(dataframe = DATA, column = \u0026quot;person\u0026quot;, terms = c(\u0026quot;sam\u0026quot;, \u0026quot;greg\u0026quot;))\r## person sex adult state code\r## 1 teacher m 1 What should we do? K3\r## 2 sally f 0 How can we be certain? K6\r## 3 sally f 0 What are you talking about? K9\r## 4 researcher f 1 Shall we move on? Good then. K10\rdrop_row(DATA, 1, c(\u0026quot;sam\u0026quot;, \u0026quot;greg\u0026quot;))\r## person sex adult state code\r## 1 teacher m 1 What should we do? K3\r## 2 sally f 0 How can we be certain? K6\r## 3 sally f 0 What are you talking about? K9\r## 4 researcher f 1 Shall we move on? Good then. K10\rkeep_row(DATA, 1, c(\u0026quot;sam\u0026quot;, \u0026quot;greg\u0026quot;))\r## person sex adult state code\r## 1 sam m 0 Computer is fun. Not too fun. K1\r## 2 greg m 0 No it\u0026#39;s not, it\u0026#39;s dumb. K2\r## 3 sam m 0 You liar, it stinks! K4\r## 4 greg m 0 I am telling the truth! K5\r## 5 greg m 0 There is no way. K7\r## 6 sam m 0 I distrust you. K8\r## 7 greg m 0 I\u0026#39;m hungry. Let\u0026#39;s eat. You already? K11\rdrop_row(DATA, \u0026quot;state\u0026quot;, c(\u0026quot;Comp\u0026quot;))\r## person sex adult state code\r## 1 greg m 0 No it\u0026#39;s not, it\u0026#39;s dumb. K2\r## 2 teacher m 1 What should we do? K3\r## 3 sam m 0 You liar, it stinks! K4\r## 4 greg m 0 I am telling the truth! K5\r## 5 sally f 0 How can we be certain? K6\r## 6 greg m 0 There is no way. K7\r## 7 sam m 0 I distrust you. K8\r## 8 sally f 0 What are you talking about? K9\r## 9 researcher f 1 Shall we move on? Good then. K10\r## 10 greg m 0 I\u0026#39;m hungry. Let\u0026#39;s eat. You already? K11\rdrop_row(DATA, \u0026quot;state\u0026quot;, c(\u0026quot;I \u0026quot;))\r## person sex adult state code\r## 1 sam m 0 Computer is fun. Not too fun. K1\r## 2 greg m 0 No it\u0026#39;s not, it\u0026#39;s dumb. K2\r## 3 teacher m 1 What should we do? K3\r## 4 sam m 0 You liar, it stinks! K4\r## 5 sally f 0 How can we be certain? K6\r## 6 greg m 0 There is no way. K7\r## 7 sally f 0 What are you talking about? K9\r## 8 researcher f 1 Shall we move on? Good then. K10\r## 9 greg m 0 I\u0026#39;m hungry. Let\u0026#39;s eat. You already? K11\rdrop_row(DATA, \u0026quot;state\u0026quot;, c(\u0026quot;you\u0026quot;), ignore.case = TRUE)\r## person sex adult state code\r## 1 sam m 0 Computer is fun. Not too fun. K1\r## 2 greg m 0 No it\u0026#39;s not, it\u0026#39;s dumb. K2\r## 3 teacher m 1 What should we do? K3\r## 4 greg m 0 I am telling the truth! K5\r## 5 sally f 0 How can we be certain? K6\r## 6 greg m 0 There is no way. K7\r## 7 researcher f 1 Shall we move on? Good then. K10\r\r","date":1590192000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590259715,"objectID":"8ba94769f2cbab1c2cac2386771b3759","permalink":"https://ianadamsresearch.com/post/markdown-testing/testing-netlify-hosting-with-r-markdown/","publishdate":"2020-05-23T00:00:00Z","relpermalink":"/post/markdown-testing/testing-netlify-hosting-with-r-markdown/","section":"post","summary":"This report was generated on 2020-12-30, as a demo of textclean from https://github.com/trinker/textclean#check-text\nThis is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code.","tags":["Demo","R Markdown"],"title":"Testing Netlify hosting with R Markdown","type":"post"},{"authors":[],"categories":["R","website"],"content":"I really appreciate Dan Quintana for his walkthrough on getting an Academic themed website up and running.\nThank you to Peter Paul Pichler for his script adapated from Lorenzo Busetto\u0026rsquo;s original script to import publication data from bibtex files (I use Zotero, but other citation software should work fine).\n","date":1590105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590189414,"objectID":"40d848486f0820d605b2acf23a56c20e","permalink":"https://ianadamsresearch.com/post/new-website/new-website/","publishdate":"2020-05-22T00:00:00Z","relpermalink":"/post/new-website/new-website/","section":"post","summary":"I really appreciate Dan Quintana for his walkthrough on getting an Academic themed website up and running.\nThank you to Peter Paul Pichler for his script adapated from Lorenzo Busetto\u0026rsquo;s original script to import publication data from bibtex files (I use Zotero, but other citation software should work fine).","tags":["R Markdown"],"title":"New Website","type":"post"},{"authors":[],"categories":[],"content":"\r\r“That’s the problem with police, you don’t show enough feeling,” she said. “You don’t feel enough.” She’s right, of course.\nA drowning in a desert town with no lakes. My partner jumped into the canal and pushed the huge man to the bank. I struggled and pulled, he pushed and slipped, both of us wet and covered with mud, neither of us feeling the cold water. Our new officer arrived and jumped into the scene, pushing on the man’s chest while I hovered over his face, trying to see inside his mouth. His wife, hands on my shoulders, screaming in my ear, screaming no. I couldn’t feel her breath on my neck, her tears on my head. Finally, my partner says to stop, just stop. I looked down and saw my pants were covered in the man’s blood, which had poured from the bullet hole in his temple. I couldn’t feel the wet blood soaking through my pants. She’s right, of course, I couldn’t feel enough.\nA gunshot in the basement of a home. As I made my way down the stairs, the air was filled with a haze. A man, missing his face, and a rifle sitting in a lake of blood on the bed. Three walls covered with meat. The mewling, writhing figure unable to speak, but clearly letting me know the horror, the pain he was feeling. A few inches of jaw, glued to the carpet, the stubble of beard visible from the inside. The home now filled with explosive gas, a mother unaware. A mother begging us to just let her son die. As I grabbed the woman by the arm, I couldn’t feel her brittle, old bones under my grip. I couldn’t smell the gas, couldn’t gag as I pulled the jaw from the carpet; I couldn’t find it inside me to feel the horror. She’s right, of course, I couldn’t feel enough.\nA mother, worn by the chemotherapy, held hostage. A son, a mind broken by drugs and now holding mom hostage until a girlfriend returns. I can’t feel the gun in my hands. A plan develops, a promise of a drink for a thirsty mother, worn by the long negotiation. A foot through the door, a son brings the knife towards my partner. I can’t feel my friend get cut. I can’t feel my gun, screwed into the temple of a man who’s so close to having a mind broken by a bullet. “I’ve got the knife,” the officer behind me yells. I can’t feel the relief. She’s right, of course, I couldn’t feel enough.\nA breathing problem, a medical call, leave it for the medics. But I’m here, and this lady isn’t breathing, won’t breathe again. I can’t feel her granddaughter behind me, watching me place shock pads on grandma’s chest, watching me push helplessly on grandma as the machine tells me to push harder. Later now, the medics gone. As her husband pulls me into a hug, I can’t feel his heart breaking inside. Her husband has to say goodbye, and I go to her first. I never knew her, but she wouldn’t want to be seen like this, not for a goodbye. I pull a breathing tube from her throat, I can’t feel the bulb catch on her teeth, her stiffening jaw fighting this release. I pull on the bone needles screwed into her shins, I can’t feel how diabetes has scarred her lower legs. I wipe the blood from her nose and mouth, I can’t feel how cold the blood already is. She’s right, of course, I couldn’t feel enough.\nA naked monster, celebrating his first day outside of prison with a cocktail of street drugs and liquor, kicking in the door. A boy, just eight, standing behind the door, holding a bat to protect his four-year-old sister from the monster outside. I can’t feel their panic, I don’t know they stand just feet from where the monster and I fight. I can’t feel his fingernails, carving deep and bloody into my arm. I can’t feel the burning as sweat, blood, and mace spray mix into the bloody cuts. He can’t feel the pain, he’s well beyond feeling. As he breaks the porch, breaks the door, breaks my skin, breaks the quiet peace of the neighborhood, I can’t feel his hair in both my hands, pulling him back from the sidewalk where he slams his head. Even today, I can’t feel those five long scars on my arm. She’s right, of course, I couldn’t feel enough.\nA stolen car, a property crime. A man, too much time inside bars, reaching for the gun wrapped in a white t-shirt, cleaner than any of his other clothing. I can’t feel his hands around my waist as we fight, as we drag him from the car. I can’t feel the cars driving by us as I punch. His girlfriend screams, but I can’t feel her fear. As I continue to punch, five, six, seven hits, why won’t he stop? My wrist breaks, but I can’t feel that right now. “I’ll give you this,” he says later as we laugh together, “you boys know how to get down. I ain’t never been punched like that.” I can’t feel this admiration, can’t feel how sometimes the only one who understands is the guy you have to fight on the other side of the game. Months later, putting together another Lego set for my son. He can’t feel the bone inside my wrist give too much, the sharp pain that makes me gasp. I can’t feel the wrist, the back, the shoulder that has given too much. She’s right, of course, I couldn’t feel enough.\nAnother friend, another loss. Years ago now, he pulled that girl out of the cold creek, did he feel how her six-year-old body was too heavy, too water-soaked? She was the same age as his daughter, his daughter the same age as mine. His daughter’s hand-made cards alongside my daughter’s on the fridge. He’d felt enough; he must’ve had enough.\n","date":1452643200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1452643200,"objectID":"345e4ecdf16c4d71fe321c81e7cd20c9","permalink":"https://ianadamsresearch.com/post/tyranny-of-feeling/tyranny-of-feeling/","publishdate":"2016-01-13T00:00:00Z","relpermalink":"/post/tyranny-of-feeling/tyranny-of-feeling/","section":"post","summary":"“That’s the problem with police, you don’t show enough feeling,” she said. “You don’t feel enough.” She’s right, of course.\nA drowning in a desert town with no lakes.","tags":["police","trauma"],"title":"The Tyranny of Feeling","type":"post"},{"authors":null,"categories":null,"content":"\r\rI was dispatched on a gunshot. The 65-year-old mother reported her son had just shot himself. She was refusing to provide CPR or go see if he was okay. I was only a couple blocks away. My best friend and I arrived simultaneously and contacted the complainant at the back door. She was in a thin nightgown and completely calm. She said, “He’s down there,” pointing downstairs.\nAs we made our way down, I walked through a haze in the air, thick enough that it was forming waves in the air. I didn’t take immediate conscious notice of it, but both my partner and I commented on a strong smell in the air. Although I must have known better, I said that it must be gunpowder, making the connection with the call description of a gunshot.\nAs we got to the bottom of the stairs, I stopped at the sound of horrible moaning coming from a back bedroom. I made eye contact with Adam. “You ready?” He nodded, and we moved in slowly.\nThe bedroom was tucked off a small hallway that intersected with the main downstairs hallway. I saw the blood before entering. Three walls of the room were covered in goo. I couldn’t see the victim, but I could hear him. I saw a rifle laying on the bed, in a literal pool of blood. I forced myself to take a few more steps through the door. Training takes over, “Don’t stop in the fatal funnel.”\nAs I came through the door, I saw him to my left. He was down on all fours, rocking back and forth. The first detail I noticed was his very red shirt. I saw the bottom hem of the shirt was white, it was just a normal white t-shirt.\n“Buddy, we’ve got help coming, just stay there.” I’m not sure what you’re supposed to say. He raised his face to me. It was completely cut in half, the cheek flaps waving back and forth. He had stuck a rifle under his chin, and when he stretched to reach the trigger, his head tipped back. The bullet entered the soft jaw, crossing the hard palate, and left at the top of his nose. There was nothing left but cheeks and eyes.\nI’ll never forget the moaning. On all fours, he rocked back and forth, shaking his head back and forth and just moaning. The sound is still with me. As he shook his head, the two flaps of cheek kept swinging back and forth, opening and closing, just like the fucking Predator. I swear, if that guy had stood up and walked towards me, I would’ve shot him.\nThe first few medics arrived. They had been told by dispatch that the subject was “echo” (obviously deceased), and hesitated until I pointed out that they needed to get on this one, and get on it now. They started to cut off his red shirt. I told dispatch to get the bird going now. She said, “You want them on standby?” She was trying to help me, as really only medics are supposed to tell the air medics to fly. “No, tell them to fly now, this one’s going.”\nMy sergeant was the third to arrive. When he got downstairs, he took a deep breath, “What’s that smell?”\n“Gunpowder, sarge.”\n“Nah, that’s not gunpowder.” He breathed a few deep nasal breaths. “That’s…like propane or something.”\nThe medics stopped and inhaled. “Fuck.” Silence took over, and we all hear at the same time what had been covered up by horrible moaning. In the wall, behind where the man shot himself, was a bullet hole. Out of the hole came a hissing sound. The bullet had cut through the natural gas main that fed the house from the outside meter.\nFor the first time in my career, I saw firefighters panic. “Get out get out get out” three of them yelling at the same time. I got on the radio and called “No flames” and told everyone to clear. I saw for the first time what the “waves of haze” I had noticed earlier were. It was natural gas which had filled up the bottom floor of the house, filled it up so much that the “top” of the gas lake was over my eyes, at about six feet deep.\nThe medics grabbed the guy and ran upstairs. I quickly cleared the downstairs, thinking the guy might have small kids in the rooms or something. They were clear. I came upstairs and found Adam sitting calmly in the sitting room with the victim’s mother. I stopped, unable to understand what he was doing still in the house.\n“Adam, get her out of here!” I wasn’t yelling yet, but I was angry. I had stayed behind for two minutes in a situation that was probably going to kill me, and he had stayed in the fucking house talking with mom?\nHe looked at me, confused. “Adam, get her the fuck out of this house,” I was yelling now. The mother stood up, saying she needed shoes. I grabbed her, probably too hard on the arm, she was old. I pushed her through the door. “Get the fuck out and do it now.” I could feel myself losing a bit of control. I’ve never, never lost control.\nAdam didn’t understand. He had turned his radio down so he could sit with the mom and not have her hear the awful details that would likely be on the radio. He didn’t hear me give out the evacuate order. He still didn’t understand, but he trusted me, or was scared by my reaction, I’m not sure. He stepped between me and the mom, who was still trying to get inside. He did what had to be done, but he did it gently, at least. I couldn’t manage that.\nI went back in with some haz-mat guys ten minutes later. The horror of the room was more noticeable this time. I saw the little bone and teeth chips were stuck in the soles of my boot. I saw that huge glops of human goo were dripping down the walls. I got to the clean wall, the only wall without spackled meat all over it. It was the wall he had faced when he pulled the trigger. I looked down and saw his chin, laying on the carpet at my feet.\n“Dispatch, check with the hospital, see if they want this tissue.” I couldn’t believe I was even asking. The dude was going to die, I knew it, everyone knew it, right?\n“That’s affirm, they want it delivered.”\n“Copy.” How…the…fuck. My sergeant, one of my best friends, offered to do it. “Nah sarge, fuck it, I’ll get it.” I knew I was already gonna feel this one, no need for anyone else to take more of a hit on this. I grabbed a bio sack from one of the firefighters and went upstairs to the freezer. I put a layer of ice inside and went back to the chin.\nYou know how sometimes it’s the false expectations that get you? I went to pick up the chin. I knew it would be hard, like chins are. When I grabbed it, though, it felt like unset jello. Of course it did, there was no bone there, it was literally just the chin. It had fallen goo side down, and stuck to the carpet fibers as it dried over the previous thirty or so minutes. I had to tug at, and when it released it landed in my palm, gooey side up. I could see where his whiskers poked out…from the inside. Whoever thought that there was another end to each of our chin whiskers?\nI gave the bad to a firefighter to run it to the hospital. It didn’t matter; I knew he was dead. Or going to be soon enough.\nI learned later that he was 45 years old. At age 35, he got a degenerative brain disease, Huntington’s or something. It took him pretty quick, leaving him with the reasoning skills of an eight-year-old. His mother begged us on the front grass, after I probably bruised her arm, just to let him die. Where’s the DNR? I asked. There wasn’t one.\nHe died two hours after shooting himself. They pumped blood bag after blood bag into him, but it all just came out his massive facial wounds.\nThe ME called me that night, asked, “So, that was a weird one huh?” Fucking ME’s.\n“What’ya need man?” I asked him.\n“You notice anything strange in there?” he asked.\n“…nope. Just another fucking suicide.”\n“Yeah, figured. Alright, see you on the next one,” he said.\n“Yup.”\n","date":1371081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1371081600,"objectID":"e45e120a09dd7f631764e218c7875413","permalink":"https://ianadamsresearch.com/post/it-will-have-blood/it-will-have-blood/","publishdate":"2013-06-13T00:00:00Z","relpermalink":"/post/it-will-have-blood/it-will-have-blood/","section":"post","summary":"I was dispatched on a gunshot. The 65-year-old mother reported her son had just shot himself. She was refusing to provide CPR or go see if he was okay.","tags":["police","trauma"],"title":"It Will Have Blood","type":"post"}]